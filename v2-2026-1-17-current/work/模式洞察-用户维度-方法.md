# 模式洞察 - 用户维度 - 方法层

> 包含模式：38, 39, 40, 41, 42
> 最后更新：2026-01-24
> 状态：⏳ 待实现

---

## 分析目标

从用户行为数据中回答：

1. **用户关心什么？** → 话题热词、高频问题
2. **用户感受如何？** → 情感分布、满意度
3. **用户需求是什么？** → 未解决的问题、内容缺口
4. **用户兴趣如何变化？** → 时间趋势、季节性
5. **不同用户群有什么差异？** → 用户画像、分群特征

---

## 一、评论热词分析（模式38）

### 方法：TF-IDF + 词频统计

```python
import jieba
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer

def analyze_hot_words(comments: list[str]) -> dict:
    """
    分析评论热词

    Args:
        comments: 评论文本列表

    Returns:
        {
            "word_freq": [(word, count), ...],      # 词频 Top 50
            "tfidf_keywords": [(word, score), ...],  # TF-IDF 关键词
            "bigrams": [(phrase, count), ...],       # 二元短语
        }
    """
    # 1. 分词
    all_words = []
    for text in comments:
        words = jieba.lcut(text)
        all_words.extend([w for w in words if len(w) > 1])

    # 2. 词频统计
    word_freq = Counter(all_words).most_common(50)

    # 3. TF-IDF 关键词提取
    vectorizer = TfidfVectorizer(
        tokenizer=jieba.lcut,
        max_features=100
    )
    tfidf_matrix = vectorizer.fit_transform(comments)
    feature_names = vectorizer.get_feature_names_out()
    # 计算每个词的平均 TF-IDF 分数
    avg_scores = tfidf_matrix.mean(axis=0).A1
    tfidf_keywords = sorted(
        zip(feature_names, avg_scores),
        key=lambda x: x[1],
        reverse=True
    )[:30]

    return {
        "word_freq": word_freq,
        "tfidf_keywords": tfidf_keywords,
    }
```

### 停用词过滤

```python
STOP_WORDS = {
    # 语气词
    "的", "了", "是", "我", "你", "他", "她", "它",
    "这", "那", "有", "在", "不", "和", "也", "就",
    "都", "很", "会", "可以", "什么", "怎么", "为什么",
    # 平台词
    "视频", "频道", "订阅", "点赞", "评论", "分享",
    # 情感词单独分析
    "好", "不错", "喜欢", "感谢", "谢谢",
}
```

---

## 二、用户问题挖掘（模式39）

### 方法：问句识别 + 聚类

```python
import re

def extract_questions(comments: list[str]) -> list[dict]:
    """
    提取用户问题

    问句识别规则：
    1. 以「？」结尾
    2. 包含疑问词（怎么、如何、为什么、什么、哪里、多少）
    3. 包含祈使句式（请问、想问、求）
    """
    question_patterns = [
        r".*？$",                          # 问号结尾
        r".*(怎么|如何|为什么|什么|哪里|多少|能不能|可不可以).*",
        r"^(请问|想问|求问|问一下).*",
    ]

    questions = []
    for text in comments:
        for pattern in question_patterns:
            if re.match(pattern, text):
                questions.append({
                    "text": text,
                    "type": classify_question(text)
                })
                break

    return questions


def classify_question(text: str) -> str:
    """
    问题分类

    Categories:
    - HOW: 方法类（怎么做、如何）
    - WHY: 原因类（为什么）
    - WHAT: 定义类（是什么、什么是）
    - WHERE: 资源类（哪里、在哪）
    - RECOMMEND: 推荐类（推荐、求推荐）
    - OTHER: 其他
    """
    if re.search(r"(怎么|如何|怎样)", text):
        return "HOW"
    elif re.search(r"为什么", text):
        return "WHY"
    elif re.search(r"(是什么|什么是)", text):
        return "WHAT"
    elif re.search(r"(哪里|在哪|去哪)", text):
        return "WHERE"
    elif re.search(r"(推荐|求)", text):
        return "RECOMMEND"
    else:
        return "OTHER"
```

### 问题聚类

```python
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer

def cluster_questions(questions: list[str], n_clusters: int = 10):
    """
    将相似问题聚类

    使用语义嵌入 + KMeans
    """
    # 1. 生成语义向量
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    embeddings = model.encode(questions)

    # 2. 聚类
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(embeddings)

    # 3. 每个簇选代表性问题
    cluster_questions = {}
    for i, q in enumerate(questions):
        cluster_id = clusters[i]
        if cluster_id not in cluster_questions:
            cluster_questions[cluster_id] = []
        cluster_questions[cluster_id].append(q)

    return cluster_questions
```

---

## 三、情感分析（模式40）

### 方法：情感词典 + 规则

```python
def analyze_sentiment(text: str) -> dict:
    """
    情感分析

    Returns:
        {
            "sentiment": "positive" | "neutral" | "negative",
            "score": float,  # -1 ~ 1
            "keywords": list[str],  # 触发情感的关键词
        }
    """
    # 情感词典
    POSITIVE = {"好", "棒", "赞", "喜欢", "感谢", "有用", "学到", "受益", "收藏"}
    NEGATIVE = {"差", "烂", "骗", "假", "无聊", "浪费", "失望", "垃圾"}
    INTENSIFIER = {"很", "非常", "太", "超", "特别", "真的"}  # 强化词
    NEGATION = {"不", "没", "无", "别"}  # 否定词

    words = jieba.lcut(text)
    pos_count = 0
    neg_count = 0
    keywords = []

    for i, word in enumerate(words):
        # 检查前面是否有否定词
        has_negation = i > 0 and words[i-1] in NEGATION

        if word in POSITIVE:
            if has_negation:
                neg_count += 1
            else:
                pos_count += 1
            keywords.append(word)

        elif word in NEGATIVE:
            if has_negation:
                pos_count += 1
            else:
                neg_count += 1
            keywords.append(word)

    total = pos_count + neg_count
    if total == 0:
        return {"sentiment": "neutral", "score": 0, "keywords": []}

    score = (pos_count - neg_count) / total

    if score > 0.3:
        sentiment = "positive"
    elif score < -0.3:
        sentiment = "negative"
    else:
        sentiment = "neutral"

    return {
        "sentiment": sentiment,
        "score": score,
        "keywords": keywords
    }
```

### 情感分布统计

```python
def sentiment_distribution(comments: list[str]) -> dict:
    """
    计算情感分布

    Returns:
        {
            "positive": 0.65,  # 65% 正面
            "neutral": 0.25,   # 25% 中性
            "negative": 0.10,  # 10% 负面
            "avg_score": 0.42, # 平均情感分数
        }
    """
    results = [analyze_sentiment(c) for c in comments]

    distribution = {
        "positive": len([r for r in results if r["sentiment"] == "positive"]) / len(results),
        "neutral": len([r for r in results if r["sentiment"] == "neutral"]) / len(results),
        "negative": len([r for r in results if r["sentiment"] == "negative"]) / len(results),
        "avg_score": sum(r["score"] for r in results) / len(results),
    }

    return distribution
```

---

## 四、话题热度趋势（模式41）

### 方法：时间序列 + 关键词追踪

```python
from datetime import datetime, timedelta
from collections import defaultdict

def topic_trend_analysis(comments: list[dict], keywords: list[str]) -> dict:
    """
    分析话题热度随时间变化

    Args:
        comments: [{"text": str, "published_at": datetime}, ...]
        keywords: 要追踪的关键词列表

    Returns:
        {
            "keyword": {
                "2026-01-01": count,
                "2026-01-02": count,
                ...
            }
        }
    """
    # 按日期聚合
    daily_counts = defaultdict(lambda: defaultdict(int))

    for comment in comments:
        date = comment["published_at"].strftime("%Y-%m-%d")
        text = comment["text"]

        for keyword in keywords:
            if keyword in text:
                daily_counts[keyword][date] += 1

    # 计算趋势方向
    trends = {}
    for keyword, dates in daily_counts.items():
        sorted_dates = sorted(dates.items())
        if len(sorted_dates) >= 7:
            recent = sum(v for k, v in sorted_dates[-7:])
            previous = sum(v for k, v in sorted_dates[-14:-7]) if len(sorted_dates) >= 14 else recent
            change = (recent - previous) / previous if previous > 0 else 0
            trends[keyword] = {
                "data": dict(sorted_dates),
                "recent_7d": recent,
                "change": change,
                "direction": "up" if change > 0.1 else ("down" if change < -0.1 else "stable")
            }

    return trends
```

---

## 五、用户画像推断（模式42）

### 方法：评论特征提取 + 标签化

```python
def infer_user_profile(comments_by_user: dict) -> dict:
    """
    根据用户评论推断画像

    Args:
        comments_by_user: {user_id: [comment_texts], ...}

    Returns:
        {
            user_id: {
                "engagement_level": "high" | "medium" | "low",
                "content_preference": ["太极", "养生"],
                "sentiment_tendency": "positive" | "neutral" | "negative",
                "question_asker": True | False,
                "active_hours": ["morning", "evening"],
            }
        }
    """
    profiles = {}

    for user_id, comments in comments_by_user.items():
        # 1. 活跃度
        engagement = "high" if len(comments) > 10 else ("medium" if len(comments) > 3 else "low")

        # 2. 内容偏好（提取高频词）
        all_text = " ".join(comments)
        words = jieba.lcut(all_text)
        content_pref = Counter(words).most_common(5)

        # 3. 情感倾向
        sentiments = [analyze_sentiment(c)["sentiment"] for c in comments]
        dominant_sentiment = Counter(sentiments).most_common(1)[0][0]

        # 4. 是否爱提问
        question_count = sum(1 for c in comments if "？" in c)
        is_asker = question_count / len(comments) > 0.3

        profiles[user_id] = {
            "engagement_level": engagement,
            "content_preference": [w for w, _ in content_pref],
            "sentiment_tendency": dominant_sentiment,
            "question_asker": is_asker,
        }

    return profiles
```

---

## 六、高赞评论特征分析

### 方法：对比高赞 vs 普通评论

```python
def analyze_top_comments(comments: list[dict], top_n: int = 100) -> dict:
    """
    分析高赞评论的特征

    Args:
        comments: [{"text": str, "like_count": int}, ...]

    Returns:
        {
            "avg_length": float,      # 平均长度
            "emoji_rate": float,      # 包含表情的比例
            "question_rate": float,   # 问句比例
            "sentiment_dist": dict,   # 情感分布
            "common_patterns": list,  # 常见句式
        }
    """
    # 按点赞排序
    sorted_comments = sorted(comments, key=lambda x: x["like_count"], reverse=True)
    top_comments = sorted_comments[:top_n]
    bottom_comments = sorted_comments[-top_n:]

    def extract_features(comment_list):
        texts = [c["text"] for c in comment_list]
        return {
            "avg_length": sum(len(t) for t in texts) / len(texts),
            "emoji_rate": sum(1 for t in texts if has_emoji(t)) / len(texts),
            "question_rate": sum(1 for t in texts if "？" in t) / len(texts),
            "sentiment_dist": sentiment_distribution(texts),
        }

    return {
        "top_features": extract_features(top_comments),
        "bottom_features": extract_features(bottom_comments),
        "difference": "高赞评论更长、情感更强烈、经常包含个人经历分享",
    }
```

---

## 七、分析流程

```
┌─────────────────────────────────────────────────────────────┐
│                      评论数据采集                            │
│  python3 scripts/fetch_comments.py --limit 200              │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                      数据预处理                              │
│  • 清洗（去重、过滤机器人）                                   │
│  • 分词（jieba）                                             │
│  • 语言检测（langdetect）                                    │
└─────────────────────────────────────────────────────────────┘
                              │
          ┌───────────────────┼───────────────────┐
          ▼                   ▼                   ▼
┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
│ 热词分析(模式38) │ │ 问题挖掘(模式39) │ │ 情感分析(模式40) │
│ TF-IDF + 词频   │ │ 问句识别 + 聚类 │ │ 情感词典 + 规则 │
└─────────────────┘ └─────────────────┘ └─────────────────┘
          │                   │                   │
          └───────────────────┼───────────────────┘
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                    话题趋势分析（模式41）                     │
│  时间序列 + 关键词追踪                                        │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                    用户画像推断（模式42）                     │
│  评论特征 + 行为模式 → 用户标签                               │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                        输出结论                              │
│  → 模式洞察-用户维度-输出.md                                  │
└─────────────────────────────────────────────────────────────┘
```

---

## 八、依赖库

```bash
# Python 依赖
pip install jieba                    # 中文分词
pip install scikit-learn             # TF-IDF, KMeans
pip install sentence-transformers    # 语义嵌入（可选，用于高级聚类）
pip install langdetect               # 语言检测
pip install pandas                   # 数据处理
pip install matplotlib               # 可视化
```

---

## 相关文档

- [模式洞察-用户维度-数据.md](./模式洞察-用户维度-数据.md) - 数据层
- [模式洞察-用户维度-输出.md](./模式洞察-用户维度-输出.md) - 输出结论
- [模式洞察-评估方法.md](./模式洞察-评估方法.md) - 通用评估方法

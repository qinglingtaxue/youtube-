#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸‚åœºè§„æ¨¡ä¸ç«äº‰æ ¼å±€åˆ†æå™¨

åˆ†æç»´åº¦ï¼š
1. å¸‚åœºè¾¹ç•Œ - æ€»è§†é¢‘æ•°ã€æ€»æ’­æ”¾é‡ã€è¦†ç›–é¢‘é“æ•°
2. ç«äº‰å¼ºåº¦ - å¤´éƒ¨é›†ä¸­åº¦ã€é•¿å°¾åˆ†å¸ƒ
3. å‘å¸ƒé¢‘ç‡ - æ—¥å‡/æœˆå‡å‘å¸ƒé‡ã€æ´»è·ƒé¢‘é“
4. æ—¶é—´è¶‹åŠ¿ - å‘å¸ƒæ—¶é—´åˆ†å¸ƒã€å¢é•¿è¶‹åŠ¿
5. è¿›å…¥å£å’ - çˆ†æ¬¾é—¨æ§›ã€å¹³å‡è¡¨ç°
"""

import json
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional
import sys

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.shared.logger import setup_logger
from src.shared.models import CompetitorVideo
from src.shared.repositories import CompetitorVideoRepository
from src.research.yt_dlp_client import YtDlpClient


@dataclass
class MarketReport:
    """å¸‚åœºåˆ†ææŠ¥å‘Š"""

    # å¸‚åœºè§„æ¨¡
    market_size: Dict[str, Any] = field(default_factory=dict)

    # é¢‘é“ç«äº‰
    channel_competition: Dict[str, Any] = field(default_factory=dict)

    # å‘å¸ƒé¢‘ç‡
    publishing_frequency: Dict[str, Any] = field(default_factory=dict)

    # æ—¶é—´è¶‹åŠ¿
    time_trends: Dict[str, Any] = field(default_factory=dict)

    # è¿›å…¥å£å’
    entry_barriers: Dict[str, Any] = field(default_factory=dict)

    # AIåˆ›ä½œè€…æœºä¼šåˆ†æ
    ai_creator_opportunities: Dict[str, Any] = field(default_factory=dict)

    # å…ƒæ•°æ®
    generated_at: str = ""
    sample_size: int = 0

    # æ—¶é—´èŒƒå›´ä¿¡æ¯
    time_context: Dict[str, Any] = field(default_factory=dict)


class MarketAnalyzer:
    """å¸‚åœºè§„æ¨¡ä¸ç«äº‰æ ¼å±€åˆ†æå™¨"""

    def __init__(self, db_path: Optional[str] = None):
        self.logger = setup_logger('market_analyzer')
        self.repo = CompetitorVideoRepository(db_path)
        self.client = YtDlpClient()
        self.videos: List[CompetitorVideo] = []

    def load_data(self, limit: int = 5000) -> int:
        """åŠ è½½æ•°æ®"""
        self.videos = self.repo.find_all(limit=limit)
        self.logger.info(f"åŠ è½½äº† {len(self.videos)} ä¸ªè§†é¢‘")
        return len(self.videos)

    def enrich_publish_dates(self, limit: int = 200) -> int:
        """
        è¡¥å……å‘å¸ƒæ—¶é—´ï¼ˆéœ€è¦å•ç‹¬è¯·æ±‚è¯¦æƒ…ï¼‰

        Args:
            limit: æœ€å¤šè¡¥å……å¤šå°‘ä¸ª

        Returns:
            æˆåŠŸè¡¥å……çš„æ•°é‡
        """
        # æ‰¾å‡ºæ²¡æœ‰å‘å¸ƒæ—¶é—´çš„è§†é¢‘ï¼ˆä¼˜å…ˆé«˜æ’­æ”¾é‡ï¼‰
        no_date = [v for v in self.videos if not v.published_at]
        no_date.sort(key=lambda x: x.view_count, reverse=True)
        no_date = no_date[:limit]

        self.logger.info(f"å¼€å§‹è¡¥å…… {len(no_date)} ä¸ªè§†é¢‘çš„å‘å¸ƒæ—¶é—´...")

        success = 0
        for i, v in enumerate(no_date, 1):
            try:
                info = self.client.get_video_info(v.youtube_id)
                if info and info.get('upload_date'):
                    # æ›´æ–°æ•°æ®åº“
                    upload_date = info['upload_date']
                    # è§£ææ—¥æœŸ YYYYMMDD æˆ– YYYY-MM-DD
                    if '-' in upload_date:
                        dt = datetime.strptime(upload_date, '%Y-%m-%d')
                    else:
                        dt = datetime.strptime(upload_date, '%Y%m%d')

                    v.published_at = dt
                    self.repo.save(v)
                    success += 1

                if i % 20 == 0:
                    self.logger.info(f"è¿›åº¦: {i}/{len(no_date)}, æˆåŠŸ: {success}")

            except Exception as e:
                self.logger.warning(f"è·å– {v.youtube_id} å¤±è´¥: {e}")

        self.logger.info(f"å‘å¸ƒæ—¶é—´è¡¥å……å®Œæˆ: {success}/{len(no_date)}")
        return success

    def analyze(self) -> MarketReport:
        """æ‰§è¡Œå¸‚åœºåˆ†æ"""
        if not self.videos:
            self.load_data()

        report = MarketReport()
        report.generated_at = datetime.now().isoformat()
        report.sample_size = len(self.videos)

        self.logger.info("å¼€å§‹å¸‚åœºåˆ†æ...")

        # 0. æ—¶é—´èŒƒå›´ä¸Šä¸‹æ–‡
        report.time_context = self._build_time_context()

        # 1. å¸‚åœºè§„æ¨¡
        report.market_size = self._analyze_market_size()

        # 2. é¢‘é“ç«äº‰
        report.channel_competition = self._analyze_channel_competition()

        # 3. å‘å¸ƒé¢‘ç‡
        report.publishing_frequency = self._analyze_publishing_frequency()

        # 4. æ—¶é—´è¶‹åŠ¿
        report.time_trends = self._analyze_time_trends()

        # 5. è¿›å…¥å£å’
        report.entry_barriers = self._analyze_entry_barriers()

        # 6. AIåˆ›ä½œè€…æœºä¼šåˆ†æ
        report.ai_creator_opportunities = self._analyze_ai_opportunities()

        self.logger.info("å¸‚åœºåˆ†æå®Œæˆ")
        return report

    def _build_time_context(self) -> Dict[str, Any]:
        """æ„å»ºæ—¶é—´èŒƒå›´ä¸Šä¸‹æ–‡"""
        now = datetime.now()
        videos_with_date = [v for v in self.videos if v.published_at]

        if not videos_with_date:
            return {
                'analysis_date': now.strftime('%Y-%m-%d'),
                'data_has_dates': False,
                'note': 'æ•°æ®ç¼ºå°‘å‘å¸ƒæ—¶é—´ï¼Œæ—¶é—´åˆ†æå—é™'
            }

        dates = [v.published_at for v in videos_with_date]
        earliest = min(dates)
        latest = max(dates)

        # ç»Ÿè®¡å„æ—¶é—´æ®µçš„è§†é¢‘æ•°é‡
        time_buckets = {
            '24å°æ—¶å†…': len([v for v in videos_with_date if (now - v.published_at).days < 1]),
            '7å¤©å†…': len([v for v in videos_with_date if (now - v.published_at).days < 7]),
            '30å¤©å†…': len([v for v in videos_with_date if (now - v.published_at).days < 30]),
            '90å¤©å†…': len([v for v in videos_with_date if (now - v.published_at).days < 90]),
            '6ä¸ªæœˆå†…': len([v for v in videos_with_date if (now - v.published_at).days < 180]),
            '1å¹´å†…': len([v for v in videos_with_date if (now - v.published_at).days < 365]),
            '1å¹´ä»¥ä¸Š': len([v for v in videos_with_date if (now - v.published_at).days >= 365]),
        }

        return {
            'analysis_date': now.strftime('%Y-%m-%d'),
            'data_has_dates': True,
            'videos_with_date_count': len(videos_with_date),
            'videos_without_date_count': len(self.videos) - len(videos_with_date),
            'date_coverage_rate': round(len(videos_with_date) / len(self.videos) * 100, 1),
            'date_range': {
                'earliest': earliest.strftime('%Y-%m-%d'),
                'latest': latest.strftime('%Y-%m-%d'),
                'span_days': (latest - earliest).days,
                'span_description': self._describe_time_span((latest - earliest).days),
            },
            'time_distribution': time_buckets,
            'time_distribution_percent': {
                k: round(v / len(videos_with_date) * 100, 1)
                for k, v in time_buckets.items()
            },
        }

    def _describe_time_span(self, days: int) -> str:
        """å°†å¤©æ•°è½¬æ¢ä¸ºå¯è¯»æè¿°"""
        if days < 7:
            return f"{days}å¤©"
        elif days < 30:
            return f"{days // 7}å‘¨{days % 7}å¤©" if days % 7 else f"{days // 7}å‘¨"
        elif days < 365:
            months = days // 30
            return f"çº¦{months}ä¸ªæœˆ"
        else:
            years = days // 365
            months = (days % 365) // 30
            return f"{years}å¹´{months}ä¸ªæœˆ" if months else f"{years}å¹´"

    def _analyze_market_size(self) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºè§„æ¨¡"""
        view_counts = [v.view_count for v in self.videos]

        return {
            'sample_videos': len(self.videos),
            'total_views': sum(view_counts),
            'avg_views': int(sum(view_counts) / len(view_counts)) if view_counts else 0,
            'median_views': sorted(view_counts)[len(view_counts)//2] if view_counts else 0,
            'max_views': max(view_counts) if view_counts else 0,
            'estimated_market': self._estimate_total_market(),
        }

    def _estimate_total_market(self) -> Dict[str, Any]:
        """ä¼°ç®—æ€»å¸‚åœºè§„æ¨¡"""
        # åŸºäºæœç´¢ç»“æœçš„æ ·æœ¬ï¼Œä¼°ç®—å¸‚åœºæ€»é‡
        # å‡è®¾æœç´¢ç»“æœè¦†ç›–äº†å¸‚åœºçš„ä¸€éƒ¨åˆ†
        return {
            'note': 'åŸºäºæœç´¢æ ·æœ¬ä¼°ç®—ï¼Œå®é™…å¸‚åœºå¯èƒ½æ›´å¤§',
            'sample_coverage': 'æœç´¢ç»“æœ Top 200/å…³é”®è¯',
            'estimated_multiplier': '2-5x',
        }

    def _analyze_channel_competition(self) -> Dict[str, Any]:
        """åˆ†æé¢‘é“ç«äº‰æ ¼å±€"""
        channel_videos = defaultdict(list)
        for v in self.videos:
            if v.channel_name:
                channel_videos[v.channel_name].append(v)

        total_channels = len(channel_videos)

        # é¢‘é“è§„æ¨¡åˆ†å¸ƒ
        size_distribution = {
            'single_video': sum(1 for vids in channel_videos.values() if len(vids) == 1),
            'small_2_4': sum(1 for vids in channel_videos.values() if 2 <= len(vids) < 5),
            'medium_5_9': sum(1 for vids in channel_videos.values() if 5 <= len(vids) < 10),
            'large_10_plus': sum(1 for vids in channel_videos.values() if len(vids) >= 10),
        }

        # å¤´éƒ¨é›†ä¸­åº¦ï¼ˆTop 10 é¢‘é“å æ¯”ï¼‰
        channel_stats = []
        for channel, vids in channel_videos.items():
            total_views = sum(v.view_count for v in vids)
            channel_stats.append({
                'channel': channel,
                'video_count': len(vids),
                'total_views': total_views,
                'avg_views': int(total_views / len(vids)),
            })

        channel_stats.sort(key=lambda x: x['total_views'], reverse=True)

        total_market_views = sum(c['total_views'] for c in channel_stats)
        top10_views = sum(c['total_views'] for c in channel_stats[:10])
        top20_views = sum(c['total_views'] for c in channel_stats[:20])

        return {
            'total_channels': total_channels,
            'size_distribution': size_distribution,
            'concentration': {
                'top10_share': round(top10_views / total_market_views * 100, 1) if total_market_views else 0,
                'top20_share': round(top20_views / total_market_views * 100, 1) if total_market_views else 0,
                'herfindahl_index': self._calculate_hhi(channel_stats),
            },
            'top_channels': channel_stats[:15],
            'new_entrants': self._identify_new_entrants(channel_videos),
        }

    def _calculate_hhi(self, channel_stats: List[Dict]) -> float:
        """è®¡ç®—èµ«èŠ¬è¾¾å°”æŒ‡æ•°ï¼ˆå¸‚åœºé›†ä¸­åº¦ï¼‰"""
        total = sum(c['total_views'] for c in channel_stats)
        if total == 0:
            return 0
        hhi = sum((c['total_views'] / total * 100) ** 2 for c in channel_stats)
        return round(hhi, 2)

    def _identify_new_entrants(self, channel_videos: Dict) -> Dict[str, Any]:
        """è¯†åˆ«æ–°è¿›å…¥è€…"""
        # åŸºäºè§†é¢‘æ•°é‡å°‘ä½†æ’­æ”¾é‡ä¸é”™çš„é¢‘é“
        new_entrants = []
        for channel, vids in channel_videos.items():
            if 1 <= len(vids) <= 3:
                total_views = sum(v.view_count for v in vids)
                avg_views = total_views / len(vids)
                if avg_views > 10000:  # å¹³å‡æ’­æ”¾é‡è¶…è¿‡ 1 ä¸‡
                    new_entrants.append({
                        'channel': channel,
                        'video_count': len(vids),
                        'avg_views': int(avg_views),
                    })

        new_entrants.sort(key=lambda x: x['avg_views'], reverse=True)
        return {
            'count': len(new_entrants),
            'promising': new_entrants[:10],
        }

    def _analyze_publishing_frequency(self) -> Dict[str, Any]:
        """åˆ†æå‘å¸ƒé¢‘ç‡"""
        videos_with_date = [v for v in self.videos if v.published_at]

        if not videos_with_date:
            return {'note': 'ç¼ºå°‘å‘å¸ƒæ—¶é—´æ•°æ®ï¼Œéœ€è¦è¡¥å……é‡‡é›†'}

        dates = [v.published_at for v in videos_with_date]
        earliest = min(dates)
        latest = max(dates)
        date_range = (latest - earliest).days

        # æ—¥å‡å‘å¸ƒ
        daily_avg = len(videos_with_date) / date_range if date_range > 0 else 0

        # æŒ‰æœˆç»Ÿè®¡
        monthly = defaultdict(int)
        for v in videos_with_date:
            key = v.published_at.strftime('%Y-%m')
            monthly[key] += 1

        # æŒ‰å‘¨ç»Ÿè®¡
        weekly = defaultdict(int)
        for v in videos_with_date:
            key = v.published_at.strftime('%Y-W%W')
            weekly[key] += 1

        # æ˜ŸæœŸåˆ†å¸ƒ
        weekday = defaultdict(int)
        for v in videos_with_date:
            weekday[v.published_at.weekday()] += 1

        return {
            'sample_with_date': len(videos_with_date),
            'date_range': {
                'start': earliest.isoformat(),
                'end': latest.isoformat(),
                'days': date_range,
            },
            'frequency': {
                'daily_avg': round(daily_avg, 2),
                'weekly_avg': round(daily_avg * 7, 1),
                'monthly_avg': round(daily_avg * 30, 0),
            },
            'monthly_distribution': dict(sorted(monthly.items())[-12:]),
            'weekday_distribution': {
                'å‘¨ä¸€': weekday[0], 'å‘¨äºŒ': weekday[1], 'å‘¨ä¸‰': weekday[2],
                'å‘¨å››': weekday[3], 'å‘¨äº”': weekday[4], 'å‘¨å…­': weekday[5], 'å‘¨æ—¥': weekday[6],
            },
        }

    def _analyze_time_trends(self) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´è¶‹åŠ¿"""
        videos_with_date = [v for v in self.videos if v.published_at]

        if not videos_with_date:
            return {'note': 'ç¼ºå°‘å‘å¸ƒæ—¶é—´æ•°æ®'}

        # æŒ‰å¹´ç»Ÿè®¡
        yearly = defaultdict(lambda: {'count': 0, 'views': 0})
        for v in videos_with_date:
            year = v.published_at.year
            yearly[year]['count'] += 1
            yearly[year]['views'] += v.view_count

        # è®¡ç®—å¹´å¢é•¿ç‡
        years = sorted(yearly.keys())
        growth_rates = []
        for i in range(1, len(years)):
            prev = yearly[years[i-1]]['count']
            curr = yearly[years[i]]['count']
            if prev > 0:
                rate = (curr - prev) / prev * 100
                growth_rates.append({'year': years[i], 'growth': round(rate, 1)})

        # è¿‘æœŸè¶‹åŠ¿ï¼ˆæœ€è¿‘ 3 ä¸ªæœˆ vs ä¹‹å‰ 3 ä¸ªæœˆï¼‰
        now = datetime.now()
        recent_3m = [v for v in videos_with_date if v.published_at >= now - timedelta(days=90)]
        prev_3m = [v for v in videos_with_date
                   if now - timedelta(days=180) <= v.published_at < now - timedelta(days=90)]

        return {
            'yearly_stats': {year: stats for year, stats in sorted(yearly.items())},
            'growth_rates': growth_rates,
            'recent_trend': {
                'recent_3m_count': len(recent_3m),
                'prev_3m_count': len(prev_3m),
                'trend': 'growing' if len(recent_3m) > len(prev_3m) * 1.1 else
                        ('declining' if len(recent_3m) < len(prev_3m) * 0.9 else 'stable'),
            },
        }

    def _analyze_entry_barriers(self) -> Dict[str, Any]:
        """åˆ†æè¿›å…¥å£å’"""
        view_counts = sorted([v.view_count for v in self.videos], reverse=True)

        # æ’­æ”¾é‡åˆ†å±‚
        tiers = {
            'viral_1m_plus': sum(1 for v in view_counts if v >= 1000000),
            'excellent_100k': sum(1 for v in view_counts if 100000 <= v < 1000000),
            'good_10k': sum(1 for v in view_counts if 10000 <= v < 100000),
            'average_1k': sum(1 for v in view_counts if 1000 <= v < 10000),
            'low_under_1k': sum(1 for v in view_counts if v < 1000),
        }

        # çˆ†æ¬¾é—¨æ§›
        p90 = view_counts[int(len(view_counts) * 0.1)] if view_counts else 0
        p95 = view_counts[int(len(view_counts) * 0.05)] if view_counts else 0
        p99 = view_counts[int(len(view_counts) * 0.01)] if view_counts else 0

        return {
            'performance_tiers': tiers,
            'thresholds': {
                'top_10_percent': p90,
                'top_5_percent': p95,
                'top_1_percent': p99,
            },
            'success_rate': {
                'viral_rate': round(tiers['viral_1m_plus'] / len(self.videos) * 100, 2),
                'excellent_rate': round(tiers['excellent_100k'] / len(self.videos) * 100, 2),
                'above_average_rate': round(
                    (tiers['viral_1m_plus'] + tiers['excellent_100k'] + tiers['good_10k'])
                    / len(self.videos) * 100, 1),
            },
            'recommendations': self._generate_entry_recommendations(tiers, p90),
        }

    def _generate_entry_recommendations(self, tiers: Dict, p90: int) -> List[str]:
        """ç”Ÿæˆè¿›å…¥å»ºè®®"""
        recs = []

        viral_rate = tiers['viral_1m_plus'] / sum(tiers.values()) * 100
        if viral_rate < 2:
            recs.append(f"çˆ†æ¬¾ç‡çº¦ {viral_rate:.1f}%ï¼Œéœ€è¦æŒç»­äº§å‡ºæ‰èƒ½å‘½ä¸­çˆ†æ¬¾")

        recs.append(f"è¿›å…¥ Top 10% éœ€è¦è¾¾åˆ° {p90:,} æ’­æ”¾é‡")

        if tiers['low_under_1k'] > sum(tiers.values()) * 0.3:
            recs.append("çº¦ 30%+ è§†é¢‘æ’­æ”¾é‡ä¸è¶³ 1000ï¼Œå¸‚åœºå­˜åœ¨å¤§é‡ä½è´¨é‡å†…å®¹")

        return recs

    def _analyze_ai_opportunities(self) -> Dict[str, Any]:
        """
        AI è§†é¢‘åˆ›ä½œè€…æœºä¼šåˆ†æ

        ä¸“ä¸ºæƒ³æ‰¹é‡å‘å¸ƒ AI è§†é¢‘çš„ç”¨æˆ·è®¾è®¡ï¼ŒæŒ–æ˜ï¼š
        1. è¿‘æœŸçˆ†æ¬¾ï¼ˆ7å¤©/30å¤©/90å¤©/6ä¸ªæœˆå†…ï¼‰- å¯æ¨¡ä»¿çš„æˆåŠŸæ¡ˆä¾‹
        2. å°ä¼—é«˜å¢é•¿ - æ—¥å‡å¢é•¿å¿«ä½†æ€»é‡ä¸å¤§çš„è“æµ·
        3. å°é¢‘é“é»‘é©¬ - è¯æ˜æ–°é¢‘é“ä¹Ÿèƒ½æˆåŠŸ
        4. é«˜äº’åŠ¨ç‡æ¨¡æ¿ - é€‚åˆ AI é…éŸ³çš„å†…å®¹ç±»å‹
        """
        now = datetime.now()
        videos_with_date = [v for v in self.videos if v.published_at]

        if not videos_with_date:
            return {
                'note': 'éœ€è¦å‘å¸ƒæ—¶é—´æ•°æ®æ‰èƒ½è¿›è¡Œæœºä¼šåˆ†æ',
                'time_range': 'æ— æ³•ç¡®å®š'
            }

        # å®šä¹‰æ—¶é—´çª—å£
        time_windows = {
            '7å¤©å†…': timedelta(days=7),
            '30å¤©å†…': timedelta(days=30),
            '90å¤©å†…': timedelta(days=90),
            '6ä¸ªæœˆå†…': timedelta(days=180),
        }

        # 1. å„æ—¶é—´çª—å£çš„è¿‘æœŸçˆ†æ¬¾
        recent_viral = {}
        for window_name, delta in time_windows.items():
            cutoff = now - delta
            recent = [v for v in videos_with_date if v.published_at >= cutoff]

            if recent:
                # æŒ‰æ’­æ”¾é‡æ’åº
                recent_sorted = sorted(recent, key=lambda x: x.view_count, reverse=True)
                top_videos = []
                for v in recent_sorted[:10]:
                    days_old = (now - v.published_at).days
                    daily_growth = v.view_count / max(days_old, 1)
                    top_videos.append({
                        'title': v.title[:50] + '...' if len(v.title) > 50 else v.title,
                        'channel': v.channel_name,
                        'views': v.view_count,
                        'published_at': v.published_at.strftime('%Y-%m-%d'),
                        'days_old': days_old,
                        'daily_growth': int(daily_growth),
                        'url': v.url,
                    })

                recent_viral[window_name] = {
                    'video_count': len(recent),
                    'total_views': sum(v.view_count for v in recent),
                    'avg_views': int(sum(v.view_count for v in recent) / len(recent)),
                    'top_performers': top_videos,
                }

        # 2. é«˜æ—¥å‡å¢é•¿è§†é¢‘ï¼ˆæ‰€æœ‰æ—¶é—´çª—å£æ±‡æ€»ï¼‰
        high_growth = []
        for v in videos_with_date:
            days_old = (now - v.published_at).days
            if days_old < 1:
                continue
            daily_growth = v.view_count / days_old
            if daily_growth > 500 and v.view_count > 5000:  # æ—¥å‡ 500+ ä¸”æ€»é‡ > 5000
                high_growth.append({
                    'title': v.title[:50] + '...' if len(v.title) > 50 else v.title,
                    'channel': v.channel_name,
                    'views': v.view_count,
                    'published_at': v.published_at.strftime('%Y-%m-%d'),
                    'days_old': days_old,
                    'time_bucket': self._get_time_bucket(days_old),
                    'daily_growth': int(daily_growth),
                    'url': v.url,
                })

        high_growth.sort(key=lambda x: x['daily_growth'], reverse=True)

        # 3. å°é¢‘é“é»‘é©¬ï¼ˆ1-3 ä¸ªè§†é¢‘çš„é¢‘é“ä½†æœ‰çˆ†æ¬¾ï¼‰
        channel_videos = defaultdict(list)
        for v in self.videos:
            if v.channel_name:
                channel_videos[v.channel_name].append(v)

        small_channel_hits = []
        for channel, vids in channel_videos.items():
            if 1 <= len(vids) <= 3:
                for v in vids:
                    if v.view_count > 10000:  # æ’­æ”¾é‡ > 1 ä¸‡
                        days_old = (now - v.published_at).days if v.published_at else None
                        small_channel_hits.append({
                            'title': v.title[:50] + '...' if len(v.title) > 50 else v.title,
                            'channel': channel,
                            'channel_video_count': len(vids),
                            'views': v.view_count,
                            'published_at': v.published_at.strftime('%Y-%m-%d') if v.published_at else 'æœªçŸ¥',
                            'days_old': days_old,
                            'time_bucket': self._get_time_bucket(days_old) if days_old else 'æœªçŸ¥',
                            'url': v.url,
                        })

        small_channel_hits.sort(key=lambda x: x['views'], reverse=True)

        # 4. é«˜äº’åŠ¨ç‡æ¨¡æ¿ï¼ˆé€‚åˆ AI é…éŸ³ï¼‰
        high_engagement = []
        for v in self.videos:
            if v.like_count and v.view_count > 1000:
                engagement_rate = (v.like_count + (v.comment_count or 0)) / v.view_count * 100
                if engagement_rate > 3:  # äº’åŠ¨ç‡ > 3%
                    days_old = (now - v.published_at).days if v.published_at else None
                    high_engagement.append({
                        'title': v.title[:50] + '...' if len(v.title) > 50 else v.title,
                        'channel': v.channel_name,
                        'views': v.view_count,
                        'likes': v.like_count,
                        'comments': v.comment_count or 0,
                        'engagement_rate': round(engagement_rate, 2),
                        'published_at': v.published_at.strftime('%Y-%m-%d') if v.published_at else 'æœªçŸ¥',
                        'days_old': days_old,
                        'time_bucket': self._get_time_bucket(days_old) if days_old else 'æœªçŸ¥',
                        'url': v.url,
                    })

        high_engagement.sort(key=lambda x: x['engagement_rate'], reverse=True)

        # ç”Ÿæˆæœºä¼šæ€»ç»“
        summary = self._generate_opportunity_summary(
            recent_viral, high_growth, small_channel_hits, high_engagement
        )

        return {
            'analysis_time': now.strftime('%Y-%m-%d %H:%M:%S'),
            'data_time_range': {
                'videos_with_date': len(videos_with_date),
                'earliest': min(v.published_at for v in videos_with_date).strftime('%Y-%m-%d'),
                'latest': max(v.published_at for v in videos_with_date).strftime('%Y-%m-%d'),
            },
            'recent_viral_by_window': recent_viral,
            'high_daily_growth': {
                'count': len(high_growth),
                'description': 'æ—¥å‡æ’­æ”¾å¢é•¿ > 500 ä¸”æ€»æ’­æ”¾ > 5000 çš„è§†é¢‘',
                'top_performers': high_growth[:20],
            },
            'small_channel_hits': {
                'count': len(small_channel_hits),
                'description': '1-3ä¸ªè§†é¢‘çš„æ–°é¢‘é“ä¸­æ’­æ”¾é‡ > 1ä¸‡çš„çˆ†æ¬¾',
                'top_performers': small_channel_hits[:20],
            },
            'high_engagement_templates': {
                'count': len(high_engagement),
                'description': 'äº’åŠ¨ç‡ > 3% çš„é«˜äº’åŠ¨è§†é¢‘ï¼ˆé€‚åˆ AI é…éŸ³ï¼‰',
                'top_performers': high_engagement[:20],
            },
            'opportunity_summary': summary,
        }

    def _get_time_bucket(self, days: int) -> str:
        """å°†å¤©æ•°è½¬æ¢ä¸ºæ—¶é—´æ¡¶æ ‡ç­¾"""
        if days < 1:
            return '24å°æ—¶å†…'
        elif days < 7:
            return '7å¤©å†…'
        elif days < 30:
            return '30å¤©å†…'
        elif days < 90:
            return '90å¤©å†…'
        elif days < 180:
            return '6ä¸ªæœˆå†…'
        elif days < 365:
            return '1å¹´å†…'
        else:
            return '1å¹´ä»¥ä¸Š'

    def _generate_opportunity_summary(
        self,
        recent_viral: Dict,
        high_growth: List,
        small_channel_hits: List,
        high_engagement: List
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæœºä¼šæ€»ç»“"""

        # ç»Ÿè®¡å„æ—¶é—´æ®µçš„æœºä¼šæ•°é‡
        time_bucket_counts = defaultdict(int)
        for item in high_growth:
            time_bucket_counts[item['time_bucket']] += 1
        for item in small_channel_hits:
            if item['time_bucket'] != 'æœªçŸ¥':
                time_bucket_counts[item['time_bucket']] += 1

        recommendations = []

        # åŸºäºæ•°æ®ç»™å‡ºå»ºè®®
        if recent_viral.get('30å¤©å†…', {}).get('video_count', 0) > 5:
            recommendations.append(f"30å¤©å†…æœ‰ {recent_viral['30å¤©å†…']['video_count']} ä¸ªæ–°è§†é¢‘ï¼Œå¸‚åœºæ´»è·ƒåº¦è¾ƒé«˜")

        if high_growth:
            avg_daily = sum(v['daily_growth'] for v in high_growth[:10]) / min(10, len(high_growth))
            recommendations.append(f"é«˜å¢é•¿è§†é¢‘æ—¥å‡æ’­æ”¾ {int(avg_daily):,}ï¼Œå­˜åœ¨å¿«é€Ÿèµ·é‡æœºä¼š")

        if small_channel_hits:
            recent_hits = [v for v in small_channel_hits if v.get('days_old') and v['days_old'] < 90]
            if recent_hits:
                recommendations.append(f"è¿‘90å¤©æœ‰ {len(recent_hits)} ä¸ªå°é¢‘é“çˆ†æ¬¾ï¼Œæ–°é¢‘é“æœ‰æœºä¼š")

        if high_engagement:
            recommendations.append(f"å‘ç° {len(high_engagement)} ä¸ªé«˜äº’åŠ¨æ¨¡æ¿ï¼Œé€‚åˆ AI é…éŸ³å¤åˆ¶")

        return {
            'opportunities_by_time': dict(time_bucket_counts),
            'best_time_window': max(time_bucket_counts.items(), key=lambda x: x[1])[0] if time_bucket_counts else 'æœªçŸ¥',
            'recommendations': recommendations,
            'action_items': [
                'ä¼˜å…ˆæ¨¡ä»¿ 30 å¤©å†…çš„è¿‘æœŸçˆ†æ¬¾',
                'å…³æ³¨æ—¥å¢é•¿ > 1000 çš„é«˜é€Ÿå¢é•¿è§†é¢‘',
                'å‚è€ƒå°é¢‘é“çˆ†æ¬¾çš„é€‰é¢˜å’Œæ ‡é¢˜',
                'ä½¿ç”¨é«˜äº’åŠ¨ç‡è§†é¢‘ä½œä¸º AI é…éŸ³æ¨¡æ¿',
            ],
        }

    def save_report(self, report: MarketReport, output_path: str = None) -> str:
        """ä¿å­˜æŠ¥å‘Š"""
        if output_path is None:
            output_dir = Path("data/analysis")
            output_dir.mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = output_dir / f"market_report_{timestamp}.json"

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                'generated_at': report.generated_at,
                'sample_size': report.sample_size,
                'time_context': report.time_context,
                'market_size': report.market_size,
                'channel_competition': report.channel_competition,
                'publishing_frequency': report.publishing_frequency,
                'time_trends': report.time_trends,
                'entry_barriers': report.entry_barriers,
                'ai_creator_opportunities': report.ai_creator_opportunities,
            }, f, ensure_ascii=False, indent=2)

        return str(output_path)

    def print_summary(self, report: MarketReport):
        """æ‰“å°æ‘˜è¦"""
        print("=" * 60)
        print("ğŸ“Š å¸‚åœºè§„æ¨¡ä¸ç«äº‰æ ¼å±€åˆ†ææŠ¥å‘Š")
        print("=" * 60)
        print()

        # å¸‚åœºè§„æ¨¡
        ms = report.market_size
        print("ã€1. å¸‚åœºè§„æ¨¡ã€‘")
        print(f"  æ ·æœ¬è§†é¢‘æ•°: {ms['sample_videos']:,}")
        print(f"  æ€»æ’­æ”¾é‡: {ms['total_views']:,}")
        print(f"  å¹³å‡æ’­æ”¾é‡: {ms['avg_views']:,}")
        print(f"  ä¸­ä½æ•°æ’­æ”¾é‡: {ms['median_views']:,}")
        print()

        # é¢‘é“ç«äº‰
        cc = report.channel_competition
        print("ã€2. é¢‘é“ç«äº‰æ ¼å±€ã€‘")
        print(f"  æ€»é¢‘é“æ•°: {cc['total_channels']}")
        print(f"  å•è§†é¢‘é¢‘é“: {cc['size_distribution']['single_video']} ({cc['size_distribution']['single_video']/cc['total_channels']*100:.1f}%)")
        print(f"  Top10 é›†ä¸­åº¦: {cc['concentration']['top10_share']}%")
        print(f"  Top20 é›†ä¸­åº¦: {cc['concentration']['top20_share']}%")
        print()
        print("  å¤´éƒ¨é¢‘é“:")
        for i, ch in enumerate(cc['top_channels'][:5], 1):
            print(f"    {i}. {ch['channel'][:20]:20} | {ch['video_count']:3} è§†é¢‘ | {ch['total_views']:>10,} æ’­æ”¾")
        print()

        # å‘å¸ƒé¢‘ç‡
        pf = report.publishing_frequency
        if 'frequency' in pf:
            print("ã€3. å‘å¸ƒé¢‘ç‡ã€‘")
            print(f"  æ—¥å‡å‘å¸ƒ: {pf['frequency']['daily_avg']} è§†é¢‘/å¤©")
            print(f"  æœˆå‡å‘å¸ƒ: {pf['frequency']['monthly_avg']} è§†é¢‘/æœˆ")
            if pf.get('weekday_distribution'):
                best_day = max(pf['weekday_distribution'].items(), key=lambda x: x[1])
                print(f"  æœ€æ´»è·ƒæ—¥: {best_day[0]} ({best_day[1]} è§†é¢‘)")
            print()

        # è¿›å…¥å£å’
        eb = report.entry_barriers
        print("ã€4. è¿›å…¥å£å’ã€‘")
        print(f"  çˆ†æ¬¾ç‡(100ä¸‡+): {eb['success_rate']['viral_rate']}%")
        print(f"  ä¼˜ç§€ç‡(10ä¸‡+): {eb['success_rate']['excellent_rate']}%")
        print(f"  Top 10% é—¨æ§›: {eb['thresholds']['top_10_percent']:,} æ’­æ”¾")
        print()
        print("  æ’­æ”¾é‡åˆ†å±‚:")
        for tier, count in eb['performance_tiers'].items():
            pct = count / report.sample_size * 100
            bar = 'â–ˆ' * int(pct / 2)
            print(f"    {tier:15}: {count:4} ({pct:5.1f}%) {bar}")

        print()
        print("=" * 60)


def analyze_market(db_path: str = None, enrich_dates: bool = False) -> MarketReport:
    """ä¾¿æ·å‡½æ•°ï¼šæ‰§è¡Œå¸‚åœºåˆ†æ"""
    analyzer = MarketAnalyzer(db_path)
    analyzer.load_data()

    if enrich_dates:
        analyzer.enrich_publish_dates(limit=200)
        analyzer.load_data()  # é‡æ–°åŠ è½½

    return analyzer.analyze()

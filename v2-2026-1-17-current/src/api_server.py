#!/usr/bin/env python3
"""
YouTube å†…å®¹åŠ©æ‰‹ API æœåŠ¡å™¨

æä¾› WebSocket å’Œ HTTP æ¥å£ï¼Œæ”¯æŒï¼š
- WebSocket å®æ—¶æ•°æ®é‡‡é›†å’Œè¿›åº¦æ¨é€
- HTTP ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢ï¼ˆä½œä¸º WebSocket æ–­å¼€åçš„è¡¥æ•‘æœºåˆ¶ï¼‰
- è¶‹åŠ¿çƒ­è¯è·å–
"""

import asyncio
import json
import queue
import sys
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
from dataclasses import dataclass, field, asdict
from enum import Enum
import uuid
import sqlite3
from src.shared.db_compat import get_connection as db_get_connection, db_exists, is_using_neon

# é¢„åŠ è½½ jiebaï¼ˆé¿å…æ¯æ¬¡è¯·æ±‚å†·å¯åŠ¨ï¼‰
try:
    import jieba
    jieba.initialize()
except Exception:
    pass

# åˆ†æç»“æœç¼“å­˜ï¼ˆVercel å‡½æ•°å®ä¾‹çº§åˆ«ï¼ŒåŒä¸€å®ä¾‹å¤ç”¨ï¼‰
_analyze_cache = {}
_CACHE_TTL = 600  # 10 åˆ†é’Ÿç¼“å­˜

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
import uvicorn

# æƒ°æ€§å¯¼å…¥ï¼šé¿å…åœ¨ Vercel ç¯å¢ƒä¸­å›  yt-dlp ä¸å¯ç”¨è€Œå¤±è´¥
# DataCollector å’Œ CompetitorVideoRepository åªåœ¨éœ€è¦æ—¶æ‰å¯¼å…¥
DataCollector = None
CompetitorVideoRepository = None


def _get_data_collector():
    """æƒ°æ€§åŠ è½½ DataCollector"""
    global DataCollector
    if DataCollector is None:
        from src.research.data_collector import DataCollector as DC
        DataCollector = DC
    return DataCollector()


def _get_repository(db_path=None):
    """æƒ°æ€§åŠ è½½ CompetitorVideoRepository"""
    global CompetitorVideoRepository
    if CompetitorVideoRepository is None:
        from src.shared.repositories import CompetitorVideoRepository as Repo
        CompetitorVideoRepository = Repo
    return CompetitorVideoRepository(db_path)


# ============================================================
# ä»»åŠ¡çŠ¶æ€ç®¡ç†
# ============================================================

class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETE = "complete"
    ERROR = "error"
    NOT_FOUND = "not_found"


@dataclass
class TaskState:
    """ä»»åŠ¡çŠ¶æ€"""
    task_id: str
    status: TaskStatus = TaskStatus.PENDING
    progress: int = 0
    message: str = ""
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "task_id": self.task_id,
            "status": self.status.value,
            "progress": self.progress,
            "message": self.message,
            "result": self.result,
            "error": self.error,
            "created_at": self.created_at,
            "updated_at": self.updated_at,
        }


# ä»»åŠ¡å­˜å‚¨ï¼ˆå†…å­˜ä¸­ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redisï¼‰
task_store: Dict[str, TaskState] = {}

# ä»»åŠ¡è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
TASK_EXPIRY = 3600  # 1å°æ—¶


def cleanup_expired_tasks():
    """æ¸…ç†è¿‡æœŸä»»åŠ¡"""
    now = time.time()
    expired = [
        task_id for task_id, task in task_store.items()
        if now - task.updated_at > TASK_EXPIRY
    ]
    for task_id in expired:
        del task_store[task_id]


# ============================================================
# FastAPI åº”ç”¨
# ============================================================

app = FastAPI(
    title="YouTube Content Assistant API",
    description="YouTube å†…å®¹åŠ©æ‰‹ API æœåŠ¡å™¨",
    version="1.0.0"
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶æ¥æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é™æ€æ–‡ä»¶æœåŠ¡ - æä¾› web ç›®å½•ä¸‹çš„å¯è§†åŒ–é¡µé¢
web_dir = Path(__file__).parent / "web"
if web_dir.exists():
    app.mount("/dashboard", StaticFiles(directory=str(web_dir), html=True), name="dashboard")

# é™æ€æ–‡ä»¶æœåŠ¡ - æä¾› public ç›®å½•ä¸‹çš„åˆ†ææŠ¥å‘Šé¡µé¢
public_dir = Path(__file__).parent / "public"
if public_dir.exists():
    app.mount("/report", StaticFiles(directory=str(public_dir), html=True), name="report")


# ============================================================
# HTTP ç«¯ç‚¹
# ============================================================

@app.get("/")
async def root():
    """è¿”å›å‰ç«¯é¡µé¢"""
    demo_path = Path(__file__).parent / "demo.html"
    if demo_path.exists():
        return FileResponse(demo_path, media_type="text/html")
    return {"status": "ok", "message": "YouTube Content Assistant API"}


@app.get("/creator")
async def creator_dashboard():
    """è¿”å›åˆ›ä½œè€…å‹å¥½ç‰ˆç•Œé¢"""
    creator_path = Path(__file__).parent / "creator-dashboard.html"
    if creator_path.exists():
        return FileResponse(creator_path, media_type="text/html")
    return {"status": "error", "message": "åˆ›ä½œè€…ä»ªè¡¨ç›˜é¡µé¢ä¸å­˜åœ¨"}


@app.get("/api/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "message": "YouTube Content Assistant API"}


@app.get("/api/task/{task_id}/status")
async def get_task_status(task_id: str):
    """
    è·å–ä»»åŠ¡çŠ¶æ€

    è¿™æ˜¯ WebSocket æ–­å¼€åçš„è¡¥æ•‘æœºåˆ¶ï¼Œå‰ç«¯å¯ä»¥è½®è¯¢æ­¤æ¥å£è·å–ç»“æœ
    """
    cleanup_expired_tasks()

    if task_id not in task_store:
        return JSONResponse(
            status_code=404,
            content={"status": "not_found", "message": "ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"}
        )

    task = task_store[task_id]
    return task.to_dict()


@app.get("/api/statistics")
async def get_statistics():
    """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        repo = _get_repository()
        stats = repo.get_statistics()
        return {"status": "ok", "data": stats}
    except Exception as e:
        return {"status": "error", "message": str(e)}


# ============================================================
# WebSocket ç«¯ç‚¹
# ============================================================

@app.websocket("/ws/{task_id}")
async def websocket_endpoint(websocket: WebSocket, task_id: str):
    """
    WebSocket è¿æ¥ç«¯ç‚¹

    åè®®ï¼š
    1. å®¢æˆ·ç«¯è¿æ¥åå‘é€ç ”ç©¶å‚æ•°
    2. æœåŠ¡å™¨æ¨é€è¿›åº¦æ›´æ–°
    3. æœåŠ¡å™¨æ¨é€æœ€ç»ˆç»“æœ
    4. æŒç»­å“åº”å¿ƒè·³ä¿æŒè¿æ¥
    """
    await websocket.accept()

    # åˆ›å»ºä»»åŠ¡çŠ¶æ€
    task_store[task_id] = TaskState(task_id=task_id)

    # æœåŠ¡å™¨ä¸»åŠ¨å¿ƒè·³ä»»åŠ¡ï¼ˆä¿æŒè¿æ¥æ´»è·ƒï¼‰
    heartbeat_task = None
    stop_heartbeat = False

    async def server_heartbeat():
        """æœåŠ¡å™¨ä¸»åŠ¨å‘é€å¿ƒè·³ï¼Œä¿æŒè¿æ¥æ´»è·ƒ"""
        nonlocal stop_heartbeat
        while not stop_heartbeat:
            try:
                await asyncio.sleep(15)  # æ¯ 15 ç§’å‘é€ä¸€æ¬¡
                if not stop_heartbeat:
                    await websocket.send_json({
                        "type": "pong",
                        "timestamp": time.time(),
                        "server_ping": True
                    })
            except Exception as e:
                if not stop_heartbeat:
                    print(f"æœåŠ¡å™¨å¿ƒè·³å‘é€å¤±è´¥: {e}")
                break

    try:
        # ç­‰å¾…å®¢æˆ·ç«¯å‘é€å‚æ•°
        data = await asyncio.wait_for(websocket.receive_text(), timeout=30)
        params = json.loads(data)

        # å¤„ç†å¿ƒè·³ï¼ˆé¦–æ¬¡è¿æ¥æ—¶å¯èƒ½å…ˆæ”¶åˆ°å¿ƒè·³ï¼‰
        while params.get("type") == "ping":
            await websocket.send_json({"type": "pong", "timestamp": time.time()})
            print(f"ğŸ’“ å“åº”åˆå§‹å¿ƒè·³: {task_id}")
            data = await asyncio.wait_for(websocket.receive_text(), timeout=30)
            params = json.loads(data)

        # æå–å‚æ•°
        topic = params.get("topic", "")
        video_count = params.get("video_count", 100)
        time_range = params.get("time_range", "month")
        sort_by = params.get("sort_by", "views")

        if not topic:
            await websocket.send_json({
                "type": "error",
                "message": "ç¼ºå°‘ä¸»é¢˜å‚æ•°"
            })
            return

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task = task_store[task_id]
        task.status = TaskStatus.RUNNING
        task.message = f"æ­£åœ¨åˆ†æ: {topic}"
        task.updated_at = time.time()

        # å‘é€å¼€å§‹æ¶ˆæ¯
        await websocket.send_json({
            "type": "status",
            "message": f"æ­£åœ¨åˆ†æ: {topic}",
            "progress": 0
        })

        # å¯åŠ¨æœåŠ¡å™¨å¿ƒè·³ä»»åŠ¡ï¼ˆä¸æ•°æ®é‡‡é›†å¹¶è¡Œè¿è¡Œï¼Œä¿æŒè¿æ¥æ´»è·ƒï¼‰
        heartbeat_task = asyncio.create_task(server_heartbeat())

        # æ‰§è¡Œæ•°æ®é‡‡é›†ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œä»¥é¿å…é˜»å¡ï¼‰
        result = await run_data_collection(
            websocket, task_id, topic, video_count, time_range, sort_by
        )

        # åœæ­¢å¿ƒè·³å“åº”
        stop_heartbeat = True
        if heartbeat_task:
            heartbeat_task.cancel()
            try:
                await heartbeat_task
            except asyncio.CancelledError:
                pass

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ˆæ— è®º WebSocket æ˜¯å¦æ–­å¼€ï¼Œéƒ½ä¿å­˜ç»“æœï¼‰
        task.status = TaskStatus.COMPLETE
        task.result = result
        task.progress = 100
        task.message = "é‡‡é›†å®Œæˆ"
        task.updated_at = time.time()

        # å°è¯•å‘é€æœ€ç»ˆç»“æœï¼ˆWebSocket å¯èƒ½å·²æ–­å¼€ï¼‰
        try:
            await websocket.send_json({
                "type": "complete",
                "result": result
            })
        except Exception as send_err:
            print(f"å‘é€å®Œæˆæ¶ˆæ¯å¤±è´¥ï¼ˆWebSocket å¯èƒ½å·²æ–­å¼€ï¼‰: {send_err}")
            # ç»“æœå·²ä¿å­˜åˆ° task_storeï¼Œå‰ç«¯å¯é€šè¿‡ HTTP è½®è¯¢è·å–

    except WebSocketDisconnect:
        print(f"WebSocket æ–­å¼€: {task_id}")
        stop_heartbeat = True
        # ä»»åŠ¡çŠ¶æ€ä¿æŒï¼Œè®© HTTP è½®è¯¢å¯ä»¥è·å–ç»“æœ

    except asyncio.TimeoutError:
        try:
            await websocket.send_json({
                "type": "error",
                "message": "ç­‰å¾…å‚æ•°è¶…æ—¶"
            })
        except:
            pass
        task_store[task_id].status = TaskStatus.ERROR
        task_store[task_id].error = "ç­‰å¾…å‚æ•°è¶…æ—¶"

    except Exception as e:
        error_msg = str(e)
        print(f"WebSocket é”™è¯¯: {error_msg}")
        traceback.print_exc()

        # åªæœ‰åœ¨ä»»åŠ¡æœªå®Œæˆæ—¶æ‰è®¾ç½®ä¸ºé”™è¯¯çŠ¶æ€
        task = task_store.get(task_id)
        if task and task.status != TaskStatus.COMPLETE:
            task.status = TaskStatus.ERROR
            task.error = error_msg

        try:
            await websocket.send_json({
                "type": "error",
                "message": error_msg
            })
        except:
            pass


async def run_data_collection(
    websocket: WebSocket,
    task_id: str,
    topic: str,
    video_count: int,
    time_range: str,
    sort_by: str
) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ•°æ®é‡‡é›†

    ä½¿ç”¨ asyncio.to_thread åœ¨åå°çº¿ç¨‹è¿è¡Œ DataCollector
    """
    # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„é˜Ÿåˆ—ï¼ˆqueue.Queue æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼‰
    progress_queue = queue.Queue()

    # åœæ­¢æ ‡å¿—
    stop_consumer = False

    # æ—¶é—´ä¼°ç®—çŠ¶æ€
    stage_start_times: Dict[str, float] = {}
    stage_item_times: Dict[str, list] = {}

    async def send_progress(stage: str, current: int, total: int, message: str = "", extra: dict = None):
        """å‘é€è¿›åº¦æ›´æ–°"""
        progress = int((current / total) * 100) if total > 0 else 0
        now = time.time()

        # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
        estimated_remaining = None
        if stage not in stage_start_times:
            stage_start_times[stage] = now
            stage_item_times[stage] = []

        if current > 0:
            # è®°å½•æ¯ä¸ªé¡¹ç›®çš„å¤„ç†æ—¶é—´
            if len(stage_item_times[stage]) < current:
                elapsed = now - stage_start_times[stage]
                avg_time_per_item = elapsed / current
                remaining_items = total - current
                estimated_remaining = avg_time_per_item * remaining_items
                stage_item_times[stage].append(avg_time_per_item)

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task = task_store.get(task_id)
        if task:
            task.progress = progress
            task.message = message or f"{stage}: {current}/{total}"
            task.updated_at = now

        # æ„å»ºè¿›åº¦æ¶ˆæ¯
        progress_data = {
            "type": "progress",
            "stage": stage,
            "current": current,
            "total": total,
            "progress": progress,
            "message": message or f"{stage}: {current}/{total}"
        }

        # æ·»åŠ æ—¶é—´ä¼°ç®—ï¼ˆå‰ç«¯æœŸæœ›çš„æ ¼å¼ï¼‰
        if estimated_remaining is not None and estimated_remaining > 0:
            if stage == "detail":
                progress_data["channel_fetch"] = {
                    "estimated_remaining_seconds": estimated_remaining
                }
            else:
                progress_data["estimated_remaining_seconds"] = estimated_remaining

        # æ·»åŠ å®æ—¶ç»Ÿè®¡ï¼ˆè§†é¢‘æ•°ã€é¢‘é“æ•°ï¼‰
        if extra:
            progress_data.update(extra)

        try:
            await websocket.send_json(progress_data)
        except:
            pass  # WebSocket å¯èƒ½å·²æ–­å¼€

    def sync_progress_callback(stage: str, current: int, total: int, extra: dict = None):
        """åŒæ­¥è¿›åº¦å›è°ƒï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è°ƒç”¨ï¼‰"""
        try:
            # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„ queue.Queue
            progress_queue.put((stage, current, total, extra or {}))
        except:
            pass

    # å¯åŠ¨è¿›åº¦æ¶ˆè´¹è€…
    async def progress_consumer():
        nonlocal stop_consumer
        while not stop_consumer:
            try:
                # éé˜»å¡æ–¹å¼æ£€æŸ¥é˜Ÿåˆ—
                try:
                    item = progress_queue.get_nowait()
                    if len(item) == 4:
                        stage, current, total, extra = item
                    else:
                        stage, current, total = item
                        extra = {}
                    await send_progress(stage, current, total, extra=extra)
                except queue.Empty:
                    # é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…ä¸€å°æ®µæ—¶é—´
                    await asyncio.sleep(0.1)
            except Exception as e:
                print(f"è¿›åº¦æ¶ˆè´¹è€…é”™è¯¯: {e}")
                await asyncio.sleep(0.1)

    progress_task = asyncio.create_task(progress_consumer())

    try:
        # å‘é€å¼€å§‹çŠ¶æ€
        await send_progress("åˆå§‹åŒ–", 0, 1, "æ­£åœ¨åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨...")

        # åœ¨åå°çº¿ç¨‹æ‰§è¡Œæ•°æ®é‡‡é›†
        def collect_data():
            collector = _get_data_collector()

            # è®¡ç®—å‚æ•°
            detail_limit = min(video_count // 2, 100)
            detail_min_views = 5000

            # åˆ›å»ºå¸¦å®æ—¶ç»Ÿè®¡çš„è¿›åº¦å›è°ƒ
            def progress_with_stats(stage: str, current: int, total: int):
                """å¸¦å®æ—¶ç»Ÿè®¡çš„è¿›åº¦å›è°ƒ"""
                # æŸ¥è¯¢å½“å‰ä¸»é¢˜ç›¸å…³çš„è§†é¢‘æ•°å’Œé¢‘é“æ•°
                try:
                    videos = collector.get_videos_from_db(
                        keyword_like=topic,
                        min_views=0,
                        limit=10000
                    )
                    video_count_now = len(videos)

                    # ç»Ÿè®¡é¢‘é“ï¼ˆå»é‡ï¼‰
                    channels = set()
                    for v in videos:
                        if v.channel_id and v.channel_id != 'None':
                            channels.add(v.channel_id)
                        elif v.channel_name:
                            channels.add(v.channel_name)
                    channel_count_now = len(channels)

                    # è°ƒç”¨åŸå§‹å›è°ƒï¼Œé™„å¸¦å®æ—¶ç»Ÿè®¡
                    sync_progress_callback(stage, current, total, {
                        "realtime_stats": {
                            "videos": video_count_now,
                            "channels": channel_count_now
                        }
                    })
                except Exception as e:
                    # å³ä½¿ç»Ÿè®¡å¤±è´¥ä¹Ÿç»§ç»­
                    sync_progress_callback(stage, current, total, {})

            # æ‰§è¡Œå¤§è§„æ¨¡é‡‡é›†
            result = collector.collect_large_scale(
                theme=topic,
                target_count=video_count,
                detail_min_views=detail_min_views,
                detail_limit=detail_limit,
                time_range=time_range,
                on_progress=progress_with_stats
            )

            # è·å–è§†é¢‘åˆ—è¡¨ï¼ˆä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼ŒåŒ…å«æ‰€æœ‰æ‰©å±•å…³é”®è¯çš„è§†é¢‘ï¼‰
            # ä¾‹å¦‚æœç´¢ "å…»ç”Ÿ"ï¼Œä¼šåŒ¹é… "å…»ç”Ÿ"ã€"ä¸­åŒ»å…»ç”Ÿ"ã€"å¥åº·å…»ç”Ÿ" ç­‰æ‰€æœ‰ç›¸å…³å…³é”®è¯

            # å…ˆè·å–å®é™…æœç´¢åˆ°çš„å…¨éƒ¨è§†é¢‘ï¼ˆä¸å—ç­›é€‰æ¡ä»¶é™åˆ¶ï¼‰ç”¨äºç»Ÿè®¡
            all_matched_videos = collector.get_videos_from_db(
                keyword_like=topic,  # æ¨¡ç³ŠåŒ¹é…ï¼ŒåŒ…å«ä¸»é¢˜è¯çš„æ‰€æœ‰è§†é¢‘
                min_views=0,  # ä¸é™åˆ¶æ’­æ”¾é‡
                limit=10000   # è¶³å¤Ÿå¤§çš„æ•°é‡
            )

            # ç»Ÿè®¡é¢‘é“æ•°é‡ï¼ˆå»é‡ï¼Œä¼˜å…ˆä½¿ç”¨ channel_idï¼Œå¦åˆ™ç”¨ channel_nameï¼‰
            channels = set()
            for v in all_matched_videos:
                # ä¼˜å…ˆç”¨ channel_idï¼Œå¦‚æœæ²¡æœ‰æˆ–è€…æ˜¯ 'None' å­—ç¬¦ä¸²ï¼Œåˆ™ç”¨ channel_name
                if v.channel_id and v.channel_id != 'None':
                    channels.add(v.channel_id)
                elif v.channel_name:
                    channels.add(v.channel_name)

            actual_total_videos = len(all_matched_videos)
            actual_total_channels = len(channels)

            # è·å–ç­›é€‰åçš„è§†é¢‘ï¼ˆç”¨äºå±•ç¤ºï¼‰
            videos = collector.get_videos_from_db(
                keyword_like=topic,
                min_views=1000,
                limit=video_count * 2  # å¤šå–ä¸€äº›ï¼Œåé¢ä¼šå»é‡æ’åº
            )

            # æŒ‰æ’åºæ–¹å¼æ’åº
            if sort_by == "views":
                videos.sort(key=lambda v: v.view_count or 0, reverse=True)
            elif sort_by == "likes":
                videos.sort(key=lambda v: v.like_count or 0, reverse=True)
            elif sort_by == "date":
                videos.sort(key=lambda v: v.published_at or "", reverse=True)
            elif sort_by == "engagement":
                videos.sort(
                    key=lambda v: ((v.like_count or 0) + (v.comment_count or 0)) / max(v.view_count or 1, 1),
                    reverse=True
                )

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            video_list = []
            for v in videos[:video_count]:
                video_list.append({
                    "youtube_id": v.youtube_id,
                    "title": v.title,
                    "channel_name": v.channel_name,
                    "channel_id": v.channel_id,
                    "view_count": v.view_count or 0,
                    "like_count": v.like_count or 0,
                    "comment_count": v.comment_count or 0,
                    "duration": v.duration or 0,
                    "published_at": v.published_at.isoformat() if v.published_at else None,
                    "thumbnail_url": v.thumbnail_url,
                    "description": v.description[:200] if v.description else None,
                    "tags": v.tags or [],
                })

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_views = sum(v["view_count"] for v in video_list)
            total_likes = sum(v["like_count"] for v in video_list)
            total_comments = sum(v["comment_count"] for v in video_list)

            # è·å–æ•°æ®åº“ä¸­è¯¥ä¸»é¢˜çš„æœ€åæ›´æ–°æ—¶é—´
            db_stats = collector.get_statistics()

            return {
                "topic": topic,
                "total_videos": actual_total_videos,  # å®é™…æœç´¢åˆ°çš„è§†é¢‘æ€»æ•°ï¼ˆä¸å—ç­›é€‰é™åˆ¶ï¼‰
                "total_channels": actual_total_channels,  # å®é™…æœç´¢åˆ°çš„é¢‘é“æ€»æ•°
                "displayed_videos": len(video_list),  # å½“å‰å±•ç¤ºçš„è§†é¢‘æ•°é‡
                "total_views": total_views,
                "total_likes": total_likes,
                "total_comments": total_comments,
                "avg_views": total_views // len(video_list) if video_list else 0,
                "avg_likes": total_likes // len(video_list) if video_list else 0,
                "videos": video_list,
                "collection_stats": result,
                "collected_at": datetime.now().isoformat(),

                # æ•°æ®åŸºæ•°ä¿¡æ¯ï¼ˆç”¨æˆ·æ–°éœ€æ±‚ï¼‰
                "data_basis": {
                    "total_videos_in_db": db_stats.get('total', 0),  # æ•°æ®åº“æ€»è§†é¢‘æ•°
                    "videos_for_this_topic": actual_total_videos,     # æœ¬ä¸»é¢˜è§†é¢‘æ•°
                    "ranking_basis": len(video_list),                 # æ’åä¾æ®çš„è§†é¢‘æ•°
                    "description": f"æœ¬æ’ååŸºäº {actual_total_videos} ä¸ªè§†é¢‘æ•°æ®ç­›é€‰"
                },
                "last_updated": datetime.now().isoformat(),  # æ•°æ®æ›´æ–°æ—¶é—´
            }

        # æ‰§è¡Œé‡‡é›†
        result = await asyncio.to_thread(collect_data)

        return result

    finally:
        stop_consumer = True
        progress_task.cancel()
        try:
            await progress_task
        except asyncio.CancelledError:
            pass


# ============================================================
# ä¸»é¢˜æ•°æ®æŸ¥è¯¢ï¼ˆç›´æ¥æŸ¥çœ‹å·²æœ‰æ•°æ®ï¼‰
# ============================================================

@app.get("/api/themes")
async def get_themes():
    """è·å–æ•°æ®åº“ä¸­çš„ä¸»é¢˜åˆ—è¡¨"""
    try:
        repo = _get_repository()
        conn = repo._get_connection()
        cursor = conn.cursor()
        cursor.execute('''
            SELECT theme, COUNT(*) as count
            FROM competitor_videos
            WHERE theme IS NOT NULL
            GROUP BY theme
            ORDER BY count DESC
        ''')
        themes = [{"theme": row[0], "count": row[1]} for row in cursor.fetchall()]
        conn.close()
        return {"status": "ok", "themes": themes}
    except Exception as e:
        return {"status": "error", "message": str(e)}


@app.get("/api/analyze/{theme}")
async def analyze_theme(
    theme: str,
    limit: int = 100,
    sort_by: str = "views",
    min_views: int = 1000,
    date_from: str = None,
    date_to: str = None
):
    """
    ç›´æ¥åˆ†æå·²æœ‰æ•°æ®ï¼ˆä¸è§¦å‘æ–°æœç´¢ï¼‰

    Args:
        theme: ä¸»é¢˜åç§°ï¼ˆå¦‚"å…»ç”Ÿ"ã€"ç§‘æŠ€"ï¼‰
        limit: è¿”å›è§†é¢‘æ•°é‡
        sort_by: æ’åºæ–¹å¼ (views/likes/date/engagement)
        min_views: æœ€å°æ’­æ”¾é‡ç­›é€‰
        date_from: å‘å¸ƒæ—¶é—´èµ·å§‹ï¼ˆYYYY-MM-DDï¼‰
        date_to: å‘å¸ƒæ—¶é—´æˆªæ­¢ï¼ˆYYYY-MM-DDï¼‰
    """
    try:
        # ç¼“å­˜æ£€æŸ¥ï¼ˆåŒä¸€å‡½æ•°å®ä¾‹å†…å¤ç”¨ç»“æœï¼‰
        cache_key = f"{theme}_{limit}_{sort_by}_{min_views}_{date_from}_{date_to}"
        if cache_key in _analyze_cache:
            cached_time, cached_result = _analyze_cache[cache_key]
            if time.time() - cached_time < _CACHE_TTL:
                return cached_result

        collector = _get_data_collector()

        # ä½¿ç”¨ theme ç²¾ç¡®åŒ¹é…ï¼ˆæ–°çš„æŸ¥è¯¢æ–¹å¼ï¼‰
        all_videos = collector.get_videos_from_db(
            theme=theme,
            min_views=0,
            limit=10000
        )

        if not all_videos:
            # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            all_videos = collector.get_videos_from_db(
                keyword_like=theme,
                min_views=0,
                limit=10000
            )

        if not all_videos:
            return {
                "status": "error",
                "message": f"æ²¡æœ‰æ‰¾åˆ°ä¸»é¢˜ '{theme}' çš„æ•°æ®ï¼Œè¯·å…ˆæœç´¢æ”¶é›†æ•°æ®"
            }

        # è®¡ç®—æ•°æ®çš„æ—¶é—´èŒƒå›´ï¼ˆè§†é¢‘å‘å¸ƒæ—¶é—´ï¼‰
        published_dates = [v.published_at for v in all_videos if v.published_at]
        collected_dates = [v.collected_at for v in all_videos if v.collected_at]

        data_time_range = {
            "published_earliest": min(published_dates).strftime("%Y-%m-%d") if published_dates else None,
            "published_latest": max(published_dates).strftime("%Y-%m-%d") if published_dates else None,
            "collected_earliest": min(collected_dates).strftime("%Y-%m-%d %H:%M") if collected_dates else None,
            "collected_latest": max(collected_dates).strftime("%Y-%m-%d %H:%M") if collected_dates else None,
        }

        # ç»Ÿè®¡é¢‘é“
        channels = set()
        for v in all_videos:
            if v.channel_id and v.channel_id != 'None':
                channels.add(v.channel_id)
            elif v.channel_name:
                channels.add(v.channel_name)

        actual_total_videos = len(all_videos)
        actual_total_channels = len(channels)

        # ç­›é€‰è§†é¢‘
        valid_videos = all_videos

        # æ’­æ”¾é‡ç­›é€‰
        if min_views > 0:
            valid_videos = [v for v in valid_videos if (v.view_count or 0) >= min_views]

        # æ—¥æœŸç­›é€‰
        if date_from:
            try:
                from_date = datetime.strptime(date_from, "%Y-%m-%d")
                valid_videos = [v for v in valid_videos if v.published_at and v.published_at >= from_date]
            except ValueError:
                pass

        if date_to:
            try:
                to_date = datetime.strptime(date_to, "%Y-%m-%d")
                # åŒ…å«å½“å¤©ç»“æŸ
                to_date = to_date.replace(hour=23, minute=59, second=59)
                valid_videos = [v for v in valid_videos if v.published_at and v.published_at <= to_date]
            except ValueError:
                pass

        filtered_count = len(valid_videos)

        # æ’åº
        if sort_by == "views":
            valid_videos.sort(key=lambda v: v.view_count or 0, reverse=True)
        elif sort_by == "likes":
            valid_videos.sort(key=lambda v: v.like_count or 0, reverse=True)
        elif sort_by == "date":
            valid_videos.sort(key=lambda v: str(v.published_at or ""), reverse=True)
        elif sort_by == "engagement":
            valid_videos.sort(
                key=lambda v: ((v.like_count or 0) + (v.comment_count or 0)) / max(v.view_count or 1, 1),
                reverse=True
            )

        # å–å‰ N ä¸ª
        display_videos = valid_videos[:limit]

        # è½¬æ¢æ ¼å¼ï¼ˆå¤§é‡è§†é¢‘æ—¶çœç•¥ description å’Œ tags ä»¥å‡å°å“åº”ä½“ç§¯ï¼‰
        compact = len(display_videos) > 500
        video_list = []
        for v in display_videos:
            item = {
                "youtube_id": v.youtube_id,
                "title": v.title,
                "channel_name": v.channel_name,
                "channel_id": v.channel_id,
                "view_count": v.view_count or 0,
                "like_count": v.like_count or 0,
                "comment_count": v.comment_count or 0,
                "duration": v.duration or 0,
                "published_at": v.published_at.isoformat() if v.published_at else None,
                "thumbnail_url": v.thumbnail_url,
            }
            if not compact:
                item["description"] = v.description[:200] if v.description else None
                item["tags"] = v.tags or []
            video_list.append(item)

        # è®¡ç®—ç»Ÿè®¡
        total_views = sum(v["view_count"] for v in video_list)
        total_likes = sum(v["like_count"] for v in video_list)
        total_comments = sum(v["comment_count"] for v in video_list)

        db_stats = collector.get_statistics()

        # æ„å»ºç­›é€‰æ¡ä»¶æè¿°
        filter_description_parts = []
        if min_views > 0:
            filter_description_parts.append(f"æ’­æ”¾é‡â‰¥{min_views:,}")
        if date_from:
            filter_description_parts.append(f"ä» {date_from}")
        if date_to:
            filter_description_parts.append(f"åˆ° {date_to}")
        filter_description = "ï¼Œ".join(filter_description_parts) if filter_description_parts else "æ— ç­›é€‰"

        # è®¡ç®—æ—¶é•¿åˆ†å¸ƒï¼ˆç”¨äºæ¨¡å¼æ´å¯Ÿï¼‰- å¢å¼ºç‰ˆï¼ŒåŒ…å«å¹³å‡æ’­æ”¾é‡
        duration_stats = {
            "short": {"count": 0, "total_views": 0, "videos": []},  # < 5åˆ†é’Ÿ
            "medium": {"count": 0, "total_views": 0, "videos": []},  # 5-15åˆ†é’Ÿ
            "long": {"count": 0, "total_views": 0, "videos": []},  # > 15åˆ†é’Ÿ
        }
        for v in valid_videos:
            d = v.duration or 0
            views = v.view_count or 0
            if d < 300:  # < 5åˆ†é’Ÿ
                duration_stats["short"]["count"] += 1
                duration_stats["short"]["total_views"] += views
                duration_stats["short"]["videos"].append(views)
            elif d < 900:  # 5-15åˆ†é’Ÿ
                duration_stats["medium"]["count"] += 1
                duration_stats["medium"]["total_views"] += views
                duration_stats["medium"]["videos"].append(views)
            else:  # > 15åˆ†é’Ÿ
                duration_stats["long"]["count"] += 1
                duration_stats["long"]["total_views"] += views
                duration_stats["long"]["videos"].append(views)

        # æ„å»ºå¢å¼ºç‰ˆæ—¶é•¿åˆ†å¸ƒ
        duration_distribution = {}
        for key, stats in duration_stats.items():
            count = stats["count"]
            total_views = stats["total_views"]
            videos = stats["videos"]
            duration_distribution[key] = {
                "count": count,
                "total_views": total_views,
                "avg_views": int(total_views / count) if count > 0 else 0,
                "max_views": max(videos) if videos else 0,
                "min_views": min(videos) if videos else 0,
            }

        # è®¡ç®—æ ‡é¢˜æ¨¡å¼ï¼ˆæå–é«˜é¢‘å…³é”®è¯ï¼‰- jieba å·²åœ¨é¡¶éƒ¨é¢„åŠ è½½
        title_word_count = {}
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿˜', 'èƒ½', 'å¯ä»¥', 'è®©', 'è¢«', 'æŠŠ', 'ç»™', 'ä¸', 'åŠ', 'ä¹‹', 'æ¥', 'ä¸º', 'ä»¥', 'äº', 'æ‰€', 'å¦‚', 'æˆ–', 'è€Œ', 'ä½†', 'å¹¶', 'ä¸”', 'å› ', 'åˆ™', 'åª', 'ä»', 'å¦‚ä½•', 'å¦‚æœ', 'å°±æ˜¯', 'ä»€éº¼', 'é€™å€‹', 'é‚£å€‹', 'å¦‚ä½•', 'ä»€ä¹ˆ', 'æ€æ ·', 'å“ªäº›', 'ç‚ºä»€éº¼', 'ç‚ºä½•', 'å¦‚ä½•', 'æ€éº¼', 'æ€éº½', '#', 'ï½œ', '|', '/', '\\', '!', 'ï¼', '?', 'ï¼Ÿ', '...', 'ã€‚', 'ï¼Œ', 'ã€', 'ï¼š', 'ï¼›', '"', '"', ''', ''', 'ã€Œ', 'ã€', 'ã€', 'ã€‘', 'ã€Š', 'ã€‹', 'ï¼ˆ', 'ï¼‰', '(', ')', '[', ']', '{', '}', ' ', '\n', '\t'}
        for v in valid_videos:
            if v.title:
                words = jieba.cut(v.title)
                for word in words:
                    word = word.strip()
                    if len(word) >= 2 and word not in stopwords and not word.isdigit():
                        title_word_count[word] = title_word_count.get(word, 0) + 1

        # æ’åºå¹¶å–å‰20ä¸ªé«˜é¢‘è¯
        title_patterns = [
            {"word": word, "count": count}
            for word, count in sorted(title_word_count.items(), key=lambda x: x[1], reverse=True)[:20]
        ]

        # è®¡ç®—å‘å¸ƒè¶‹åŠ¿ï¼ˆæŒ‰æœˆç»Ÿè®¡ï¼‰
        publishing_trend = {}
        for v in valid_videos:
            if v.published_at:
                month_key = v.published_at.strftime("%Y-%m")
                publishing_trend[month_key] = publishing_trend.get(month_key, 0) + 1

        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰æœˆä»½æ’åº
        publishing_trend_list = [
            {"month": month, "count": count}
            for month, count in sorted(publishing_trend.items())
        ]

        # è®¡ç®—è¿‘æœŸçˆ†æ¬¾ï¼ˆ30å¤©å†…å‘å¸ƒï¼Œé«˜æ’­æ”¾é‡ï¼‰
        now = datetime.now()
        recent_hits = []
        evergreen_videos = []
        for v in valid_videos:
            if v.published_at:
                days_old = (now - v.published_at).days
                daily_views = (v.view_count or 0) / max(days_old, 1)
                if days_old <= 30 and (v.view_count or 0) >= 10000:
                    recent_hits.append({
                        "youtube_id": v.youtube_id,
                        "title": v.title,
                        "channel_name": v.channel_name,
                        "channel_id": v.channel_id,
                        "view_count": v.view_count or 0,
                        "video_age_days": days_old,
                        "daily_views": int(daily_views),
                        "published_at": v.published_at.isoformat() if v.published_at else None,
                    })
                elif days_old > 180 and daily_views > 100:  # åŠå¹´ä»¥ä¸Šï¼Œæ—¥å‡æ’­æ”¾>100
                    evergreen_videos.append({
                        "youtube_id": v.youtube_id,
                        "title": v.title,
                        "channel_name": v.channel_name,
                        "channel_id": v.channel_id,
                        "view_count": v.view_count or 0,
                        "video_age_days": days_old,
                        "daily_views": int(daily_views),
                        "published_at": v.published_at.isoformat() if v.published_at else None,
                    })

        # æŒ‰æ—¥å‡æ’­æ”¾æ’åº
        recent_hits.sort(key=lambda x: x["daily_views"], reverse=True)
        evergreen_videos.sort(key=lambda x: x["daily_views"], reverse=True)

        # è®¡ç®—å†…å®¹ç±»å‹ï¼ˆcategoryï¼‰ç»Ÿè®¡
        category_stats = {}
        for v in valid_videos:
            cat = getattr(v, 'category', None) or 'æœªåˆ†ç±»'
            if cat not in category_stats:
                category_stats[cat] = {
                    "count": 0,
                    "total_views": 0,
                    "videos": [],
                }
            category_stats[cat]["count"] += 1
            category_stats[cat]["total_views"] += v.view_count or 0
            category_stats[cat]["videos"].append(v.view_count or 0)

        # è®¡ç®—å„åˆ†ç±»çš„å¹³å‡æ’­æ”¾é‡ï¼Œå¹¶æ’åº
        category_list = []
        for cat, stats in category_stats.items():
            count = stats["count"]
            total_views = stats["total_views"]
            videos = stats["videos"]
            category_list.append({
                "category": cat,
                "count": count,
                "total_views": total_views,
                "avg_views": int(total_views / count) if count > 0 else 0,
                "max_views": max(videos) if videos else 0,
            })
        category_list.sort(key=lambda x: x["count"], reverse=True)

        # è®¡ç®—æ—¶é•¿+åˆ†ç±»çš„ç»„åˆç»Ÿè®¡ï¼ˆç”¨äºå‘ç°æœ€ä½³ç»„åˆï¼‰
        duration_category_combo = {}
        for v in valid_videos:
            cat = getattr(v, 'category', None) or 'æœªåˆ†ç±»'
            d = v.duration or 0
            if d < 300:
                duration_key = "short"
            elif d < 900:
                duration_key = "medium"
            else:
                duration_key = "long"
            combo_key = f"{duration_key}_{cat}"
            if combo_key not in duration_category_combo:
                duration_category_combo[combo_key] = {
                    "duration_type": duration_key,
                    "category": cat,
                    "count": 0,
                    "total_views": 0,
                }
            duration_category_combo[combo_key]["count"] += 1
            duration_category_combo[combo_key]["total_views"] += v.view_count or 0

        # è®¡ç®—æ¯ä¸ªç»„åˆçš„å¹³å‡æ’­æ”¾é‡
        for combo in duration_category_combo.values():
            combo["avg_views"] = int(combo["total_views"] / combo["count"]) if combo["count"] > 0 else 0

        # æŒ‰å¹³å‡æ’­æ”¾é‡æ’åºæ‰¾å‡ºæœ€ä½³ç»„åˆ
        best_combos = sorted(
            [c for c in duration_category_combo.values() if c["count"] >= 3],  # è‡³å°‘3ä¸ªè§†é¢‘æ‰æœ‰å‚è€ƒä»·å€¼
            key=lambda x: x["avg_views"],
            reverse=True
        )[:5]  # å–Top5

        # è®¡ç®—é¢‘é“æ›´æ–°é¢‘ç‡åˆ†æ
        channel_update_freq = {}
        for v in valid_videos:
            channel_key = v.channel_id if (v.channel_id and v.channel_id != 'None') else v.channel_name
            if not channel_key or not v.published_at:
                continue
            if channel_key not in channel_update_freq:
                channel_update_freq[channel_key] = {
                    "channel_name": v.channel_name,
                    "videos": [],
                    "total_views": 0,
                }
            channel_update_freq[channel_key]["videos"].append({
                "published_at": v.published_at,
                "views": v.view_count or 0,
            })
            channel_update_freq[channel_key]["total_views"] += v.view_count or 0

        # è®¡ç®—æ¯ä¸ªé¢‘é“çš„æ›´æ–°é¢‘ç‡
        update_frequency_stats = {
            "daily": {"count": 0, "total_views": 0, "channels": []},     # æ¯å¤©æ›´æ–°
            "weekly": {"count": 0, "total_views": 0, "channels": []},    # æ¯å‘¨1-2æ¬¡
            "biweekly": {"count": 0, "total_views": 0, "channels": []},  # æ¯ä¸¤å‘¨ä¸€æ¬¡
            "monthly": {"count": 0, "total_views": 0, "channels": []},   # æ¯æœˆä¸€æ¬¡
            "irregular": {"count": 0, "total_views": 0, "channels": []}, # ä¸è§„å¾‹
        }

        for channel_key, data in channel_update_freq.items():
            videos = data["videos"]
            if len(videos) < 2:
                continue  # è‡³å°‘2ä¸ªè§†é¢‘æ‰èƒ½è®¡ç®—é¢‘ç‡

            # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
            videos.sort(key=lambda x: x["published_at"])

            # è®¡ç®—å¹³å‡é—´éš”å¤©æ•°
            total_days = 0
            for i in range(1, len(videos)):
                delta = (videos[i]["published_at"] - videos[i-1]["published_at"]).days
                total_days += delta
            avg_interval = total_days / (len(videos) - 1) if len(videos) > 1 else 0

            # åˆ†ç±»æ›´æ–°é¢‘ç‡
            if avg_interval <= 2:
                freq_type = "daily"
            elif avg_interval <= 7:
                freq_type = "weekly"
            elif avg_interval <= 14:
                freq_type = "biweekly"
            elif avg_interval <= 30:
                freq_type = "monthly"
            else:
                freq_type = "irregular"

            update_frequency_stats[freq_type]["count"] += 1
            update_frequency_stats[freq_type]["total_views"] += data["total_views"]
            if len(update_frequency_stats[freq_type]["channels"]) < 5:  # æ¯ç±»æœ€å¤šè®°å½•5ä¸ªé¢‘é“
                update_frequency_stats[freq_type]["channels"].append({
                    "channel_name": data["channel_name"],
                    "video_count": len(videos),
                    "avg_interval_days": round(avg_interval, 1),
                    "total_views": data["total_views"],
                    "avg_views": data["total_views"] // len(videos) if videos else 0,
                })

        # è®¡ç®—æ¯ä¸ªé¢‘ç‡ç±»å‹çš„å¹³å‡æ’­æ”¾é‡
        for freq_type, stats in update_frequency_stats.items():
            stats["avg_views"] = stats["total_views"] // stats["count"] if stats["count"] > 0 else 0

        # æ‰¾å‡ºæœ€ä½³æ›´æ–°é¢‘ç‡
        best_frequency = max(
            [(k, v) for k, v in update_frequency_stats.items() if v["count"] > 0],
            key=lambda x: x[1]["avg_views"],
            default=("unknown", {"avg_views": 0})
        )

        # ========== æ™ºèƒ½åˆ†æç»“è®º ==========
        insights = _generate_insights(
            theme=theme,
            duration_distribution=duration_distribution,
            title_patterns=title_patterns,
            publishing_trend=publishing_trend_list,
            recent_hits=recent_hits,
            evergreen_videos=evergreen_videos,
            channels=_extract_channel_stats(valid_videos),  # ä½¿ç”¨æ—¶é—´è¿‡æ»¤åçš„è§†é¢‘
            total_videos=actual_total_videos,
            filtered_videos=filtered_count,
            avg_views=total_views // len(video_list) if video_list else 0,
        )

        result = {
            "status": "ok",
            "result": {
                "topic": theme,
                "total_videos": actual_total_videos,
                "total_channels": actual_total_channels,
                "filtered_videos": filtered_count,
                "displayed_videos": len(video_list),
                "total_views": total_views,
                "total_likes": total_likes,
                "total_comments": total_comments,
                "avg_views": total_views // len(video_list) if video_list else 0,
                "avg_likes": total_likes // len(video_list) if video_list else 0,
                "videos": video_list,
                "data_basis": {
                    "total_videos_in_db": db_stats.get('total', 0),
                    "videos_for_this_topic": actual_total_videos,
                    "filtered_count": filtered_count,
                    "ranking_basis": len(video_list),
                    "description": f"æœ¬æ’ååŸºäº {actual_total_videos} ä¸ªè§†é¢‘ä¸­ç­›é€‰å‡ºçš„ {filtered_count} ä¸ªè§†é¢‘"
                },
                "data_time_range": data_time_range,
                "filter_applied": {
                    "min_views": min_views,
                    "date_from": date_from,
                    "date_to": date_to,
                    "description": filter_description
                },
                "last_updated": datetime.now().isoformat(),
                "channels": _extract_channel_stats(valid_videos),  # ä½¿ç”¨æ—¶é—´è¿‡æ»¤åçš„è§†é¢‘
                # æ¨¡å¼æ´å¯Ÿæ•°æ®
                "duration_distribution": duration_distribution,
                "title_patterns": title_patterns,  # æ ‡é¢˜é«˜é¢‘è¯
                "publishing_trend": publishing_trend_list,  # å‘å¸ƒè¶‹åŠ¿
                "recent_hits": recent_hits[:20],  # è¿‘æœŸçˆ†æ¬¾ Top 20
                "evergreen_videos": evergreen_videos[:20],  # é•¿é’è§†é¢‘ Top 20
                "category_stats": category_list,  # å†…å®¹ç±»å‹ç»Ÿè®¡
                "best_duration_category_combos": best_combos,  # æœ€ä½³æ—¶é•¿+ç±»å‹ç»„åˆ
                "update_frequency_stats": update_frequency_stats,  # æ›´æ–°é¢‘ç‡ç»Ÿè®¡
                "best_update_frequency": {
                    "type": best_frequency[0],
                    "avg_views": best_frequency[1]["avg_views"],
                    "channel_count": best_frequency[1]["count"],
                },
                # æ™ºèƒ½åˆ†æç»“è®º
                "insights": insights,
                # é¢‘é“æ¦œå•ï¼ˆç”¨äºé¢‘é“è¿è¥æ¨¡å—çš„åŠ¨æ€æ¸²æŸ“ï¼‰- ä½¿ç”¨è¿‡æ»¤åçš„è§†é¢‘
                "channel_rankings": _generate_channel_rankings(valid_videos),
                # é¢‘é“ç¨³å®šæ€§åˆ†æ - ä½¿ç”¨è¿‡æ»¤åçš„è§†é¢‘
                "channel_stability": _calculate_channel_stability(valid_videos),
                # åœ°åŒºåˆ†å¸ƒåˆ†æï¼ˆTab2 å¥—åˆ©æŒ–æ˜ï¼‰
                "region_distribution": _analyze_region_distribution(_extract_channel_stats(valid_videos)),
                # æ˜ŸæœŸå‘å¸ƒæ•ˆæœåˆ†æï¼ˆTab5 å‘å¸ƒç­–ç•¥ï¼‰
                "weekday_performance": _analyze_weekday_performance(valid_videos),
                # å†…å®¹ç”Ÿå‘½å‘¨æœŸåˆ†æï¼ˆTab3 é€‰é¢˜å†³ç­–ï¼‰
                "content_lifecycle": _analyze_content_lifecycle(valid_videos),
            }
        }

        # ç¼“å­˜ç»“æœ
        _analyze_cache[cache_key] = (time.time(), result)

        return result
    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


def _generate_insights(
    theme: str,
    duration_distribution: dict,
    title_patterns: list,
    publishing_trend: list,
    recent_hits: list,
    evergreen_videos: list,
    channels: list,
    total_videos: int,
    filtered_videos: int,
    avg_views: int,
    duration_arbitrage: list = None,  # æ—¶é•¿å¥—åˆ©æ•°æ®
    channel_arbitrage: list = None,   # é¢‘é“å¥—åˆ©æ•°æ®
    topic_arbitrage: list = None,     # è¯é¢˜å¥—åˆ©æ•°æ®
) -> dict:
    """
    ç”Ÿæˆæ™ºèƒ½åˆ†æç»“è®º
    åŒ…æ‹¬ï¼šå•é¡¹åˆ†æç»“è®º + ç»¼åˆåˆ†æç»“è®º
    """
    insights = {
        "duration_insight": None,  # æ—¶é•¿åˆ†å¸ƒç»“è®º
        "trend_insight": None,     # å‘å¸ƒè¶‹åŠ¿ç»“è®º
        "title_insight": None,     # æ ‡é¢˜æ¨¡å¼ç»“è®º
        "channel_insight": None,   # é¢‘é“åˆ†å¸ƒç»“è®º
        "comprehensive": None,     # ç»¼åˆç»“è®º
        "opportunities": [],       # æœºä¼šç‚¹
        "recommendations": [],     # å»ºè®®
        # === æœ‰è¶£åº¦åˆ†æï¼ˆå¥—åˆ©åˆ†æï¼‰===
        "arbitrage_analysis": {
            "duration_arbitrage": [],   # æ—¶é•¿å¥—åˆ©ï¼šéœ€æ±‚/ä¾›ç»™
            "channel_arbitrage": [],    # é¢‘é“å¥—åˆ©ï¼šå°é¢‘é“çˆ†æ¬¾
            "topic_arbitrage": [],      # è¯é¢˜å¥—åˆ©ï¼šä¸­ä»‹ä¸­å¿ƒæ€§/åº¦ä¸­å¿ƒæ€§
        },
        # === åšä¸»åˆ†ç±»åˆ†æ ===
        "creator_classification": {
            "most_influential": [],     # æœ€æœ‰å½±å“åŠ›çš„åšä¸»ï¼ˆæ’­æ”¾é‡+è®¢é˜…é‡æœ€é«˜ï¼‰
            "underrated": [],           # è¢«ä½ä¼°çš„åšä¸»ï¼ˆè®¢é˜…å°‘ä½†æ’­æ”¾é«˜ï¼‰
            "high_value_niche": [],     # é«˜ä»·å€¼ä½†å°ä¼—çš„åšä¸»ï¼ˆæœ‰è¶£åº¦é«˜ï¼‰
        },
        # === è§†é¢‘ç”Ÿå‘½åŠ›åˆ†æ ===
        "video_vitality": {
            "evergreen_content": [],    # é•¿é’å†…å®¹ï¼ˆå‘å¸ƒè¶…180å¤©ä»æœ‰é«˜æ—¥å‡ï¼‰
            "recent_hot": [],           # è¿‘æœŸé«˜çƒ­åº¦ï¼ˆå‘å¸ƒæ—©æœŸè¡¨ç°çªå‡ºï¼Œéœ€æŒç»­ç›‘æ§ç¡®è®¤å¢é•¿è¶‹åŠ¿ï¼‰
        },
    }

    # ===== 1. æ—¶é•¿åˆ†å¸ƒç»“è®º =====
    # å…¼å®¹æ–°æ—§æ ¼å¼ï¼šæ–°æ ¼å¼æ˜¯ {"short": {"count": N, "avg_views": M}, ...}
    def get_duration_count(key):
        val = duration_distribution.get(key, 0)
        if isinstance(val, dict):
            return val.get("count", 0)
        return val

    total_duration = get_duration_count("short") + get_duration_count("medium") + get_duration_count("long")
    if total_duration > 0:
        short_pct = get_duration_count("short") / total_duration * 100
        medium_pct = get_duration_count("medium") / total_duration * 100
        long_pct = get_duration_count("long") / total_duration * 100

        if short_pct > 60:
            insights["duration_insight"] = {
                "type": "short_dominated",
                "title": "çŸ­è§†é¢‘ä¸»å¯¼",
                "description": f"è¯¥é¢†åŸŸä»¥çŸ­è§†é¢‘ä¸ºä¸»ï¼ˆ{short_pct:.0f}%ï¼‰ï¼Œ5åˆ†é’Ÿä»¥å†…çš„å†…å®¹æ›´æ˜“è·å¾—å…³æ³¨ã€‚",
                "recommendation": "å»ºè®®åˆ¶ä½œ3-5åˆ†é’Ÿçš„ç²¾ç‚¼å†…å®¹ï¼Œå¿«é€Ÿä¼ é€’æ ¸å¿ƒä»·å€¼ã€‚"
            }
        elif long_pct > 40:
            insights["duration_insight"] = {
                "type": "long_content",
                "title": "æ·±åº¦å†…å®¹å—æ¬¢è¿",
                "description": f"é•¿è§†é¢‘å æ¯”è¾ƒé«˜ï¼ˆ{long_pct:.0f}%ï¼‰ï¼Œè§‚ä¼—æ„¿æ„æŠ•å…¥æ—¶é—´å­¦ä¹ æ·±åº¦å†…å®¹ã€‚",
                "recommendation": "å¯ä»¥åˆ¶ä½œ15-30åˆ†é’Ÿçš„ç³»ç»Ÿæ€§æ•™ç¨‹æˆ–æ·±åº¦è§£æã€‚"
            }
        else:
            insights["duration_insight"] = {
                "type": "mixed",
                "title": "æ—¶é•¿å¤šå…ƒåŒ–",
                "description": f"çŸ­({short_pct:.0f}%)/ä¸­({medium_pct:.0f}%)/é•¿({long_pct:.0f}%)è§†é¢‘å‡æœ‰å¸‚åœºã€‚",
                "recommendation": "å¯æ ¹æ®å†…å®¹å¤æ‚åº¦çµæ´»é€‰æ‹©æ—¶é•¿ï¼ŒçŸ­è§†é¢‘å¼•æµï¼Œé•¿è§†é¢‘å˜ç°ã€‚"
            }

    # ===== 2. å‘å¸ƒè¶‹åŠ¿ç»“è®º =====
    if len(publishing_trend) >= 3:
        recent_months = publishing_trend[-3:]
        older_months = publishing_trend[-6:-3] if len(publishing_trend) >= 6 else publishing_trend[:3]

        recent_avg = sum(m["count"] for m in recent_months) / len(recent_months)
        older_avg = sum(m["count"] for m in older_months) / len(older_months) if older_months else recent_avg

        if older_avg > 0:
            growth_rate = (recent_avg - older_avg) / older_avg * 100
        else:
            growth_rate = 0

        if growth_rate > 30:
            insights["trend_insight"] = {
                "type": "rapid_growth",
                "title": "å¿«é€Ÿå¢é•¿æœŸ",
                "description": f"è¿‘æœŸå†…å®¹å‘å¸ƒé‡å¢é•¿{growth_rate:.0f}%ï¼Œå¸‚åœºçƒ­åº¦æŒç»­ä¸Šå‡ã€‚",
                "recommendation": "å»ºè®®å°½å¿«å…¥åœºï¼ŒæŠ¢å å…ˆå‘ä¼˜åŠ¿ã€‚"
            }
        elif growth_rate < -20:
            insights["trend_insight"] = {
                "type": "declining",
                "title": "çƒ­åº¦ä¸‹é™",
                "description": f"è¿‘æœŸå‘å¸ƒé‡ä¸‹é™{abs(growth_rate):.0f}%ï¼Œå¸‚åœºå¯èƒ½è¶‹äºé¥±å’Œæˆ–è½¬ç§»ã€‚",
                "recommendation": "éœ€è¦å¯»æ‰¾å·®å¼‚åŒ–è§’åº¦æˆ–æ–°çš„ç»†åˆ†æ–¹å‘ã€‚"
            }
        else:
            insights["trend_insight"] = {
                "type": "stable",
                "title": "ç¨³å®šå‘å±•",
                "description": f"å‘å¸ƒé‡ä¿æŒç¨³å®šï¼Œå¸‚åœºæˆç†Ÿåº¦è¾ƒé«˜ã€‚",
                "recommendation": "é€‚åˆæŒç»­æ·±è€•ï¼Œæ³¨é‡å†…å®¹è´¨é‡æå‡ã€‚"
            }

    # ===== 3. æ ‡é¢˜æ¨¡å¼ç»“è®º =====
    if title_patterns and len(title_patterns) >= 5:
        top_words = [p["word"] for p in title_patterns[:5]]
        insights["title_insight"] = {
            "type": "keyword_pattern",
            "title": "é«˜é¢‘å…³é”®è¯",
            "description": f"ã€Œ{top_words[0]}ã€ã€Œ{top_words[1]}ã€ã€Œ{top_words[2]}ã€ç­‰è¯æ±‡é¢‘ç¹å‡ºç°ã€‚",
            "keywords": top_words,
            "recommendation": f"æ ‡é¢˜ä¸­åŒ…å«ã€Œ{top_words[0]}ã€ç­‰å…³é”®è¯å¯æé«˜æœç´¢æ›å…‰ã€‚"
        }

    # ===== 4. é¢‘é“åˆ†å¸ƒç»“è®º =====
    if channels and len(channels) >= 3:
        top_channel = channels[0]
        top3_share = sum(c.get("total_views", 0) for c in channels[:3])
        total_channel_views = sum(c.get("total_views", 0) for c in channels)
        concentration = top3_share / total_channel_views * 100 if total_channel_views > 0 else 0

        if concentration > 50:
            insights["channel_insight"] = {
                "type": "concentrated",
                "title": "å¤´éƒ¨é›†ä¸­",
                "description": f"Top 3 é¢‘é“å æ® {concentration:.0f}% çš„æ’­æ”¾é‡ï¼Œå¤´éƒ¨æ•ˆåº”æ˜æ˜¾ã€‚",
                "top_channel": top_channel.get("channel_name", "æœªçŸ¥"),
                "recommendation": "éœ€è¦å·®å¼‚åŒ–ç«äº‰ï¼Œé¿å…ä¸å¤´éƒ¨æ­£é¢ç«äº‰ã€‚"
            }
        else:
            insights["channel_insight"] = {
                "type": "dispersed",
                "title": "ç«äº‰åˆ†æ•£",
                "description": f"æ’­æ”¾é‡åˆ†å¸ƒè¾ƒä¸ºåˆ†æ•£ï¼Œæ–°é¢‘é“ä»æœ‰æœºä¼šã€‚",
                "top_channel": top_channel.get("channel_name", "æœªçŸ¥"),
                "recommendation": "å¸‚åœºæœºä¼šè¾ƒå¤šï¼Œä¸“æ³¨å‚ç›´ç»†åˆ†é¢†åŸŸå¯å¿«é€Ÿçªå›´ã€‚"
            }

    # ===== 5. ç»¼åˆåˆ†æç»“è®º =====
    comprehensive_parts = []
    opportunities = []

    # å¸‚åœºæ¦‚è§ˆ
    market_size = "å¤§" if total_videos > 1000 else "ä¸­ç­‰" if total_videos > 200 else "å°ä¼—"
    comprehensive_parts.append(f"ã€Œ{theme}ã€é¢†åŸŸå…±æœ‰ {total_videos} ä¸ªè§†é¢‘ï¼Œå¸‚åœºè§„æ¨¡{market_size}ã€‚")

    # åŸºäºå„ç»´åº¦ç”Ÿæˆæœºä¼šç‚¹
    if insights.get("duration_insight"):
        di = insights["duration_insight"]
        if di["type"] == "short_dominated":
            opportunities.append({
                "title": "é•¿è§†é¢‘è“æµ·",
                "description": "å¸‚åœºä»¥çŸ­è§†é¢‘ä¸ºä¸»ï¼Œæ·±åº¦é•¿è§†é¢‘å†…å®¹è¾ƒå°‘ï¼Œå¯èƒ½å­˜åœ¨å·®å¼‚åŒ–æœºä¼šã€‚",
                "score": "B"
            })
        elif di["type"] == "long_content":
            opportunities.append({
                "title": "çŸ­è§†é¢‘å¼•æµ",
                "description": "å¯åˆ¶ä½œç²¾åçŸ­è§†é¢‘å¸å¼•ç”¨æˆ·ï¼Œå†å¯¼æµè‡³é•¿è§†é¢‘ã€‚",
                "score": "A"
            })

    if insights.get("trend_insight"):
        ti = insights["trend_insight"]
        if ti["type"] == "rapid_growth":
            opportunities.append({
                "title": "å¢é•¿çº¢åˆ©",
                "description": "å¸‚åœºå¿«é€Ÿå¢é•¿ä¸­ï¼Œæ–°è¿›å…¥è€…å®¹æ˜“è·å¾—å…³æ³¨ã€‚",
                "score": "A"
            })
        elif ti["type"] == "stable":
            opportunities.append({
                "title": "ç²¾å“åŒ–æœºä¼š",
                "description": "å¸‚åœºæˆç†Ÿä½†å†…å®¹åŒè´¨åŒ–ä¸¥é‡ï¼Œé«˜è´¨é‡å·®å¼‚åŒ–å†…å®¹æœ‰æœºä¼šè„±é¢–è€Œå‡ºã€‚",
                "score": "B"
            })

    if len(evergreen_videos) > 5:
        opportunities.append({
            "title": "é•¿é’å†…å®¹",
            "description": f"å­˜åœ¨ {len(evergreen_videos)} ä¸ªé•¿æœŸè·å¾—æµé‡çš„è§†é¢‘ï¼Œè¯´æ˜è¯¥é¢†åŸŸé€‚åˆåšå¸¸é’å†…å®¹ã€‚",
            "score": "A"
        })

    if len(recent_hits) > 3:
        opportunities.append({
            "title": "è¿‘æœŸçƒ­ç‚¹",
            "description": f"è¿‘30å¤©æœ‰ {len(recent_hits)} ä¸ªçˆ†æ¬¾è§†é¢‘ï¼Œå¸‚åœºæ´»è·ƒåº¦é«˜ã€‚",
            "score": "A"
        })

    insights["opportunities"] = opportunities

    # ç”Ÿæˆç»¼åˆå»ºè®®
    recommendations = []
    if insights.get("title_insight"):
        recommendations.append(f"æ ‡é¢˜ç­–ç•¥ï¼šä½¿ç”¨ã€Œ{insights['title_insight']['keywords'][0]}ã€ã€Œ{insights['title_insight']['keywords'][1]}ã€ç­‰å…³é”®è¯")
    if insights.get("duration_insight"):
        recommendations.append(f"æ—¶é•¿ç­–ç•¥ï¼š{insights['duration_insight']['recommendation']}")
    if insights.get("channel_insight"):
        recommendations.append(f"ç«äº‰ç­–ç•¥ï¼š{insights['channel_insight']['recommendation']}")

    insights["recommendations"] = recommendations

    # ===== 6. æœ‰è¶£åº¦åˆ†æï¼ˆå¥—åˆ©åˆ†æï¼‰=====
    arbitrage = insights["arbitrage_analysis"]

    # 6.1 æ—¶é•¿å¥—åˆ©ï¼šæœ‰è¶£åº¦ = å¹³å‡æ’­æ”¾é‡ / ä¾›ç»™æ¯”ä¾‹
    # æ‰¾å‡ºéœ€æ±‚å¤§ï¼ˆæ’­æ”¾é‡é«˜ï¼‰ä½†ä¾›ç»™å°‘ï¼ˆè§†é¢‘æ•°å°‘ï¼‰çš„æ—¶é•¿åŒºé—´
    if duration_arbitrage:
        arbitrage["duration_arbitrage"] = duration_arbitrage
    else:
        # ä½¿ç”¨ä¼ å…¥çš„ duration_distribution è®¡ç®—ï¼ˆå…¼å®¹æ–°æ—§æ ¼å¼ï¼‰
        total_videos_duration = sum(
            v.get("count", 0) if isinstance(v, dict) else v
            for v in duration_distribution.values()
        )
        if total_videos_duration > 0:
            duration_arb = []
            duration_labels = {
                "short": "çŸ­è§†é¢‘(<5åˆ†é’Ÿ)",
                "medium": "ä¸­ç­‰(5-15åˆ†é’Ÿ)",
                "long": "é•¿è§†é¢‘(>15åˆ†é’Ÿ)"
            }
            for key, label in duration_labels.items():
                val = duration_distribution.get(key, 0)
                if isinstance(val, dict):
                    count = val.get("count", 0)
                    actual_avg_views = val.get("avg_views", 0)
                else:
                    count = val
                    actual_avg_views = avg_views if avg_views > 0 else 10000

                supply_ratio = count / total_videos_duration if total_videos_duration > 0 else 0
                # ä½¿ç”¨çœŸå®å¹³å‡æ’­æ”¾é‡ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ä¼°ç®—
                if actual_avg_views > 0:
                    estimated_demand = actual_avg_views
                else:
                    base_avg = avg_views if avg_views > 0 else 10000
                    if key == "short":
                        estimated_demand = base_avg * 0.8
                    elif key == "medium":
                        estimated_demand = base_avg * 1.0
                    else:
                        estimated_demand = base_avg * 1.5

                interestingness = estimated_demand / (supply_ratio * 100) if supply_ratio > 0 else 0
                duration_arb.append({
                    "bucket": label,
                    "supply_count": count,
                    "supply_ratio": round(supply_ratio * 100, 1),
                    "avg_views": int(actual_avg_views),  # çœŸå®å¹³å‡æ’­æ”¾é‡
                    "estimated_avg_views": int(estimated_demand),
                    "interestingness": round(interestingness, 2),
                    "insight": "è“æµ·æœºä¼š" if interestingness > (avg_views or 10000) else "çº¢æµ·ç«äº‰" if interestingness < (avg_views or 10000) * 0.5 else "ä¸­ç­‰ç«äº‰"
                })
            duration_arb.sort(key=lambda x: x["interestingness"], reverse=True)
            arbitrage["duration_arbitrage"] = duration_arb

    # 6.2 é¢‘é“å¥—åˆ©ï¼šæœ‰è¶£åº¦ = æœ€é«˜æ’­æ”¾é‡ / é¢‘é“å¹³å‡æ’­æ”¾é‡
    # æ‰¾å‡ºå°é¢‘é“çš„çˆ†æ¬¾è§†é¢‘ï¼ˆè¯æ˜å†…å®¹è€Œéæµé‡å†³å®šæ’­æ”¾ï¼‰
    if channel_arbitrage:
        arbitrage["channel_arbitrage"] = channel_arbitrage
    elif channels:
        channel_arb = []
        for ch in channels[:20]:  # åˆ†æ Top 20 é¢‘é“
            max_views = ch.get("total_views", 0)  # è¿™é‡Œåº”è¯¥æ˜¯å•è§†é¢‘æœ€é«˜æ’­æ”¾
            avg_ch_views = ch.get("avg_views", 0)
            video_count = ch.get("video_count", 0)

            if avg_ch_views > 0 and video_count >= 2:
                # è®¡ç®—çˆ†æ¬¾ç³»æ•°ï¼šæœ€é«˜æ’­æ”¾ vs å¹³å‡æ’­æ”¾
                # å¦‚æœæŸä¸ªè§†é¢‘æ’­æ”¾è¿œè¶…é¢‘é“å¹³å‡ï¼Œè¯´æ˜å†…å®¹æœ¬èº«æœ‰å¸å¼•åŠ›
                viral_ratio = max_views / avg_ch_views if avg_ch_views > 0 else 0

                # å°é¢‘é“çˆ†æ¬¾æ›´æœ‰ä»·å€¼ï¼ˆè®¢é˜…å°‘ä½†æ’­æ”¾é«˜ï¼‰
                subscriber_count = ch.get("subscriber_count", 0) or ch.get("subscriberCount", 0)
                if subscriber_count > 0:
                    views_per_sub = max_views / subscriber_count
                else:
                    views_per_sub = 0

                channel_arb.append({
                    "channel_name": ch.get("channel_name", "æœªçŸ¥"),
                    "channel_id": ch.get("channel_id"),
                    "video_count": video_count,
                    "total_views": ch.get("total_views", 0),
                    "avg_views": avg_ch_views,
                    "subscriber_count": subscriber_count,
                    "viral_ratio": round(viral_ratio, 2),
                    "views_per_subscriber": round(views_per_sub, 2),
                    "interestingness": round(viral_ratio * (1 + views_per_sub / 10), 2) if views_per_sub > 0 else round(viral_ratio, 2),
                    "insight": "é«˜æ½œåŠ›" if viral_ratio > 3 and views_per_sub > 5 else "å†…å®¹é©±åŠ¨" if viral_ratio > 2 else "æµé‡ç¨³å®š"
                })

        channel_arb.sort(key=lambda x: x["interestingness"], reverse=True)
        arbitrage["channel_arbitrage"] = channel_arb[:10]  # è¿”å› Top 10

    # 6.3 è¯é¢˜å¥—åˆ©ï¼šåŸºäºå…³é”®è¯å…±ç°åˆ†æ
    # æœ‰è¶£åº¦ = ä¸­ä»‹ä¸­å¿ƒæ€§ / åº¦ä¸­å¿ƒæ€§ï¼ˆè¿æ¥ä¸åŒè¯é¢˜ç¾¤çš„å…³é”®è¯æ›´æœ‰ä»·å€¼ï¼‰
    if topic_arbitrage:
        arbitrage["topic_arbitrage"] = topic_arbitrage
    elif title_patterns and len(title_patterns) >= 5:
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨è¯é¢‘å’Œå…±ç°é¢‘ç‡ä¼°ç®—
        # é«˜é¢‘ä½†ç‹¬ç‰¹ï¼ˆä¸å…¶ä»–è¯å…±ç°å°‘ï¼‰çš„è¯æ›´æœ‰"å¥—åˆ©"ä»·å€¼
        topic_arb = []
        top_words = title_patterns[:15]  # åˆ†æå‰15ä¸ªé«˜é¢‘è¯
        total_word_count = sum(w["count"] for w in top_words)

        for i, word_data in enumerate(top_words):
            word = word_data["word"]
            count = word_data["count"]

            # é¢‘ç‡å æ¯”
            frequency_ratio = count / total_word_count if total_word_count > 0 else 0

            # æ’åç³»æ•°ï¼ˆæ’åè¶Šé åä½†é¢‘ç‡ä¸ä½ï¼Œè¯´æ˜æ˜¯"éšè—å®è—"ï¼‰
            rank_score = (len(top_words) - i) / len(top_words)

            # ç®€åŒ–çš„æœ‰è¶£åº¦è®¡ç®—ï¼šé¢‘ç‡é€‚ä¸­ä½†æ’åé å‰çš„è¯å¯èƒ½è¿‡äºæ³›åŒ–
            # é¢‘ç‡é€‚ä¸­ä¸”æ’åé åçš„è¯å¯èƒ½æ˜¯å·®å¼‚åŒ–æœºä¼š
            if frequency_ratio > 0.15:
                insight = "æ³›åŒ–å…³é”®è¯ï¼ˆç«äº‰æ¿€çƒˆï¼‰"
                interestingness = frequency_ratio * 0.5
            elif frequency_ratio > 0.08:
                insight = "ä¸»æµå…³é”®è¯ï¼ˆé€‚åº¦ç«äº‰ï¼‰"
                interestingness = frequency_ratio * 1.0
            else:
                insight = "å·®å¼‚åŒ–å…³é”®è¯ï¼ˆè“æµ·æœºä¼šï¼‰"
                interestingness = frequency_ratio * 2.0  # ç»™äºˆæ›´é«˜æƒé‡

            topic_arb.append({
                "keyword": word,
                "frequency": count,
                "frequency_ratio": round(frequency_ratio * 100, 2),
                "interestingness": round(interestingness * 100, 2),
                "insight": insight
            })

        # æŒ‰æœ‰è¶£åº¦æ’åºï¼Œçªå‡ºè“æµ·æœºä¼š
        topic_arb.sort(key=lambda x: x["interestingness"], reverse=True)
        arbitrage["topic_arbitrage"] = topic_arb

    # ===== 7. åšä¸»åˆ†ç±»åˆ†æ =====
    creator_class = insights["creator_classification"]

    if channels and len(channels) >= 3:
        # 7.1 æœ€æœ‰å½±å“åŠ›çš„åšä¸»ï¼ˆæ’­æ”¾é‡+è®¢é˜…é‡æœ€é«˜ï¼‰
        # è®¡ç®—å½±å“åŠ›åˆ†æ•° = æ€»æ’­æ”¾é‡ Ã— log10(è®¢é˜…æ•°+1)
        import math
        influential_list = []
        for ch in channels[:30]:
            total_views = ch.get("total_views", 0)
            subscriber_count = ch.get("subscriber_count", 0) or ch.get("subscriberCount", 0)
            video_count = ch.get("video_count", 0)

            # å½±å“åŠ›åˆ†æ•°ï¼šç»¼åˆè€ƒè™‘æ’­æ”¾é‡å’Œè®¢é˜…
            influence_score = total_views * math.log10(max(subscriber_count, 1) + 1) / 1000
            influential_list.append({
                "channel_name": ch.get("channel_name", "æœªçŸ¥"),
                "channel_id": ch.get("channel_id"),
                "total_views": total_views,
                "subscriber_count": subscriber_count,
                "video_count": video_count,
                "avg_views": ch.get("avg_views", 0),
                "influence_score": round(influence_score, 2),
            })
        influential_list.sort(key=lambda x: x["influence_score"], reverse=True)
        creator_class["most_influential"] = influential_list[:10]

        # 7.2 è¢«ä½ä¼°çš„åšä¸»ï¼ˆè®¢é˜…å°‘ä½†æ’­æ”¾é«˜ï¼‰
        # æœ‰è¶£åº¦ = æ€»æ’­æ”¾é‡ / (è®¢é˜…æ•° + 1)
        underrated_list = []
        for ch in channels:
            total_views = ch.get("total_views", 0)
            subscriber_count = ch.get("subscriber_count", 0) or ch.get("subscriberCount", 0)
            avg_views = ch.get("avg_views", 0)

            if total_views > 5000:  # è‡³å°‘æœ‰ä¸€å®šæ’­æ”¾é‡
                # è¢«ä½ä¼°æŒ‡æ•° = æ’­æ”¾é‡ / è®¢é˜…æ•°ï¼ˆè®¢é˜…è¶Šå°‘ä½†æ’­æ”¾è¶Šé«˜è¶Šè¢«ä½ä¼°ï¼‰
                underrated_score = total_views / (subscriber_count + 1) if subscriber_count < 10000 else 0
                if underrated_score > 0:
                    underrated_list.append({
                        "channel_name": ch.get("channel_name", "æœªçŸ¥"),
                        "channel_id": ch.get("channel_id"),
                        "total_views": total_views,
                        "subscriber_count": subscriber_count,
                        "video_count": ch.get("video_count", 0),
                        "avg_views": avg_views,
                        "underrated_score": round(underrated_score, 2),
                        "insight": "é«˜åº¦è¢«ä½ä¼°" if underrated_score > 100 else "ä¸­åº¦è¢«ä½ä¼°" if underrated_score > 10 else "è½»åº¦è¢«ä½ä¼°"
                    })
        underrated_list.sort(key=lambda x: x["underrated_score"], reverse=True)
        creator_class["underrated"] = underrated_list[:10]

        # 7.3 é«˜ä»·å€¼ä½†å°ä¼—çš„åšä¸»ï¼ˆæœ‰è¶£åº¦é«˜ = å†…å®¹è´¨é‡é«˜ä½†æ¸ é“å°ï¼‰
        # æœ‰è¶£åº¦ = å¹³å‡æ’­æ”¾é‡ / (è®¢é˜…æ•° / è§†é¢‘æ•° + 1)
        high_value_niche = []
        for ch in channels:
            avg_views = ch.get("avg_views", 0)
            subscriber_count = ch.get("subscriber_count", 0) or ch.get("subscriberCount", 0)
            video_count = ch.get("video_count", 0)

            if avg_views > 1000 and video_count >= 2:  # æœ‰ä¸€å®šè´¨é‡
                # é«˜ä»·å€¼æŒ‡æ•° = å¹³å‡æ’­æ”¾é‡ / æ¯è§†é¢‘é¢„æœŸè®¢é˜…è½¬åŒ–
                expected_subs_per_video = subscriber_count / (video_count + 1)
                niche_score = avg_views / (expected_subs_per_video + 100)
                if niche_score > 1:
                    high_value_niche.append({
                        "channel_name": ch.get("channel_name", "æœªçŸ¥"),
                        "channel_id": ch.get("channel_id"),
                        "total_views": ch.get("total_views", 0),
                        "subscriber_count": subscriber_count,
                        "video_count": video_count,
                        "avg_views": avg_views,
                        "niche_score": round(niche_score, 2),
                        "insight": "é«˜ä»·å€¼å°ä¼—" if niche_score > 50 else "æ½œåŠ›å°ä¼—" if niche_score > 10 else "å€¼å¾—å…³æ³¨"
                    })
        high_value_niche.sort(key=lambda x: x["niche_score"], reverse=True)
        creator_class["high_value_niche"] = high_value_niche[:10]

    # ===== 8. è§†é¢‘ç”Ÿå‘½åŠ›åˆ†æ =====
    video_vitality = insights["video_vitality"]

    # 8.1 é•¿é’å†…å®¹ï¼ˆä½¿ç”¨ä¼ å…¥çš„ evergreen_videosï¼‰
    if evergreen_videos:
        evergreen_with_vitality = []
        for v in evergreen_videos[:20]:
            days_old = v.get("video_age_days", 0)
            daily_views = v.get("daily_views", 0)
            total_views = v.get("view_count", 0)

            # ç”Ÿå‘½åŠ›æŒ‡æ•° = æ—¥å‡æ’­æ”¾ Ã— sqrt(è§†é¢‘å¤©æ•°)
            vitality_score = daily_views * math.sqrt(days_old) if days_old > 0 else 0
            evergreen_with_vitality.append({
                **v,
                "vitality_score": round(vitality_score, 2),
                "vitality_level": "è¶…çº§é•¿é’" if vitality_score > 5000 else "ä¼˜è´¨é•¿é’" if vitality_score > 1000 else "ç¨³å®šé•¿é’"
            })
        evergreen_with_vitality.sort(key=lambda x: x["vitality_score"], reverse=True)
        video_vitality["evergreen_content"] = evergreen_with_vitality[:10]

    # 8.2 è¿‘æœŸé«˜çƒ­åº¦ï¼ˆä½¿ç”¨ä¼ å…¥çš„ recent_hitsï¼Œä»…è¡¨ç¤ºå‘å¸ƒæ—©æœŸè¡¨ç°çªå‡ºï¼Œéœ€æŒç»­ç›‘æ§ç¡®è®¤å¢é•¿è¶‹åŠ¿ï¼‰
    if recent_hits:
        recent_hot = []
        for v in recent_hits[:20]:
            days_old = v.get("video_age_days", 1)
            daily_views = v.get("daily_views", 0)
            total_views = v.get("view_count", 0)

            # çƒ­åº¦æŒ‡æ•° = æ—¥å‡æ’­æ”¾ / sqrt(è§†é¢‘å¤©æ•°+1)ï¼ˆè¶Šæ–°çš„é«˜æ’­æ”¾è§†é¢‘çƒ­åº¦è¶Šé«˜ï¼‰
            hot_score = daily_views / math.sqrt(days_old + 1)
            recent_hot.append({
                **v,
                "hot_score": round(hot_score, 2),
                "hot_level": "çˆ†æ¬¾æ½œåŠ›" if hot_score > 10000 else "çƒ­é—¨" if hot_score > 3000 else "ä¸Šå‡ä¸­"
            })
        recent_hot.sort(key=lambda x: x["hot_score"], reverse=True)
        video_vitality["recent_hot"] = recent_hot[:10]

    # ç»¼åˆç»“è®º
    insights["comprehensive"] = {
        "market_summary": " ".join(comprehensive_parts),
        "opportunity_count": len(opportunities),
        "key_finding": opportunities[0]["description"] if opportunities else "éœ€è¦æ›´å¤šæ•°æ®è¿›è¡Œæ·±å…¥åˆ†æ",
        "top_performing": recent_hits[0] if recent_hits else None,
    }

    return insights


def _get_channels_info_from_db() -> dict:
    """ä» channels è¡¨è·å–é¢‘é“è¯¦ç»†ä¿¡æ¯"""
    if not db_exists():
        return {}

    channels_info = {}
    try:
        conn = db_get_connection(row_factory=sqlite3.Row)
        cursor = conn.cursor()
        cursor.execute("""
            SELECT channel_id, channel_name, handle, subscriber_count,
                   video_count, total_views, country, description,
                   created_at, canonical_url
            FROM channels
        """)
        for row in cursor.fetchall():
            channels_info[row['channel_id']] = {
                'handle': row['handle'],
                'channel_subscriber_count': row['subscriber_count'],  # channels è¡¨çš„è®¢é˜…æ•°
                'channel_video_count': row['video_count'],  # é¢‘é“æ€»è§†é¢‘æ•°
                'channel_total_views': row['total_views'],  # é¢‘é“æ€»è§‚çœ‹æ•°
                'country': row['country'],
                'description': row['description'],
                'created_at': row['created_at'],
                'canonical_url': row['canonical_url'],
            }
        conn.close()
    except Exception as e:
        print(f"æŸ¥è¯¢ channels è¡¨å¤±è´¥: {e}")

    return channels_info


def _extract_channel_stats(videos) -> list:
    """æå–é¢‘é“ç»Ÿè®¡ä¿¡æ¯ï¼ˆåˆå¹¶ channels è¡¨æ•°æ®ï¼‰"""
    # å…ˆä» channels è¡¨è·å–é¢å¤–ä¿¡æ¯
    channels_extra_info = _get_channels_info_from_db()

    channel_data = {}
    for v in videos:
        channel_key = v.channel_id if (v.channel_id and v.channel_id != 'None') else v.channel_name
        if not channel_key:
            continue

        if channel_key not in channel_data:
            channel_data[channel_key] = {
                "channel_id": v.channel_id,
                "channel_name": v.channel_name,
                "video_count": 0,  # è¯¥ä¸»é¢˜ä¸‹çš„è§†é¢‘æ•°
                "total_views": 0,  # è¯¥ä¸»é¢˜ä¸‹çš„æ€»æ’­æ”¾
                "total_likes": 0,
                "subscriber_count": 0,  # è®¢é˜…æ•°ï¼ˆå–æœ€å¤§å€¼ï¼‰
            }

        channel_data[channel_key]["video_count"] += 1
        channel_data[channel_key]["total_views"] += v.view_count or 0
        channel_data[channel_key]["total_likes"] += v.like_count or 0
        # å–è¯¥é¢‘é“è§†é¢‘ä¸­æœ€å¤§çš„è®¢é˜…æ•°ï¼ˆå› ä¸ºä¸åŒè§†é¢‘é‡‡é›†æ—¶é—´å¯èƒ½ä¸åŒï¼‰
        video_subscriber = getattr(v, 'subscriber_count', 0) or 0
        if video_subscriber > channel_data[channel_key]["subscriber_count"]:
            channel_data[channel_key]["subscriber_count"] = video_subscriber

    # è®¡ç®—å¹³å‡å€¼ã€åˆå¹¶ channels è¡¨æ•°æ®å¹¶æ’åº
    channels = list(channel_data.values())
    for c in channels:
        c["avg_views"] = c["total_views"] // c["video_count"] if c["video_count"] > 0 else 0
        c["subscriberCount"] = c["subscriber_count"]  # å‰ç«¯æœŸæœ›çš„å­—æ®µå

        # åˆå¹¶ channels è¡¨çš„é¢å¤–ä¿¡æ¯
        channel_id = c.get("channel_id")
        if channel_id and channel_id in channels_extra_info:
            extra = channels_extra_info[channel_id]
            c["handle"] = extra.get("handle")  # @ç”¨æˆ·å
            c["country"] = extra.get("country")  # å›½å®¶/åœ°åŒº
            c["description"] = extra.get("description")  # é¢‘é“æè¿°
            c["channel_video_count"] = extra.get("channel_video_count")  # é¢‘é“æ€»è§†é¢‘æ•°
            c["channel_total_views"] = extra.get("channel_total_views")  # é¢‘é“æ€»è§‚çœ‹æ•°
            c["created_at"] = extra.get("created_at")  # é¢‘é“åˆ›å»ºæ—¶é—´
            c["canonical_url"] = extra.get("canonical_url")  # é¢‘é“ URL
            # å¦‚æœ channels è¡¨æœ‰è®¢é˜…æ•°ä¸”æ¯”è§†é¢‘è¡¨çš„å¤§ï¼Œä½¿ç”¨ channels è¡¨çš„
            channel_subs = extra.get("channel_subscriber_count") or 0
            if channel_subs > c["subscriber_count"]:
                c["subscriber_count"] = channel_subs
                c["subscriberCount"] = channel_subs

            # è®¡ç®—é¢‘é“å¹´é¾„å’Œæ—¥å‡æ¶¨ç²‰
            created_at = extra.get("created_at")
            if created_at and c["subscriber_count"] > 0:
                try:
                    from datetime import datetime
                    created = datetime.strptime(created_at, "%Y-%m-%d")
                    days = (datetime.now() - created).days
                    if days > 0:
                        c["days_since_creation"] = days
                        c["subs_per_day"] = round(c["subscriber_count"] / days, 1)
                except:
                    pass

    channels.sort(key=lambda c: c["total_views"], reverse=True)
    return channels  # è¿”å›å…¨éƒ¨é¢‘é“


# ============================================================
# è¶‹åŠ¿çƒ­è¯
# ============================================================

@app.get("/api/trending")
async def get_trending_keywords():
    """è·å–è¶‹åŠ¿çƒ­è¯"""
    # è¿™é‡Œå¯ä»¥é›†æˆçœŸå®çš„è¶‹åŠ¿æ•°æ®æº
    # ç›®å‰è¿”å›é¢„è®¾çƒ­è¯
    keywords = [
        {"keyword": "AIå·¥å…·", "heat": 98},
        {"keyword": "å¥åº·å…»ç”Ÿ", "heat": 95},
        {"keyword": "Pythonæ•™ç¨‹", "heat": 92},
        {"keyword": "å‰¯ä¸šèµšé’±", "heat": 90},
        {"keyword": "æŠ•èµ„ç†è´¢", "heat": 88},
        {"keyword": "æ•ˆç‡å·¥å…·", "heat": 85},
        {"keyword": "è‡ªåª’ä½“è¿è¥", "heat": 82},
        {"keyword": "è‹±è¯­å­¦ä¹ ", "heat": 80},
    ]
    return {"status": "ok", "keywords": keywords}


# ============================================================
# åˆ›ä½œè€…åŠ©æ‰‹ APIï¼ˆåˆ›ä½œè€…å‹å¥½ç‰ˆï¼‰
# ============================================================

@app.get("/api/creator-helper/{theme}")
async def get_creator_helper(theme: str):
    """
    åˆ›ä½œè€…åŠ©æ‰‹ API - ç›´æ¥ç»™å‡ºå¯ç”¨çš„å»ºè®®

    è¿”å› 5 ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š
    1. é€‰é¢˜åŠ©æ‰‹ - ä»Šæ—¥æ¨èé€‰é¢˜ + ç†ç”± + æ ‡é¢˜å‚è€ƒ
    2. çˆ†æ¬¾å­¦ä¹  - å€¼å¾—å­¦ä¹ çš„çˆ†æ¬¾è§†é¢‘ + å­¦ä¹ ç‚¹
    3. æ ‡é¢˜åŠ©æ‰‹ - é«˜æ•ˆå…³é”®è¯ + æ ‡é¢˜å…¬å¼æ¨¡æ¿
    4. å­¦ä¹ å¯¹è±¡ - é¢‘é“æ¦œå•ï¼ˆæ€»æ’­æ”¾æ¦œ/å¹³å‡æ’­æ”¾æ¦œ/è§†é¢‘æ•°æ¦œ/é»‘é©¬æ¦œ/é«˜æ•ˆæ¦œï¼‰
    5. é¿å‘æé†’ - 3 ä¸ªæœ€é‡è¦çš„è­¦å‘Š
    """
    try:
        collector = _get_data_collector()

        # è·å–æ‰€æœ‰è§†é¢‘æ•°æ®
        all_videos = collector.get_videos_from_db(
            theme=theme,
            min_views=0,
            limit=10000
        )

        if not all_videos:
            all_videos = collector.get_videos_from_db(
                keyword_like=theme,
                min_views=0,
                limit=10000
            )

        if not all_videos:
            return {
                "status": "error",
                "message": f"æ²¡æœ‰æ‰¾åˆ°ä¸»é¢˜ '{theme}' çš„æ•°æ®"
            }

        now = datetime.now()
        import math

        # ========== 1. é€‰é¢˜åŠ©æ‰‹ ==========
        topic_recommendations = _generate_topic_recommendations(all_videos, theme, now)

        # ========== 2. çˆ†æ¬¾å­¦ä¹  ==========
        viral_learning = _generate_viral_learning(all_videos, now)

        # ========== 3. æ ‡é¢˜åŠ©æ‰‹ ==========
        title_helper = _generate_title_helper(all_videos)

        # ========== 4. å­¦ä¹ å¯¹è±¡ï¼ˆé¢‘é“æ¦œå•ï¼‰==========
        channel_rankings = _generate_channel_rankings(all_videos)

        # ========== 5. é¿å‘æé†’ ==========
        warnings = _generate_warnings(all_videos, now)

        return {
            "status": "ok",
            "theme": theme,
            "data_basis": len(all_videos),
            "modules": {
                "topic_recommendations": topic_recommendations,
                "viral_learning": viral_learning,
                "title_helper": title_helper,
                "channel_rankings": channel_rankings,
                "warnings": warnings,
            }
        }

    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


def _generate_topic_recommendations(videos, theme: str, now) -> dict:
    """ç”Ÿæˆé€‰é¢˜æ¨è"""
    import jieba
    from collections import Counter

    # åˆ†æè¿‘æœŸçˆ†æ¬¾çš„å…³é”®è¯
    recent_viral = []
    evergreen = []

    for v in videos:
        if not v.published_at:
            continue
        days_old = (now - v.published_at).days
        daily_views = (v.view_count or 0) / max(days_old, 1)

        if days_old <= 30 and (v.view_count or 0) >= 10000:
            recent_viral.append({
                "title": v.title,
                "views": v.view_count,
                "daily_views": daily_views,
                "days_old": days_old,
            })
        elif days_old > 180 and daily_views > 100:
            evergreen.append({
                "title": v.title,
                "views": v.view_count,
                "daily_views": daily_views,
                "days_old": days_old,
            })

    # æå–çˆ†æ¬¾è§†é¢‘ä¸­çš„å…³é”®è¯
    stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿˜', 'èƒ½', 'å¯ä»¥', 'è®©', 'è¢«', 'æŠŠ', 'ç»™', 'å¦‚ä½•'}
    keyword_counter = Counter()

    for v in recent_viral[:20]:
        words = jieba.cut(v["title"])
        for word in words:
            word = word.strip()
            if len(word) >= 2 and word not in stopwords and not word.isdigit():
                keyword_counter[word] += 1

    hot_keywords = [k for k, _ in keyword_counter.most_common(10)]

    # ç”Ÿæˆ 3 ä¸ªé€‰é¢˜æ¨è
    recommendations = []

    if recent_viral:
        # æ¨è 1: åŸºäºè¿‘æœŸçˆ†æ¬¾
        top_viral = recent_viral[0]
        recommendations.append({
            "topic": f"{theme} + {hot_keywords[0] if hot_keywords else 'å®ç”¨æŠ€å·§'}",
            "reason": f"è¿‘æœŸã€Œ{top_viral['title'][:20]}...ã€æ—¥å‡ {int(top_viral['daily_views'])} æ’­æ”¾ï¼Œè¯´æ˜è¿™ä¸ªè§’åº¦æœ‰çƒ­åº¦",
            "title_reference": top_viral["title"],
            "confidence": "é«˜",
        })

    if len(hot_keywords) >= 2:
        # æ¨è 2: å…³é”®è¯ç»„åˆ
        recommendations.append({
            "topic": f"ã€Œ{hot_keywords[0]}ã€+ã€Œ{hot_keywords[1]}ã€ç»„åˆ",
            "reason": f"è¿™ä¸¤ä¸ªè¯åœ¨çˆ†æ¬¾æ ‡é¢˜ä¸­é¢‘ç¹å‡ºç°ï¼Œç»„åˆä½¿ç”¨å¯æé«˜æœç´¢æ›å…‰",
            "title_reference": f"ã€{hot_keywords[0]}ã€‘{theme}å¿…çœ‹çš„{hot_keywords[1]}æŒ‡å—",
            "confidence": "ä¸­",
        })

    if evergreen:
        # æ¨è 3: åŸºäºé•¿é’å†…å®¹
        top_evergreen = evergreen[0]
        recommendations.append({
            "topic": "åˆ¶ä½œç³»ç»Ÿæ€§æ•™ç¨‹ï¼ˆé•¿é’å†…å®¹ï¼‰",
            "reason": f"ã€Œ{top_evergreen['title'][:20]}...ã€å‘å¸ƒ {top_evergreen['days_old']} å¤©ä»æœ‰æ—¥å‡ {int(top_evergreen['daily_views'])} æ’­æ”¾",
            "title_reference": top_evergreen["title"],
            "confidence": "é«˜",
        })

    return {
        "today_picks": recommendations[:3],
        "hot_keywords": hot_keywords[:5],
        "data_source": f"åŸºäº {len(recent_viral)} ä¸ªè¿‘æœŸçˆ†æ¬¾å’Œ {len(evergreen)} ä¸ªé•¿é’è§†é¢‘åˆ†æ",
    }


def _generate_viral_learning(videos, now) -> dict:
    """ç”Ÿæˆçˆ†æ¬¾å­¦ä¹ å†…å®¹"""
    viral_videos = []

    for v in videos:
        if not v.published_at or not v.view_count:
            continue
        days_old = (now - v.published_at).days
        if days_old <= 0:
            days_old = 1

        daily_views = v.view_count / days_old
        engagement_rate = ((v.like_count or 0) + (v.comment_count or 0)) / max(v.view_count, 1)

        # ç­›é€‰çˆ†æ¬¾ï¼šæ’­æ”¾é‡é«˜æˆ–æ—¥å‡é«˜
        if v.view_count >= 50000 or (daily_views > 1000 and v.view_count >= 10000):
            # åˆ†æå­¦ä¹ ç‚¹
            learning_points = []

            # æ—¶é•¿åˆ†æ
            duration = v.duration or 0
            if duration < 300:
                learning_points.append("çŸ­è§†é¢‘ç­–ç•¥ï¼š5åˆ†é’Ÿå†…å¿«é€Ÿä¼ é€’ä»·å€¼")
            elif duration > 900:
                learning_points.append("æ·±åº¦å†…å®¹ï¼šé•¿è§†é¢‘æ»¡è¶³æ·±åº¦å­¦ä¹ éœ€æ±‚")

            # äº’åŠ¨ç‡åˆ†æ
            if engagement_rate > 0.05:
                learning_points.append("é«˜äº’åŠ¨ç‡ï¼šæ ‡é¢˜/å†…å®¹å¼•å‘å…±é¸£")

            # æ—¥å‡åˆ†æ
            if daily_views > 5000:
                learning_points.append("ç—…æ¯’ä¼ æ’­ï¼šå…·æœ‰åˆ†äº«å±æ€§")
            elif days_old > 180 and daily_views > 200:
                learning_points.append("é•¿é’å†…å®¹ï¼šæŒç»­è·å¾—æœç´¢æµé‡")

            viral_videos.append({
                "youtube_id": v.youtube_id,
                "title": v.title,
                "channel_name": v.channel_name,
                "view_count": v.view_count,
                "daily_views": int(daily_views),
                "days_old": days_old,
                "duration": duration,
                "learning_points": learning_points or ["å€¼å¾—æ·±å…¥åˆ†æå…¶æˆåŠŸå› ç´ "],
                "thumbnail_url": v.thumbnail_url,
            })

    # æ’åºå¹¶å–å‰ 10 ä¸ª
    viral_videos.sort(key=lambda x: x["daily_views"], reverse=True)

    return {
        "videos": viral_videos[:10],
        "summary": f"å…±å‘ç° {len(viral_videos)} ä¸ªå€¼å¾—å­¦ä¹ çš„çˆ†æ¬¾è§†é¢‘",
    }


def _generate_title_helper(videos) -> dict:
    """ç”Ÿæˆæ ‡é¢˜åŠ©æ‰‹å†…å®¹"""
    import jieba
    from collections import Counter

    # æå–æ‰€æœ‰æ ‡é¢˜çš„å…³é”®è¯
    stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿˜', 'èƒ½', 'å¯ä»¥', 'è®©', 'è¢«', 'æŠŠ', 'ç»™', 'å¦‚ä½•', 'å¦‚æœ', 'ä¸ºä»€ä¹ˆ', 'æ€æ ·', 'å“ªäº›'}

    # æŒ‰æ’­æ”¾é‡åŠ æƒç»Ÿè®¡å…³é”®è¯
    keyword_scores = Counter()
    keyword_video_count = Counter()

    for v in videos:
        if not v.title or not v.view_count:
            continue
        words = jieba.cut(v.title)
        seen_words = set()
        for word in words:
            word = word.strip()
            if len(word) >= 2 and word not in stopwords and not word.isdigit() and word not in seen_words:
                seen_words.add(word)
                keyword_scores[word] += v.view_count
                keyword_video_count[word] += 1

    # è®¡ç®—é«˜æ•ˆå…³é”®è¯ï¼šæ€»æ’­æ”¾é‡ / å‡ºç°æ¬¡æ•°
    efficient_keywords = []
    for word, total_views in keyword_scores.most_common(50):
        count = keyword_video_count[word]
        if count >= 3:  # è‡³å°‘å‡ºç° 3 æ¬¡
            avg_views = total_views / count
            efficient_keywords.append({
                "keyword": word,
                "avg_views": int(avg_views),
                "video_count": count,
                "total_views": total_views,
            })

    efficient_keywords.sort(key=lambda x: x["avg_views"], reverse=True)

    # æå–æ ‡é¢˜æ¨¡æ¿
    title_templates = [
        {"template": "ã€æ•°å­—ã€‘+ ä¸»é¢˜ + ç»“æœ", "example": "ã€3ä¸ªæ–¹æ³•ã€‘è®©ä½ çš„XXXæå‡200%"},
        {"template": "å¦‚ä½• + åŠ¨ä½œ + è·å¾—å¥½å¤„", "example": "å¦‚ä½•åœ¨7å¤©å†…æŒæ¡XXX"},
        {"template": "ä¸»é¢˜ + é¿å‘æŒ‡å—", "example": "æ–°æ‰‹å¿…çœ‹ï¼XXXå¸¸è§çš„5ä¸ªé”™è¯¯"},
        {"template": "ä¸ºä»€ä¹ˆ + é—®é¢˜ + ï¼ˆç­”æ¡ˆé¢„å‘Šï¼‰", "example": "ä¸ºä»€ä¹ˆä½ çš„XXXæ€»æ˜¯å¤±è´¥ï¼Ÿ"},
        {"template": "ä¸»é¢˜ + å®Œæ•´æ•™ç¨‹/æ”»ç•¥", "example": "2025å¹´æœ€æ–°XXXå®Œæ•´æ”»ç•¥"},
    ]

    return {
        "efficient_keywords": efficient_keywords[:10],
        "title_templates": title_templates,
        "tip": "åœ¨æ ‡é¢˜ä¸­ä½¿ç”¨é«˜æ•ˆå…³é”®è¯ï¼Œå¹³å‡èƒ½è·å¾—æ›´é«˜æ’­æ”¾é‡",
    }


def _generate_channel_rankings(videos) -> dict:
    """ç”Ÿæˆé¢‘é“æ¦œå•ï¼ˆæ€»æ’­æ”¾æ¦œ/å¹³å‡æ’­æ”¾æ¦œ/è§†é¢‘æ•°æ¦œ/é»‘é©¬æ¦œ/é«˜æ•ˆæ¦œ/å¿«é€Ÿå¢é•¿æ¦œï¼‰"""
    import math
    from datetime import datetime

    # ä» channels è¡¨è·å–é¢‘é“è®¢é˜…æ•°å’Œåˆ›å»ºæ—¶é—´
    channel_info = {}  # channel_id/channel_name -> {subscriber_count, created_at, days_since_creation, subs_per_day}
    if db_exists():
        try:
            conn = db_get_connection()
            cursor = conn.cursor()
            # è·å–æ‰€æœ‰é¢‘é“çš„è®¢é˜…æ•°ï¼ˆä¸é™åˆ¶ created_atï¼‰
            cursor.execute("""
                SELECT channel_id, channel_name, subscriber_count, created_at
                FROM channels
                WHERE subscriber_count > 0
            """)
            now = datetime.now()
            for row in cursor.fetchall():
                ch_id, ch_name, subs, created_at = row
                info = {
                    "subscriber_count": subs or 0,
                    "created_at": None,
                    "days_since_creation": None,
                    "subs_per_day": 0,
                }
                # å¦‚æœæœ‰åˆ›å»ºæ—¶é—´ï¼Œè®¡ç®—å¢é•¿é€Ÿåº¦
                if created_at and created_at.strip():
                    try:
                        created = datetime.strptime(created_at, "%Y-%m-%d")
                        days = (now - created).days
                        if days > 0:
                            info["created_at"] = created_at
                            info["days_since_creation"] = days
                            info["subs_per_day"] = round(subs / days, 1) if subs else 0
                    except:
                        pass
                # ç”¨ channel_id å’Œ channel_name éƒ½ä½œä¸º key
                if ch_id:
                    channel_info[ch_id] = info
                if ch_name:
                    channel_info[ch_name] = info
            conn.close()
        except Exception as e:
            print(f"è·å–é¢‘é“æ•°æ®å¤±è´¥: {e}")

    # ç»Ÿè®¡æ¯ä¸ªé¢‘é“çš„æ•°æ®
    channel_data = {}
    channel_videos = {}  # å­˜å‚¨æ¯ä¸ªé¢‘é“çš„è§†é¢‘åˆ—è¡¨ï¼Œç”¨äºè¯¦ç»†åˆ†æ

    for v in videos:
        channel_key = v.channel_id if (v.channel_id and v.channel_id != 'None') else v.channel_name
        if not channel_key:
            continue

        if channel_key not in channel_data:
            channel_data[channel_key] = {
                "channel_id": v.channel_id,
                "channel_name": v.channel_name,
                "video_count": 0,
                "total_views": 0,
                "total_likes": 0,
                "subscriber_count": 0,
                "max_views": 0,
                "min_views": float('inf'),
                "total_duration": 0,
                "videos": [],  # ç”¨äºåˆ†æä¼˜ç¼ºç‚¹
            }
            channel_videos[channel_key] = []

        channel_data[channel_key]["video_count"] += 1
        channel_data[channel_key]["total_views"] += v.view_count or 0
        channel_data[channel_key]["total_likes"] += v.like_count or 0
        channel_data[channel_key]["total_duration"] += v.duration or 0

        if (v.view_count or 0) > channel_data[channel_key]["max_views"]:
            channel_data[channel_key]["max_views"] = v.view_count or 0
        if (v.view_count or 0) < channel_data[channel_key]["min_views"]:
            channel_data[channel_key]["min_views"] = v.view_count or 0

        channel_data[channel_key]["videos"].append({
            "title": v.title,
            "views": v.view_count or 0,
            "likes": v.like_count or 0,
            "duration": v.duration or 0,
        })

        video_subscriber = getattr(v, 'subscriber_count', 0) or 0
        if video_subscriber > channel_data[channel_key]["subscriber_count"]:
            channel_data[channel_key]["subscriber_count"] = video_subscriber

    # è®¡ç®—æ´¾ç”ŸæŒ‡æ ‡
    channels = list(channel_data.values())
    for c in channels:
        c["avg_views"] = c["total_views"] // c["video_count"] if c["video_count"] > 0 else 0
        c["avg_duration"] = c["total_duration"] // c["video_count"] if c["video_count"] > 0 else 0
        c["engagement_rate"] = c["total_likes"] / c["total_views"] if c["total_views"] > 0 else 0
        c["views_variance"] = c["max_views"] - c["min_views"] if c["min_views"] != float('inf') else 0

        # ã€é‡è¦ã€‘å…ˆä» channels è¡¨è·å–è®¢é˜…æ•°ï¼ˆæ›´å‡†ç¡®ï¼‰ï¼Œå†è®¡ç®—é»‘é©¬æŒ‡æ•°
        channel_key = c["channel_id"] or c["channel_name"]
        if channel_key in channel_info:
            info = channel_info[channel_key]
            # ä½¿ç”¨ channels è¡¨çš„è®¢é˜…æ•°ï¼ˆæ›´å‡†ç¡®ï¼‰
            if info["subscriber_count"] > c["subscriber_count"]:
                c["subscriber_count"] = info["subscriber_count"]
            c["created_at"] = info["created_at"]
            c["days_since_creation"] = info["days_since_creation"]
            c["subs_per_day"] = info["subs_per_day"]
        else:
            c["created_at"] = None
            c["days_since_creation"] = None
            c["subs_per_day"] = 0

        # é»‘é©¬æŒ‡æ•°ï¼ˆåœ¨è·å–è®¢é˜…æ•°ä¹‹åè®¡ç®—ï¼‰
        if c["subscriber_count"] > 0 and c["subscriber_count"] < 10000:
            c["dark_horse_score"] = c["max_views"] / (c["subscriber_count"] + 1)
        else:
            c["dark_horse_score"] = 0

        # é«˜æ•ˆæŒ‡æ•°
        if c["subscriber_count"] > 0:
            c["efficiency_score"] = c["avg_views"] / c["subscriber_count"]
        else:
            c["efficiency_score"] = 0

    def format_channel_with_analysis(c, rank_type: str) -> dict:
        """æ ¼å¼åŒ–é¢‘é“ä¿¡æ¯ï¼ŒåŒ…å«è¯¦ç»†åˆ†æ"""
        # åˆ†æä¼˜ç‚¹
        strengths = []
        weaknesses = []

        # åŸºäºæ•°æ®åˆ†æä¼˜ç‚¹
        if c["avg_views"] > 50000:
            strengths.append({
                "point": "å†…å®¹å¸å¼•åŠ›å¼º",
                "data": f"å¹³å‡æ’­æ”¾ {c['avg_views']:,}",
                "reasoning": "è¿œé«˜äºè¯¥é¢†åŸŸå¹³å‡æ°´å¹³ï¼Œè¯´æ˜å†…å®¹æœ‰è¾ƒå¼ºå¸å¼•åŠ›"
            })
        elif c["avg_views"] > 10000:
            strengths.append({
                "point": "ç¨³å®šè¡¨ç°",
                "data": f"å¹³å‡æ’­æ”¾ {c['avg_views']:,}",
                "reasoning": "é«˜äºè¡Œä¸šä¸­ä½æ•°ï¼Œè¯´æ˜å†…å®¹è´¨é‡ç¨³å®š"
            })

        if c["engagement_rate"] > 0.05:
            strengths.append({
                "point": "é«˜äº’åŠ¨ç‡",
                "data": f"ç‚¹èµç‡ {c['engagement_rate']*100:.1f}%",
                "reasoning": "è¯´æ˜å†…å®¹èƒ½å¼•å‘ç”¨æˆ·å…±é¸£ï¼Œå€¼å¾—å­¦ä¹ å…¶äº’åŠ¨æŠ€å·§"
            })

        if c["video_count"] >= 10:
            strengths.append({
                "point": "æŒç»­è¾“å‡º",
                "data": f"å‘å¸ƒäº† {c['video_count']} ä¸ªè§†é¢‘",
                "reasoning": "è¯´æ˜è¯¥é¢‘é“æ·±è€•è¯¥é¢†åŸŸï¼Œæœ‰ç³»ç»ŸåŒ–çš„å†…å®¹ç­–ç•¥"
            })

        avg_duration_min = c["avg_duration"] / 60
        if 5 <= avg_duration_min <= 15:
            strengths.append({
                "point": "æ—¶é•¿é€‚ä¸­",
                "data": f"å¹³å‡æ—¶é•¿ {avg_duration_min:.0f} åˆ†é’Ÿ",
                "reasoning": "ç¬¦åˆè§‚ä¼—æ³¨æ„åŠ›æ›²çº¿ï¼Œæ—¢æœ‰æ·±åº¦åˆä¸å†—é•¿"
            })

        # åˆ†æç¼ºç‚¹
        if c["views_variance"] > c["avg_views"] * 3:
            weaknesses.append({
                "point": "è¡¨ç°ä¸ç¨³å®š",
                "data": f"æ’­æ”¾é‡æ³¢åŠ¨: {c['min_views']:,} ~ {c['max_views']:,}",
                "reasoning": "è§†é¢‘è´¨é‡ä¸ç¨³å®šï¼Œéƒ¨åˆ†å†…å®¹æœªèƒ½è·å¾—è¶³å¤Ÿå…³æ³¨"
            })

        if c["engagement_rate"] < 0.02:
            weaknesses.append({
                "point": "äº’åŠ¨ç‡åä½",
                "data": f"ç‚¹èµç‡ä»… {c['engagement_rate']*100:.1f}%",
                "reasoning": "å¯èƒ½å†…å®¹åä¿¡æ¯æ€§ï¼Œç¼ºä¹æƒ…æ„Ÿå…±é¸£ç‚¹"
            })

        if avg_duration_min > 30:
            weaknesses.append({
                "point": "è§†é¢‘è¿‡é•¿",
                "data": f"å¹³å‡æ—¶é•¿ {avg_duration_min:.0f} åˆ†é’Ÿ",
                "reasoning": "å¯èƒ½å¯¼è‡´å®Œæ’­ç‡ä¸‹é™ï¼Œå»ºè®®å­¦ä¹ å…¶å†…å®¹ä½†ç¼©çŸ­æ—¶é•¿"
            })

        # è®¡ç®—æ¨èç†ç”±
        why_learn = _get_learning_reason_detailed(c, rank_type)

        return {
            "channel_name": c["channel_name"],
            "channel_id": c["channel_id"],
            "total_views": c["total_views"],
            "avg_views": c["avg_views"],
            "video_count": c["video_count"],
            "subscriber_count": c["subscriber_count"],
            "max_views": c["max_views"],
            "engagement_rate": round(c["engagement_rate"] * 100, 2),
            "avg_duration": c["avg_duration"],
            # é¢‘é“å¹´é¾„ç›¸å…³
            "created_at": c.get("created_at"),
            "days_since_creation": c.get("days_since_creation"),
            "subs_per_day": c.get("subs_per_day", 0),
            "why_learn": why_learn["summary"],
            "analysis": {
                "conclusion": why_learn["conclusion"],
                "data_basis": why_learn["data_basis"],
                "calculation": why_learn["calculation"],
                "strengths": strengths[:3],  # æœ€å¤š3ä¸ªä¼˜ç‚¹
                "weaknesses": weaknesses[:2],  # æœ€å¤š2ä¸ªç¼ºç‚¹
            }
        }

    def _get_learning_reason_detailed(c, rank_type: str) -> dict:
        """æ ¹æ®æ¦œå•ç±»å‹ç»™å‡ºè¯¦ç»†çš„å­¦ä¹ ç†ç”±"""
        if rank_type == "total_views":
            return {
                "summary": f"æ€»æ’­æ”¾ {c['total_views']:,}ï¼Œæ˜¯è¯¥é¢†åŸŸçš„å¤´éƒ¨é¢‘é“",
                "conclusion": "å¤´éƒ¨é¢‘é“ï¼Œå€¼å¾—ä½œä¸ºæ ‡æ†å­¦ä¹ ",
                "data_basis": [
                    f"æ€»æ’­æ”¾é‡: {c['total_views']:,}",
                    f"è§†é¢‘æ•°é‡: {c['video_count']}",
                    f"å¹³å‡æ’­æ”¾: {c['avg_views']:,}",
                ],
                "calculation": "æŒ‰æ€»æ’­æ”¾é‡é™åºæ’åˆ—",
            }
        elif rank_type == "avg_views":
            return {
                "summary": f"å¹³å‡æ¯ä¸ªè§†é¢‘ {c['avg_views']:,} æ’­æ”¾ï¼Œå†…å®¹è´¨é‡ç¨³å®š",
                "conclusion": "å†…å®¹è´¨é‡æ ‡æ†ï¼Œæ¯ä¸ªè§†é¢‘éƒ½å€¼å¾—å­¦ä¹ ",
                "data_basis": [
                    f"å¹³å‡æ’­æ”¾: {c['avg_views']:,}",
                    f"æ€»æ’­æ”¾: {c['total_views']:,}",
                    f"è§†é¢‘æ•°: {c['video_count']}",
                ],
                "calculation": "å¹³å‡æ’­æ”¾ = æ€»æ’­æ”¾ / è§†é¢‘æ•°ï¼ˆç­›é€‰è‡³å°‘3ä¸ªè§†é¢‘ï¼‰",
            }
        elif rank_type == "video_count":
            return {
                "summary": f"å‘å¸ƒäº† {c['video_count']} ä¸ªè§†é¢‘ï¼Œæ·±è€•è¯¥é¢†åŸŸ",
                "conclusion": "å¯å­¦ä¹ å…¶é€‰é¢˜ç­–ç•¥å’Œå†…å®¹ä½“ç³»",
                "data_basis": [
                    f"è§†é¢‘æ•°é‡: {c['video_count']}",
                    f"æ€»æ’­æ”¾: {c['total_views']:,}",
                    f"å¹³å‡æ’­æ”¾: {c['avg_views']:,}",
                ],
                "calculation": "æŒ‰è§†é¢‘å‘å¸ƒæ•°é‡é™åºæ’åˆ—",
            }
        elif rank_type == "dark_horse":
            return {
                "summary": f"è®¢é˜…ä»… {c['subscriber_count']:,} å´æœ‰ {c['max_views']:,} æ’­æ”¾çš„è§†é¢‘",
                "conclusion": "å†…å®¹é©±åŠ¨å‹ï¼Œè¯æ˜å¥½å†…å®¹ä¸éœ€è¦å¤§æµé‡åŸºç¡€",
                "data_basis": [
                    f"è®¢é˜…æ•°: {c['subscriber_count']:,}",
                    f"æœ€é«˜æ’­æ”¾: {c['max_views']:,}",
                    f"é»‘é©¬æŒ‡æ•°: {c['dark_horse_score']:.1f}",
                ],
                "calculation": "é»‘é©¬æŒ‡æ•° = æœ€é«˜æ’­æ”¾ / (è®¢é˜…æ•°+1)ï¼Œä»…é™è®¢é˜…<1ä¸‡çš„é¢‘é“",
            }
        elif rank_type == "efficiency":
            return {
                "summary": f"æ¯ä¸ªè®¢é˜…è´¡çŒ® {c['efficiency_score']:.1f} æ’­æ”¾ï¼Œè½¬åŒ–æ•ˆç‡é«˜",
                "conclusion": "è½¬åŒ–æ•ˆç‡æ ‡æ†ï¼Œå¯å­¦ä¹ å…¶ç”¨æˆ·è¿è¥æ–¹å¼",
                "data_basis": [
                    f"å¹³å‡æ’­æ”¾: {c['avg_views']:,}",
                    f"è®¢é˜…æ•°: {c['subscriber_count']:,}",
                    f"æ•ˆç‡æŒ‡æ•°: {c['efficiency_score']:.2f}",
                ],
                "calculation": "æ•ˆç‡æŒ‡æ•° = å¹³å‡æ’­æ”¾ / è®¢é˜…æ•°",
            }
        elif rank_type == "fast_growth":
            days = c.get('days_since_creation', 0)
            subs = c.get('subscriber_count', 0)
            subs_per_day = c.get('subs_per_day', 0)
            created_at = c.get('created_at', 'æœªçŸ¥')
            return {
                "summary": f"åˆ›å»º {days} å¤©æ¶¨åˆ° {subs:,} ç²‰ä¸ï¼Œæ—¥å‡æ¶¨ç²‰ {subs_per_day:.1f}",
                "conclusion": "å¿«é€Ÿå¢é•¿æ ‡æ†ï¼Œå¯ç ”ç©¶å…¶å†·å¯åŠ¨ç­–ç•¥",
                "data_basis": [
                    f"é¢‘é“åˆ›å»º: {created_at}",
                    f"é¢‘é“å¹´é¾„: {days} å¤©",
                    f"å½“å‰è®¢é˜…: {subs:,}",
                    f"æ—¥å‡æ¶¨ç²‰: {subs_per_day:.1f}",
                ],
                "calculation": "æ—¥å‡æ¶¨ç²‰ = è®¢é˜…æ•° / é¢‘é“å¤©æ•°",
            }
        return {
            "summary": "",
            "conclusion": "",
            "data_basis": [],
            "calculation": "",
        }

    # ç”Ÿæˆå„æ¦œå•ï¼ˆæ ·æœ¬é‡æå‡åˆ°100+ï¼Œç¡®ä¿ç»Ÿè®¡æ„ä¹‰ï¼‰
    # å‰ç«¯å¯æ ¹æ®éœ€è¦é€‰æ‹©å±•ç¤ºæ•°é‡
    RANKING_LIMIT = 100

    total_views_rank = sorted(channels, key=lambda x: x["total_views"], reverse=True)[:RANKING_LIMIT]
    avg_views_rank = sorted([c for c in channels if c["video_count"] >= 3], key=lambda x: x["avg_views"], reverse=True)[:RANKING_LIMIT]
    video_count_rank = sorted(channels, key=lambda x: x["video_count"], reverse=True)[:RANKING_LIMIT]
    dark_horse_rank = sorted([c for c in channels if c["dark_horse_score"] > 0], key=lambda x: x["dark_horse_score"], reverse=True)[:RANKING_LIMIT]
    efficiency_rank = sorted([c for c in channels if c["subscriber_count"] > 0], key=lambda x: x["efficiency_score"], reverse=True)[:RANKING_LIMIT]

    # å¿«é€Ÿå¢é•¿æ¦œï¼š30å¤©å†…è¾¾åˆ°1000ç²‰ä¸ï¼Œæˆ–æ—¥å‡æ¶¨ç²‰æœ€å¿«çš„é¢‘é“
    fast_growth_rank = sorted(
        [c for c in channels if c.get("subs_per_day", 0) > 0 and c.get("days_since_creation")],
        key=lambda x: x["subs_per_day"],
        reverse=True
    )[:RANKING_LIMIT]

    # ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºæ ·æœ¬é‡ï¼‰
    total_channels = len(channels)
    channels_with_subs = len([c for c in channels if c["subscriber_count"] > 0])

    return {
        # ç»Ÿè®¡å…ƒä¿¡æ¯ - ç”¨äºå‰ç«¯æ˜¾ç¤ºæ ·æœ¬é‡è¯´æ˜
        "stats": {
            "total_channels": total_channels,
            "channels_with_subs": channels_with_subs,
            "ranking_limit": RANKING_LIMIT,
            "sample_description": f"åŸºäº {total_channels} ä¸ªé¢‘é“çš„æ•°æ®ï¼Œå±•ç¤º Top {RANKING_LIMIT}",
        },
        "total_views_rank": {
            "title": "æ€»æ’­æ”¾æ¦œ",
            "emoji": "ğŸ“Š",
            "description": "ç´¯è®¡æ’­æ”¾é‡æœ€é«˜çš„é¢‘é“",
            "ranking_formula": "æŒ‰æ€»æ’­æ”¾é‡é™åºæ’åˆ—",
            "sample_size": len(total_views_rank),
            "channels": [format_channel_with_analysis(c, "total_views") for c in total_views_rank],
        },
        "avg_views_rank": {
            "title": "å¹³å‡æ’­æ”¾æ¦œ",
            "emoji": "âš¡",
            "description": "å•è§†é¢‘å¹³å‡æ’­æ”¾é‡æœ€é«˜",
            "ranking_formula": "å¹³å‡æ’­æ”¾ = æ€»æ’­æ”¾ / è§†é¢‘æ•°ï¼ˆè‡³å°‘3ä¸ªè§†é¢‘ï¼‰",
            "sample_size": len(avg_views_rank),
            "channels": [format_channel_with_analysis(c, "avg_views") for c in avg_views_rank],
        },
        "video_count_rank": {
            "title": "è§†é¢‘æ•°æ¦œ",
            "emoji": "ğŸ“¹",
            "description": "å‘å¸ƒè§†é¢‘æ•°é‡æœ€å¤š",
            "ranking_formula": "æŒ‰è§†é¢‘æ•°é‡é™åºæ’åˆ—",
            "sample_size": len(video_count_rank),
            "channels": [format_channel_with_analysis(c, "video_count") for c in video_count_rank],
        },
        "dark_horse_rank": {
            "title": "é»‘é©¬æ¦œ",
            "emoji": "ğŸ´",
            "description": "å°é¢‘é“å¤§çˆ†æ¬¾ï¼Œå†…å®¹é©±åŠ¨å‹",
            "ranking_formula": "é»‘é©¬æŒ‡æ•° = æœ€é«˜æ’­æ”¾ / (è®¢é˜…æ•°+1)ï¼Œä»…é™è®¢é˜…<1ä¸‡",
            "sample_size": len(dark_horse_rank),
            "note": "éœ€è¦è®¢é˜…æ•°æ®" if len(dark_horse_rank) == 0 else None,
            "channels": [format_channel_with_analysis(c, "dark_horse") for c in dark_horse_rank],
        },
        "efficiency_rank": {
            "title": "é«˜æ•ˆæ¦œ",
            "emoji": "ğŸš€",
            "description": "è®¢é˜…è½¬åŒ–æ•ˆç‡æœ€é«˜",
            "ranking_formula": "æ•ˆç‡æŒ‡æ•° = å¹³å‡æ’­æ”¾ / è®¢é˜…æ•°",
            "sample_size": len(efficiency_rank),
            "note": "éœ€è¦è®¢é˜…æ•°æ®" if len(efficiency_rank) == 0 else None,
            "channels": [format_channel_with_analysis(c, "efficiency") for c in efficiency_rank],
        },
        "fast_growth_rank": {
            "title": "å¿«é€Ÿå¢é•¿æ¦œ",
            "emoji": "ğŸ“ˆ",
            "description": "ä»0åˆ°1000ç²‰ä¸æœ€å¿«çš„é¢‘é“",
            "ranking_formula": "æ—¥å‡æ¶¨ç²‰ = è®¢é˜…æ•° / é¢‘é“å¤©æ•°",
            "sample_size": len(fast_growth_rank),
            "note": "éœ€è¦é¢‘é“åˆ›å»ºæ—¶é—´æ•°æ®" if len(fast_growth_rank) == 0 else None,
            "channels": [format_channel_with_analysis(c, "fast_growth") for c in fast_growth_rank],
        },
    }


def _calculate_channel_stability(videos) -> dict:
    """
    è®¡ç®—é¢‘é“ç¨³å®šæ€§åˆ†æ

    ç¨³å®šæ€§æŒ‡æ ‡ï¼šmax/avg æ¯”å€¼
    - < 3: æç¨³å®šï¼ˆæ¯ä¸ªè§†é¢‘è¡¨ç°æ¥è¿‘ï¼‰
    - 3-10: è¾ƒç¨³å®šï¼ˆå¶æœ‰æ³¢åŠ¨ï¼‰
    - 10-20: ä¸ç¨³å®šï¼ˆé çˆ†æ¬¾æ‹‰åŠ¨ï¼‰
    - > 20: æä¸ç¨³å®šï¼ˆé«˜åº¦ä¾èµ–å•ä¸€çˆ†æ¬¾ï¼‰
    """
    # ç»Ÿè®¡æ¯ä¸ªé¢‘é“çš„æ•°æ®
    channel_data = {}
    for v in videos:
        channel_key = v.channel_id if (v.channel_id and v.channel_id != 'None') else v.channel_name
        if not channel_key:
            continue

        if channel_key not in channel_data:
            channel_data[channel_key] = {
                "channel_id": v.channel_id,
                "channel_name": v.channel_name,
                "views": [],
                "total_views": 0,
                "video_count": 0,
            }

        channel_data[channel_key]["views"].append(v.view_count or 0)
        channel_data[channel_key]["total_views"] += v.view_count or 0
        channel_data[channel_key]["video_count"] += 1

    # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡ï¼ˆåªåˆ†æè‡³å°‘æœ‰3ä¸ªè§†é¢‘çš„é¢‘é“ï¼‰
    stability_results = []
    for key, data in channel_data.items():
        if data["video_count"] < 3:
            continue

        views = data["views"]
        avg_views = data["total_views"] / data["video_count"]
        max_views = max(views)
        min_views = min(views)

        # max/avg æ¯”å€¼
        max_avg_ratio = max_views / avg_views if avg_views > 0 else 0

        # åˆ¤æ–­ç¨³å®šæ€§ç­‰çº§
        if max_avg_ratio < 3:
            stability = "æç¨³å®š"
            stability_class = "success"
        elif max_avg_ratio < 10:
            stability = "è¾ƒç¨³å®š"
            stability_class = "highlight"
        elif max_avg_ratio < 20:
            stability = "ä¸ç¨³å®š"
            stability_class = "warning"
        else:
            stability = "æä¸ç¨³å®š"
            stability_class = "danger"

        stability_results.append({
            "channel_name": data["channel_name"],
            "channel_id": data["channel_id"],
            "video_count": data["video_count"],
            "avg_views": int(avg_views),
            "max_views": max_views,
            "min_views": min_views,
            "max_avg_ratio": round(max_avg_ratio, 2),
            "stability": stability,
            "stability_class": stability_class,
        })

    # æŒ‰ max/avg æ¯”å€¼æ’åºï¼ˆä»é«˜åˆ°ä½ï¼Œä¸ç¨³å®šçš„åœ¨å‰ï¼‰
    stability_results.sort(key=lambda x: x["max_avg_ratio"], reverse=True)

    # ç»Ÿè®¡å„ç¨³å®šæ€§ç­‰çº§çš„é¢‘é“æ•°é‡
    stability_distribution = {
        "æç¨³å®š": len([s for s in stability_results if s["stability"] == "æç¨³å®š"]),
        "è¾ƒç¨³å®š": len([s for s in stability_results if s["stability"] == "è¾ƒç¨³å®š"]),
        "ä¸ç¨³å®š": len([s for s in stability_results if s["stability"] == "ä¸ç¨³å®š"]),
        "æä¸ç¨³å®š": len([s for s in stability_results if s["stability"] == "æä¸ç¨³å®š"]),
    }

    return {
        "total_channels": len(stability_results),
        "distribution": stability_distribution,
        "top_unstable": stability_results[:10],  # æœ€ä¸ç¨³å®šçš„ Top 10
        "top_stable": sorted(stability_results, key=lambda x: x["max_avg_ratio"])[:10],  # æœ€ç¨³å®šçš„ Top 10
        "insight": _generate_stability_insight(stability_distribution, stability_results),
    }


def _generate_stability_insight(distribution: dict, results: list) -> dict:
    """ç”Ÿæˆç¨³å®šæ€§åˆ†ææ´å¯Ÿ"""
    total = sum(distribution.values())
    if total == 0:
        return {"summary": "æš‚æ— è¶³å¤Ÿæ•°æ®", "recommendation": ""}

    stable_pct = (distribution["æç¨³å®š"] + distribution["è¾ƒç¨³å®š"]) / total * 100
    unstable_pct = (distribution["ä¸ç¨³å®š"] + distribution["æä¸ç¨³å®š"]) / total * 100

    # æ‰¾å‡ºæœ€ç¨³å®šå’Œæœ€ä¸ç¨³å®šçš„é¢‘é“
    most_stable = sorted(results, key=lambda x: x["max_avg_ratio"])[:3] if results else []
    most_unstable = sorted(results, key=lambda x: x["max_avg_ratio"], reverse=True)[:3] if results else []

    summary = f"{stable_pct:.0f}% çš„é¢‘é“è¡¨ç°ç¨³å®šï¼ˆmax/avg < 10ï¼‰ï¼Œ{unstable_pct:.0f}% çš„é¢‘é“é«˜åº¦ä¾èµ–çˆ†æ¬¾"

    recommendation = "å»ºè®®å­¦ä¹ ç¨³å®šé¢‘é“çš„å†…å®¹ç­–ç•¥ï¼Œè€Œéè¿½æ±‚ä¸€æ¬¡æ€§çˆ†æ¬¾"
    if stable_pct > 70:
        recommendation = "è¯¥é¢†åŸŸé¢‘é“æ™®éç¨³å®šï¼Œå»ºè®®è¿½æ±‚æŒç»­è¾“å‡ºè€Œéç­‰å¾…çˆ†æ¬¾"
    elif unstable_pct > 50:
        recommendation = "è¯¥é¢†åŸŸé«˜åº¦ä¾èµ–çˆ†æ¬¾ï¼Œéœ€è¦æ‰¾åˆ°å¯å¤åˆ¶çš„çˆ†æ¬¾å…¬å¼"

    return {
        "summary": summary,
        "recommendation": recommendation,
        "stable_pct": round(stable_pct, 1),
        "unstable_pct": round(unstable_pct, 1),
        "most_stable_channels": [c["channel_name"] for c in most_stable],
        "most_unstable_channels": [c["channel_name"] for c in most_unstable],
    }


def _analyze_region_distribution(channels: list) -> dict:
    """
    åˆ†æé¢‘é“åœ°åŒºåˆ†å¸ƒï¼ˆTab2 å¥—åˆ©æŒ–æ˜ï¼‰
    """
    region_stats = {}

    for ch in channels:
        country = ch.get("country") or "æœªçŸ¥"
        if country not in region_stats:
            region_stats[country] = {
                "channel_count": 0,
                "total_views": 0,
                "total_videos": 0,
            }
        region_stats[country]["channel_count"] += 1
        region_stats[country]["total_views"] += ch.get("total_views", 0)
        region_stats[country]["total_videos"] += ch.get("video_count", 0)

    # è®¡ç®—å‡æ’­æ”¾
    results = []
    for region, stats in region_stats.items():
        if stats["channel_count"] > 0:
            avg_views = stats["total_views"] // max(stats["total_videos"], 1)
            results.append({
                "region": region,
                "channel_count": stats["channel_count"],
                "avg_views": avg_views,
                "total_views": stats["total_views"],
                "feature": _get_region_feature(region, avg_views, stats["channel_count"]),
            })

    # æŒ‰å‡æ’­æ”¾æ’åº
    results.sort(key=lambda x: x["avg_views"], reverse=True)

    # ç”Ÿæˆæ´å¯Ÿ
    if len(results) >= 2:
        top = results[0]
        second = results[1] if len(results) > 1 else None
        ratio = top["avg_views"] / max(results[-1]["avg_views"], 1) if results else 1
        insight = f"{top['region']}å¸‚åœºå‡æ’­æœ€é«˜ï¼ˆ{top['avg_views']:,}ï¼‰ï¼Œæ˜¯æœ€ä½çš„ {ratio:.1f} å€"
    else:
        insight = "æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•åˆ†æåœ°åŒºå·®å¼‚"

    return {
        "regions": results[:10],  # Top 10 åœ°åŒº
        "insight": insight,
        "total_regions": len(results),
    }


def _get_region_feature(region: str, avg_views: int, channel_count: int) -> str:
    """æ ¹æ®åœ°åŒºç‰¹å¾è¿”å›æè¿°"""
    features = {
        "Taiwan": "ç¹ä½“ä¸»å¸‚åœº",
        "Hong Kong": "ç²¤è¯­å¸‚åœº",
        "United States": "æµ·å¤–åäºº",
        "Singapore": "é«˜è´¨é‡",
        "Malaysia": "ä¸œå—äºš",
        "Canada": "åŒ—ç¾åäºº",
        "Australia": "æ¾³æ´²åäºº",
        "China": "ç®€ä½“å¸‚åœº",
    }
    if region in features:
        return features[region]
    if avg_views > 100000:
        return "å‡æ’­æœ€é«˜"
    if channel_count > 50:
        return "é¢‘é“æœ€å¤š"
    return "æ–°å…´å¸‚åœº"


def _analyze_weekday_performance(videos: list) -> dict:
    """
    åˆ†æä¸åŒæ˜ŸæœŸçš„å‘å¸ƒæ•ˆæœï¼ˆTab5 å‘å¸ƒç­–ç•¥ï¼‰
    """
    weekday_names = ["å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥"]
    weekday_stats = {i: {"count": 0, "total_views": 0, "max_views": 0} for i in range(7)}

    for v in videos:
        if v.published_at:
            weekday = v.published_at.weekday()  # 0=å‘¨ä¸€, 6=å‘¨æ—¥
            views = v.view_count or 0
            weekday_stats[weekday]["count"] += 1
            weekday_stats[weekday]["total_views"] += views
            weekday_stats[weekday]["max_views"] = max(weekday_stats[weekday]["max_views"], views)

    results = []
    for i, stats in weekday_stats.items():
        if stats["count"] > 0:
            avg_views = stats["total_views"] // stats["count"]
            results.append({
                "weekday": weekday_names[i],
                "weekday_index": i,
                "video_count": stats["count"],
                "avg_views": avg_views,
                "max_views": stats["max_views"],
            })

    # æŒ‰å‡æ’­æ”¾æ’åº
    results.sort(key=lambda x: x["avg_views"], reverse=True)

    # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®
    best = results[0] if results else None
    worst = results[-1] if results else None

    insight = ""
    if best and worst and best["avg_views"] > 0:
        ratio = best["avg_views"] / max(worst["avg_views"], 1)
        insight = f"{best['weekday']}å‘å¸ƒæ•ˆæœæœ€ä½³ï¼ˆå‡æ’­{best['avg_views']:,}ï¼‰ï¼Œæ˜¯{worst['weekday']}çš„ {ratio:.1f} å€"

    return {
        "weekdays": results,
        "best_day": best["weekday"] if best else None,
        "worst_day": worst["weekday"] if worst else None,
        "insight": insight,
    }


def _analyze_content_lifecycle(videos: list) -> dict:
    """
    åˆ†æå†…å®¹ç±»å‹ç”Ÿå‘½å‘¨æœŸï¼ˆTab3 é€‰é¢˜å†³ç­–ï¼‰
    """
    from datetime import datetime
    now = datetime.now()

    # æŒ‰å†…å®¹ç±»å‹åˆ†ç»„
    type_stats = {}
    for v in videos:
        content_type = _classify_content_type(v.title or "")
        if content_type not in type_stats:
            type_stats[content_type] = {
                "videos": [],
                "total_views": 0,
                "earliest": None,
                "latest": None,
            }

        type_stats[content_type]["videos"].append(v)
        type_stats[content_type]["total_views"] += v.view_count or 0

        if v.published_at:
            if type_stats[content_type]["earliest"] is None or v.published_at < type_stats[content_type]["earliest"]:
                type_stats[content_type]["earliest"] = v.published_at
            if type_stats[content_type]["latest"] is None or v.published_at > type_stats[content_type]["latest"]:
                type_stats[content_type]["latest"] = v.published_at

    results = []
    for content_type, stats in type_stats.items():
        video_count = len(stats["videos"])
        if video_count < 3:  # è‡³å°‘3ä¸ªè§†é¢‘æ‰åˆ†æ
            continue

        avg_views = stats["total_views"] // video_count

        # è®¡ç®—æ´»è·ƒè·¨åº¦
        if stats["earliest"] and stats["latest"]:
            span_days = (stats["latest"] - stats["earliest"]).days
            if span_days > 365:
                span_text = f"{span_days // 365}å¹´"
            elif span_days > 30:
                span_text = f"{span_days // 30}ä¸ªæœˆ"
            else:
                span_text = f"{span_days}å¤©"
        else:
            span_days = 0
            span_text = "æœªçŸ¥"

        # åˆ¤æ–­é˜¶æ®µ
        if span_days > 365 * 3 and avg_views > 50000:
            stage = "é•¿é’ç»å…¸"
        elif span_days > 365 and avg_views > 30000:
            stage = "æŒç»­å¢é•¿"
        elif span_days < 180 and avg_views > 50000:
            stage = "æ–°å…´çˆ†å‘"
        elif avg_views > 80000:
            stage = "é«˜çƒ­æœŸ"
        else:
            stage = "ç¨³å®šæœŸ"

        results.append({
            "content_type": content_type,
            "video_count": video_count,
            "avg_views": avg_views,
            "span_text": span_text,
            "span_days": span_days,
            "stage": stage,
        })

    # æŒ‰å‡æ’­æ”¾æ’åº
    results.sort(key=lambda x: x["avg_views"], reverse=True)

    # ç”Ÿæˆæ´å¯Ÿ
    evergreen = [r for r in results if r["stage"] == "é•¿é’ç»å…¸"]
    emerging = [r for r in results if r["stage"] == "æ–°å…´çˆ†å‘"]

    insight = ""
    if evergreen:
        insight += f"é•¿é’è¯é¢˜ï¼š{', '.join([e['content_type'] for e in evergreen[:3]])}ã€‚"
    if emerging:
        insight += f"æ–°å…´è¯é¢˜ï¼š{', '.join([e['content_type'] for e in emerging[:3]])}ã€‚"
    if not insight:
        insight = "å„è¯é¢˜å‘å±•å‡è¡¡ï¼Œå»ºè®®å…³æ³¨é«˜å‡æ’­è¯é¢˜ã€‚"

    return {
        "topics": results[:10],
        "insight": insight,
        "evergreen_topics": [e["content_type"] for e in evergreen],
        "emerging_topics": [e["content_type"] for e in emerging],
    }


def _classify_content_type(title: str) -> str:
    """æ ¹æ®æ ‡é¢˜åˆ†ç±»å†…å®¹ç±»å‹"""
    title_lower = title.lower()

    if any(kw in title_lower for kw in ["å…«æ®µé”¦", "å¤ªæ", "æ°”åŠŸ", "åŠŸæ³•", "ç«™æ¡©"]):
        return "åŠŸæ³•å…»ç”Ÿ"
    elif any(kw in title_lower for kw in ["ç©´ä½", "æŒ‰æ‘©", "ç»ç»œ", "æ¨æ‹¿", "é’ˆç¸"]):
        return "ç©´ä½ç»ç»œ"
    elif any(kw in title_lower for kw in ["é£Ÿç–—", "é£Ÿç‰©", "åƒ", "å–", "é¥®é£Ÿ", "è¥å…»"]):
        return "é£Ÿç–—å…»ç”Ÿ"
    elif any(kw in title_lower for kw in ["ä¸­åŒ»", "ä¸­è¯", "è°ƒç†", "ä½“è´¨"]):
        return "ä¸­åŒ»è°ƒç†"
    elif any(kw in title_lower for kw in ["è‚", "è‚¾", "è„¾", "èƒƒ", "å¿ƒ", "è‚º", "è¡€ç®¡", "è¡€å‹"]):
        return "å™¨å®˜ä¿å¥"
    else:
        return "å…¶ä»–"


def _generate_warnings(videos, now) -> list:
    """ç”Ÿæˆé¿å‘æé†’ï¼ˆæœ€å¤š 3 æ¡ï¼‰"""
    warnings = []

    # åˆ†ææ•°æ®
    durations = [v.duration for v in videos if v.duration]
    views = [v.view_count for v in videos if v.view_count]
    recent_videos = [v for v in videos if v.published_at and (now - v.published_at).days <= 90]

    # è­¦å‘Š 1: æ—¶é•¿é™·é˜±
    if durations:
        avg_duration = sum(durations) / len(durations)
        short_count = len([d for d in durations if d < 60])
        if short_count > len(durations) * 0.3:
            warnings.append({
                "icon": "â±ï¸",
                "title": "é¿å…è¿‡çŸ­è§†é¢‘",
                "content": f"è¯¥é¢†åŸŸ {short_count} ä¸ªè§†é¢‘æ—¶é•¿ä¸è¶³ 1 åˆ†é’Ÿï¼Œå¹³å‡æ’­æ”¾é‡åä½ã€‚å»ºè®®è‡³å°‘ 3-5 åˆ†é’Ÿã€‚",
                "severity": "high",
            })

    # è­¦å‘Š 2: ç«äº‰æ¿€çƒˆ
    if views:
        median_views = sorted(views)[len(views) // 2]
        if median_views < 1000:
            warnings.append({
                "icon": "âš ï¸",
                "title": "ç«äº‰æ¿€çƒˆï¼Œæ³¨æ„å·®å¼‚åŒ–",
                "content": f"è¯¥é¢†åŸŸä¸­ä½æ•°æ’­æ”¾é‡ä»… {median_views}ï¼Œè¯´æ˜å¤§éƒ¨åˆ†è§†é¢‘è¡¨ç°ä¸€èˆ¬ã€‚å¿…é¡»æ‰¾åˆ°å·®å¼‚åŒ–è§’åº¦ã€‚",
                "severity": "medium",
            })

    # è­¦å‘Š 3: å‘å¸ƒæ—¶æœº
    if recent_videos:
        recent_success = [v for v in recent_videos if (v.view_count or 0) >= 10000]
        success_rate = len(recent_success) / len(recent_videos) if recent_videos else 0
        if success_rate < 0.1:
            warnings.append({
                "icon": "ğŸ“…",
                "title": "è¿‘æœŸçˆ†æ¬¾ç‡è¾ƒä½",
                "content": f"è¿‘ 90 å¤©ä»… {len(recent_success)}/{len(recent_videos)} ä¸ªè§†é¢‘çªç ´ä¸‡æ’­ï¼Œå¸‚åœºå¯èƒ½è¶‹äºé¥±å’Œã€‚",
                "severity": "medium",
            })

    # å¦‚æœæ²¡æœ‰è­¦å‘Šï¼Œç»™ä¸€ä¸ªé€šç”¨æé†’
    if not warnings:
        warnings.append({
            "icon": "ğŸ’¡",
            "title": "åšæŒåŸåˆ›å†…å®¹",
            "content": "æ•°æ®æ˜¾ç¤ºè¯¥é¢†åŸŸæ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œå»ºè®®ä¸“æ³¨å†…å®¹è´¨é‡å’ŒæŒç»­è¾“å‡ºã€‚",
            "severity": "low",
        })

    return warnings[:3]


# ============================================================
# ç”¨æˆ·æ´å¯Ÿ APIï¼ˆè¯„è®ºåˆ†æï¼‰
# ============================================================

@app.get("/api/user-insights/{keyword}")
async def get_user_insights(
    keyword: str,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None
):
    """
    ç”¨æˆ·æ´å¯Ÿ API - åˆ†æè¯„è®ºæ•°æ®

    å‚æ•°ï¼š
    - keyword: å…³é”®è¯
    - date_from: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
    - date_to: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)

    è¿”å›ï¼š
    1. çƒ­è¯åˆ†æï¼ˆæ¨¡å¼38ï¼‰
    2. é—®é¢˜ç±»å‹åˆ†å¸ƒï¼ˆæ¨¡å¼39ï¼‰
    3. æƒ…æ„Ÿåˆ†å¸ƒï¼ˆæ¨¡å¼40ï¼‰
    4. è¯é¢˜è¶‹åŠ¿ï¼ˆæ¨¡å¼41ï¼‰
    5. é«˜èµè¯„è®ºç‰¹å¾ï¼ˆæ¨¡å¼42ï¼‰
    6. ç”¨æˆ·è¯­è¨€åˆ†å¸ƒï¼ˆæ¨¡å¼43ï¼‰
    """
    import re
    from collections import Counter, defaultdict

    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å¯ç”¨ï¼ˆNeon æˆ–æœ¬åœ° SQLiteï¼‰
    if not db_exists():
        return {"status": "error", "message": "æ•°æ®åº“ä¸å­˜åœ¨"}

    conn = db_get_connection()
    cursor = conn.cursor()

    # æ„å»ºæ—¶é—´è¿‡æ»¤æ¡ä»¶
    date_conditions = []
    params = [f"%{keyword}%", f"%{keyword}%", f"%{keyword}%"]

    if date_from:
        date_conditions.append("cv.published_at >= ?")
        params.append(date_from)
    if date_to:
        date_conditions.append("cv.published_at <= ?")
        params.append(date_to + " 23:59:59")

    date_filter = " AND " + " AND ".join(date_conditions) if date_conditions else ""

    # è·å–ä¸å…³é”®è¯ç›¸å…³è§†é¢‘çš„è¯„è®ºï¼ˆæ”¯æŒæ—¶é—´è¿‡æ»¤ï¼‰
    cursor.execute(f"""
        SELECT vc.text, vc.like_count, vc.published_at
        FROM video_comments vc
        JOIN competitor_videos cv ON vc.youtube_id = cv.youtube_id
        WHERE (cv.title LIKE ? OR cv.keyword_source LIKE ? OR cv.theme LIKE ?)
        AND vc.text IS NOT NULL
        {date_filter}
    """, params)

    comments = cursor.fetchall()

    # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œè·å–æ‰€æœ‰è¯„è®ºï¼ˆä¿æŒæ—¶é—´è¿‡æ»¤ï¼‰
    if len(comments) < 100:
        fallback_params = []
        fallback_conditions = ["text IS NOT NULL"]
        if date_from:
            fallback_conditions.append("published_at >= ?")
            fallback_params.append(date_from)
        if date_to:
            fallback_conditions.append("published_at <= ?")
            fallback_params.append(date_to + " 23:59:59")

        cursor.execute(f"""
            SELECT text, like_count, published_at
            FROM video_comments
            WHERE {' AND '.join(fallback_conditions)}
        """, fallback_params)
        comments = cursor.fetchall()

    if not comments:
        conn.close()
        return {
            "status": "ok",
            "total_comments": 0,
            "message": "æš‚æ— è¯„è®ºæ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ python3 scripts/fetch_comments.py é‡‡é›†è¯„è®º"
        }

    total_comments = len(comments)

    # ========== æ¨¡å¼38ï¼šçƒ­è¯åˆ†æ ==========
    try:
        import jieba
        all_text = ' '.join([c[0] for c in comments if c[0]])
        words = jieba.lcut(all_text)
        stopwords = {'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'æˆ‘', 'æœ‰', 'ä¹Ÿ', 'ä¸', 'å°±', 'éƒ½', 'è¿™', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'å¾ˆ', 'ä¼š', 'è¦', 'å¯ä»¥', 'èƒ½', 'åˆ°', 'è¯´', 'å»', 'å¯¹', 'è¢«', 'è®©', 'æŠŠ', 'ç»™', 'ä»', 'ä½†', 'è¿˜', 'ä¸º', 'ä¸', 'ç€', 'å¾—', 'ä¸Š', 'ä¸‹', 'æ¥', 'å‡º', 'ä¸ª', 'ä»¬', 'ä¹ˆ', 'é‚£', 'è¿™ä¸ª', 'ä¸€', 'ä¸€ä¸ª', '\n', ' ', '', 'å¯ä»¥', 'è§‰å¾—', 'è¿™æ ·', 'è‡ªå·±', 'ç°åœ¨', 'çŸ¥é“', 'æ²¡æœ‰', 'çœŸçš„', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶', 'ä½†æ˜¯', 'è€Œä¸”', 'æˆ–è€…', 'åªæ˜¯', 'è¿™ä¹ˆ', 'é‚£ä¹ˆ', 'éå¸¸', 'å·²ç»', 'ä»¥å', 'ä¹‹å‰', 'ä¸€äº›', 'ä¸€æ ·', 'å¸Œæœ›', 'å¤§å®¶'}
        words = [w.strip() for w in words if len(w.strip()) >= 2 and w.strip() not in stopwords and not any(c in w for c in '\n\r\t')]
        word_counts = Counter(words).most_common(20)

        # å½’ç±»çƒ­è¯
        hotwords = []
        category_map = {
            'æ„Ÿæ©': 'äº’åŠ¨', 'è¬è¬': 'äº’åŠ¨', 'è°¢è°¢': 'äº’åŠ¨', 'æ„Ÿè¬': 'äº’åŠ¨', 'æ„Ÿè°¢': 'äº’åŠ¨',
            'è€å¸«': 'äº’åŠ¨', 'è€å¸ˆ': 'äº’åŠ¨', 'é†«å¸«': 'äº’åŠ¨', 'åŒ»å¸ˆ': 'äº’åŠ¨', 'é†«ç”Ÿ': 'äº’åŠ¨', 'åŒ»ç”Ÿ': 'äº’åŠ¨',
            'åˆ†äº«': 'äº’åŠ¨', 'å¥åº·': 'æ•ˆæœ', 'èº«é«”': 'æ•ˆæœ', 'èº«ä½“': 'æ•ˆæœ', 'æœ‰æ•ˆ': 'æ•ˆæœ',
            'è«‹å•': 'ç–‘é—®', 'è¯·é—®': 'ç–‘é—®', 'æ¯å¤©': 'è¡ŒåŠ¨', 'é‹å‹•': 'åŠŸæ³•', 'è¿åŠ¨': 'åŠŸæ³•'
        }
        for i, (word, count) in enumerate(word_counts, 1):
            category = category_map.get(word, 'å…¶ä»–')
            hotwords.append({
                "rank": i,
                "word": word,
                "count": count,
                "category": category
            })
    except ImportError:
        hotwords = []

    # ========== æ¨¡å¼39ï¼šé—®é¢˜ç±»å‹åˆ†æ ==========
    question_patterns = {
        'YES_NO': r'[å¯ä»¥å—|å¯ä»¥å—|è¡Œå—|è¡Œå—|å¥½å—|å¥½å—|å°å—|å¯¹å—|æ˜¯å—|æ˜¯å—|æœƒå—|ä¼šå—|èƒ½å—|èƒ½å—|å—\?|å—\?]',
        'HOW': r'[æ€éº¼|æ€ä¹ˆ|å¦‚ä½•|æ€æ¨£|æ€æ ·|å’‹]',
        'WHY': r'[ç‚ºä»€éº¼|ä¸ºä»€ä¹ˆ|ç‚ºä½•|ä¸ºä½•|ä½•æ•…]',
        'WHAT': r'[ä»€éº¼æ˜¯|ä»€ä¹ˆæ˜¯|æ˜¯ä»€éº¼|æ˜¯ä»€ä¹ˆ|å•¥æ˜¯]',
        'WHERE': r'[å“ªè£¡|å“ªé‡Œ|å“ªå…’|å“ªå„¿|ä½•è™•|ä½•å¤„|åœ¨å“ª]'
    }

    question_counts = {k: 0 for k in question_patterns}
    question_counts['OTHER'] = 0
    total_questions = 0

    for text, _, _ in comments:
        if not text:
            continue
        if '?' in text or 'ï¼Ÿ' in text or 'å—' in text or 'å—' in text:
            total_questions += 1
            matched = False
            for qtype, pattern in question_patterns.items():
                if re.search(pattern, text):
                    question_counts[qtype] += 1
                    matched = True
                    break
            if not matched:
                question_counts['OTHER'] += 1

    questions_data = {
        "total": total_questions,
        "percentage": round(total_questions / total_comments * 100, 1) if total_comments > 0 else 0,
        "types": [
            {"type": "YES_NO", "count": question_counts['YES_NO'], "label": "ç¡®è®¤ç±»ã€Œå¯ä»¥å—ï¼Ÿã€"},
            {"type": "HOW", "count": question_counts['HOW'], "label": "æ–¹æ³•ç±»ã€Œæ€ä¹ˆåšï¼Ÿã€"},
            {"type": "WHERE", "count": question_counts['WHERE'], "label": "èµ„æºç±»ã€Œå“ªé‡Œæ‰¾ï¼Ÿã€"},
            {"type": "WHAT", "count": question_counts['WHAT'], "label": "å®šä¹‰ç±»ã€Œæ˜¯ä»€ä¹ˆï¼Ÿã€"},
            {"type": "WHY", "count": question_counts['WHY'], "label": "åŸå› ç±»ã€Œä¸ºä»€ä¹ˆï¼Ÿã€"},
            {"type": "OTHER", "count": question_counts['OTHER'], "label": "å…¶ä»–é—®å¥"}
        ]
    }

    # ========== æ¨¡å¼40ï¼šæƒ…æ„Ÿåˆ†æ ==========
    positive_words = ['è¬è¬', 'è°¢è°¢', 'æ„Ÿæ©', 'æ„Ÿè¬', 'æ„Ÿè°¢', 'æ£’', 'å¥½', 'è®š', 'èµ', 'å–œæ­¡', 'å–œæ¬¢', 'æœ‰æ•ˆ', 'å²å®³', 'å‰å®³', 'æ¸…æ¥š', 'å¯¦ç”¨', 'å®ç”¨', 'å—ç›Š', 'å¹«åŠ©', 'å¸®åŠ©']
    negative_words = ['çˆ›', 'çƒ‚', 'å·®', 'å»¢', 'åºŸ', 'æ²’ç”¨', 'æ²¡ç”¨', 'é¨™', 'éª—', 'å‡', 'åƒåœ¾', 'ç„¡èŠ', 'æ— èŠ', 'å¤±æœ›']

    pos_count = 0
    neg_count = 0
    neu_count = 0

    for text, _, _ in comments:
        if not text:
            continue
        has_pos = any(w in text for w in positive_words)
        has_neg = any(w in text for w in negative_words)
        if has_pos and not has_neg:
            pos_count += 1
        elif has_neg and not has_pos:
            neg_count += 1
        else:
            neu_count += 1

    sentiment_data = {
        "positive": {"count": pos_count, "percentage": round(pos_count / total_comments * 100, 1)},
        "neutral": {"count": neu_count, "percentage": round(neu_count / total_comments * 100, 1)},
        "negative": {"count": neg_count, "percentage": round(neg_count / total_comments * 100, 1)},
        "score": round((pos_count - neg_count) / total_comments, 3) if total_comments > 0 else 0
    }

    # ========== æ¨¡å¼43ï¼šç”¨æˆ·è¯­è¨€åˆ†å¸ƒ ==========
    def detect_language(text):
        """
        æ£€æµ‹è¯„è®ºè¯­è¨€ï¼ˆåŸºäºå­—ç¬¦é›†åˆ†æï¼‰

        é€»è¾‘ï¼š
        1. å¦‚æœæœ‰æ—¥æ–‡å‡å â†’ æ—¥è¯­
        2. å¦‚æœæœ‰éŸ©æ–‡å­—ç¬¦ â†’ éŸ©è¯­
        3. å¦‚æœæœ‰CJKæ±‰å­— â†’ ä¸­æ–‡ï¼ˆå†åŒºåˆ†ç®€ç¹ä½“ï¼‰
        4. å¦‚æœæœ‰è‹±æ–‡å­—æ¯ â†’ è‹±è¯­
        5. çº¯è¡¨æƒ…/çº¯æ•°å­—/çº¯æ ‡ç‚¹ â†’ æ ¹æ®å†…å®¹é•¿åº¦åˆ¤æ–­
        """
        if not text or not text.strip():
            return 'unknown'

        # ç»Ÿè®¡å„ç±»å­—ç¬¦
        cjk_count = 0       # ä¸­æ—¥éŸ©æ±‰å­—æ€»æ•°
        english = 0         # è‹±æ–‡å­—æ¯
        japanese_kana = 0   # æ—¥æ–‡å‡åï¼ˆå¹³å‡å+ç‰‡å‡åï¼‰
        korean = 0          # éŸ©æ–‡

        # ç®€ä½“ç‰¹å¾å­—ï¼ˆå¤§é™†ç®€åŒ–å­—ï¼‰
        simplified_chars = set(
            'å›½ä¸ºå­¦è¿™æ¥è¯´æ—¶ä¼šå¼€å…³é—¨é—®ä¸œè½¦é•¿é©¬é£è§è®©è®¤è¯†è¯­è¯»å†™ç”»å›¾ç”µè§†å¬çˆ±é’±é“¶'
            'å›¢ç»„ç»‡å‘˜å†³è®¾è®¡ä»è¾¾è¿›è¿œè¿è¿è¿˜è¾¹è¿‡åä¼—å‚åŠå¤„å¤‡å¤å¤´å‘ç°å®åº”è¯¥æ¡çº§çº¸'
            'ç¼–ç»ƒä¹ åŠ¡ç¡®éªŒè¯è°ˆæŠ¤è½¬è½»è¾ƒå†œè®²è·æ•°å“å¼¹åŠ³å«è§„åˆ™æ€»ç»“æ„èŠ‚çº¦ä¸“ä¸šå‹åŒ»è¯'
            'å‚å¹¿äº§ä¸šå•æŠ¥å¯¼å¤´å·å¼ æœºæ¨æ¡£ææ ‡æƒæ±‡æ±‰æ²Ÿæµç‚¹çƒ­è¥ç¯çº¿ç»†ç»ˆç»ç½‘ç»“è·'
        )
        # ç¹ä½“ç‰¹å¾å­—ï¼ˆå°æ¸¯æ¾³ç¹ä½“å­—ï¼‰
        traditional_chars = set(
            'åœ‹ç‚ºå­¸é€™ä¾†èªªæ™‚æœƒé–‹é—œé–€å•æ±è»Šé•·é¦¬é¢¨è¦‹è®“èªè­˜èªè®€å¯«ç•«åœ–é›»è¦–è½æ„›éŒ¢éŠ€'
            'åœ˜çµ„ç¹”å“¡æ±ºè¨­è¨ˆå¾é”é€²é é€£é‹é‚„é‚Šéè¯çœ¾åƒè¾¦è™•å‚™å¾©é ­ç™¼ç¾å¯¦æ‡‰è©²æ¢ç´šç´™'
            'ç·¨ç·´ç¿’å‹™ç¢ºé©—è­‰è«‡è­·è½‰è¼•è¼ƒè¾²è¬›ç²æ•¸éŸ¿å½ˆå‹è¡›è¦å‰‡ç¸½çµæ§‹ç¯€ç´„å°ˆæ¥­å£“é†«è—¥'
            'å» å»£ç”¢æ¥­å–®å ±å°é ­è™Ÿå¼µæ©Ÿæ¥Šæª”æ¥µæ¨™æ¬ŠåŒ¯æ¼¢æºæ¿Ÿé»ç†±ç‡Ÿç’°ç·šç´°çµ‚ç¶“ç¶²çµç²'
        )

        zh_simplified_score = 0
        zh_traditional_score = 0

        for char in text:
            code = ord(char)
            # è‹±æ–‡å­—æ¯ (A-Z, a-z)
            if (0x0041 <= code <= 0x005A) or (0x0061 <= code <= 0x007A):
                english += 1
            # æ—¥æ–‡å¹³å‡å (3040-309F) å’Œç‰‡å‡å (30A0-30FF)
            elif 0x3040 <= code <= 0x30FF:
                japanese_kana += 1
            # éŸ©æ–‡ (AC00-D7AF)
            elif 0xAC00 <= code <= 0xD7AF:
                korean += 1
            # ä¸­æ—¥éŸ©æ±‰å­— (4E00-9FFF) - åŒ…å«æ‰€æœ‰æ±‰å­—
            elif 0x4E00 <= code <= 0x9FFF:
                cjk_count += 1
                if char in simplified_chars:
                    zh_simplified_score += 1
                elif char in traditional_chars:
                    zh_traditional_score += 1

        # åˆ¤æ–­è¯­è¨€ä¼˜å…ˆçº§ï¼šæ—¥è¯­ > éŸ©è¯­ > ä¸­æ–‡ > è‹±è¯­

        # 1. æœ‰æ—¥æ–‡å‡å â†’ æ—¥è¯­ï¼ˆæ—¥è¯­ç‰¹æœ‰ï¼Œå³ä½¿æ··æœ‰æ±‰å­—ä¹Ÿæ˜¯æ—¥è¯­ï¼‰
        if japanese_kana >= 2:
            return 'ja'

        # 2. æœ‰éŸ©æ–‡ â†’ éŸ©è¯­
        if korean >= 2:
            return 'ko'

        # 3. æœ‰æ±‰å­— â†’ ä¸­æ–‡ï¼ˆåŒºåˆ†ç®€ç¹ä½“ï¼‰
        if cjk_count > 0:
            # æ ¹æ®ç‰¹å¾å­—åˆ†æ•°åŒºåˆ†ç®€ç¹ä½“
            if zh_traditional_score > zh_simplified_score:
                return 'zh-TW'  # ç¹ä½“ä¸­æ–‡
            else:
                return 'zh-CN'  # ç®€ä½“ä¸­æ–‡ï¼ˆé»˜è®¤ï¼Œå› ä¸ºå¤§éƒ¨åˆ†æ±‰å­—æ˜¯é€šç”¨çš„ï¼‰

        # 4. æœ‰è‹±æ–‡å­—æ¯ â†’ è‹±è¯­
        if english > 0:
            return 'en'

        # 5. çº¯è¡¨æƒ…/æ•°å­—/æ ‡ç‚¹ â†’ å½’å…¥"è¡¨æƒ…"ç±»åˆ«ï¼ˆè¿™æ˜¯çœŸæ­£çš„"æ— æ³•åˆ¤æ–­æ–‡å­—è¯­è¨€"ï¼‰
        return 'emoji'

    # ç»Ÿè®¡è¯­è¨€åˆ†å¸ƒ
    language_counts = defaultdict(int)
    for text, _, _ in comments:
        if text:
            lang = detect_language(text)
            language_counts[lang] += 1

    # è¯­è¨€åç§°æ˜ å°„
    lang_names = {
        'zh-CN': 'ç®€ä½“ä¸­æ–‡',
        'zh-TW': 'ç¹ä½“ä¸­æ–‡',
        'en': 'è‹±è¯­',
        'ja': 'æ—¥è¯­',
        'ko': 'éŸ©è¯­',
        'emoji': 'çº¯è¡¨æƒ…/ç¬¦å·',  # æ²¡æœ‰æ–‡å­—å†…å®¹ï¼Œåªæœ‰è¡¨æƒ…æˆ–ç¬¦å·
        'unknown': 'æœªçŸ¥'
    }

    language_data = {
        "total": total_comments,
        "distribution": [
            {
                "code": code,
                "name": lang_names.get(code, code),
                "count": count,
                "percentage": round(count / total_comments * 100, 1) if total_comments > 0 else 0
            }
            for code, count in sorted(language_counts.items(), key=lambda x: -x[1])
        ]
    }

    # ========== æ¨¡å¼41ï¼šè¯é¢˜è¶‹åŠ¿ ==========
    topics = {
        'å…»ç”Ÿ': r'é¤Šç”Ÿ|å…»ç”Ÿ|ä¿å¥|å¥åº·',
        'ç¡çœ ': r'ç¡çœ |å¤±çœ |ç¡ä¸è‘—|ç¡ä¸ç€|å…¥ç¡',
        'ç©´ä½': r'ç©´ä½|ç©´é“|æŒ‰æ‘©',
        'å…«æ®µé”¦': r'å…«æ®µéŒ¦|å…«æ®µé”¦',
        'å¤ªæ': r'å¤ªæ¥µ|å¤ªæ|å¤ªæ¥µæ‹³|å¤ªææ‹³',
        'æ°”åŠŸ': r'æ°£åŠŸ|æ°”åŠŸ'
    }

    monthly_topics = defaultdict(lambda: defaultdict(int))

    for text, _, pub_time in comments:
        if not text or not pub_time:
            continue
        month_match = re.search(r'(\d{4})-(\d{2})', str(pub_time))
        if month_match:
            month_key = f"{month_match.group(1)}-{month_match.group(2)}"
            for topic, pattern in topics.items():
                if re.search(pattern, text):
                    monthly_topics[month_key][topic] += 1

    sorted_months = sorted(monthly_topics.keys())[-6:] if monthly_topics else []

    trend_data = {
        "months": sorted_months,
        "topics": {}
    }
    for topic in topics:
        trend_data["topics"][topic] = [monthly_topics[m].get(topic, 0) for m in sorted_months]

    # ========== æ¨¡å¼42ï¼šé«˜èµè¯„è®ºç‰¹å¾ ==========
    cursor.execute("""
        SELECT text, like_count FROM video_comments
        WHERE like_count > 0 AND text IS NOT NULL
        ORDER BY like_count DESC LIMIT 100
    """)
    top_comments = cursor.fetchall()

    if top_comments:
        avg_len = sum(len(t[0]) for t in top_comments) / len(top_comments)
        experience_words = ['æˆ‘', 'è‡ªå·±', 'è¦ªèº«', 'äº²èº«', 'è©¦é', 'è¯•è¿‡', 'ç¶“æ­·', 'ç»å†', 'å¯¦è¸', 'å®è·µ']
        has_experience = sum(1 for t in top_comments if any(w in t[0] for w in experience_words))
        has_question = sum(1 for t in top_comments if '?' in t[0] or 'ï¼Ÿ' in t[0])

        high_liked_data = {
            "avg_length": round(avg_len),
            "has_experience_pct": has_experience,
            "has_question_pct": has_question,
            "max_likes": top_comments[0][1] if top_comments else 0
        }
    else:
        high_liked_data = {
            "avg_length": 0,
            "has_experience_pct": 0,
            "has_question_pct": 0,
            "max_likes": 0,
            "examples": []
        }

    # ========== çœŸå®æ¡ˆä¾‹æ•°æ® ==========

    # è¯„è®ºæœ€å¤šçš„è§†é¢‘ï¼ˆäº’åŠ¨çƒ­é—¨ï¼‰
    # PostgreSQL è¦æ±‚ GROUP BY åŒ…å«æ‰€æœ‰éèšåˆåˆ—
    cursor.execute("""
        SELECT cv.title, cv.channel_name, cv.view_count, cv.youtube_id, COUNT(vc.id) as comment_count
        FROM competitor_videos cv
        JOIN video_comments vc ON cv.youtube_id = vc.youtube_id
        GROUP BY cv.youtube_id, cv.title, cv.channel_name, cv.view_count
        ORDER BY comment_count DESC
        LIMIT 5
    """)
    top_commented_videos = [
        {"title": r[0][:50] + "..." if len(r[0]) > 50 else r[0], "channel": r[1], "views": r[2], "youtube_id": r[3], "comments": r[4]}
        for r in cursor.fetchall()
    ]

    # é«˜èµè¯„è®ºæ¡ˆä¾‹ï¼ˆå¸¦è§†é¢‘æ¥æºï¼‰
    cursor.execute("""
        SELECT vc.text, vc.like_count, cv.title, cv.channel_name, cv.youtube_id
        FROM video_comments vc
        JOIN competitor_videos cv ON vc.youtube_id = cv.youtube_id
        WHERE vc.like_count > 50 AND length(vc.text) > 20 AND length(vc.text) < 200
        ORDER BY vc.like_count DESC
        LIMIT 5
    """)
    top_liked_comments = [
        {"text": r[0][:100] + "..." if len(r[0]) > 100 else r[0], "likes": r[1], "video_title": r[2][:40] + "..." if len(r[2]) > 40 else r[2], "channel": r[3], "youtube_id": r[4]}
        for r in cursor.fetchall()
    ]

    # ç”¨æˆ·é—®é¢˜æ¡ˆä¾‹ï¼ˆå¸¦è§†é¢‘æ¥æºï¼‰
    cursor.execute("""
        SELECT vc.text, cv.title, cv.channel_name, cv.youtube_id
        FROM video_comments vc
        JOIN competitor_videos cv ON vc.youtube_id = cv.youtube_id
        WHERE (vc.text LIKE '%å¯ä»¥å—%' OR vc.text LIKE '%å¯ä»¥å—%' OR vc.text LIKE '%æ€éº¼%' OR vc.text LIKE '%æ€ä¹ˆ%' OR vc.text LIKE '%å¦‚ä½•%')
        AND length(vc.text) > 10 AND length(vc.text) < 100
        LIMIT 5
    """)
    question_examples = [
        {"text": r[0], "video_title": r[1][:40] + "..." if len(r[1]) > 40 else r[1], "channel": r[2], "youtube_id": r[3]}
        for r in cursor.fetchall()
    ]

    # æ›´æ–°é«˜èµæ•°æ®æ·»åŠ æ¡ˆä¾‹
    if top_comments:
        high_liked_data["examples"] = [
            {"text": t[0][:150] + "..." if len(t[0]) > 150 else t[0], "likes": t[1]}
            for t in top_comments[:3]
        ]

    conn.close()

    return {
        "status": "ok",
        "keyword": keyword,
        "total_comments": total_comments,
        "hotwords": hotwords,
        "questions": questions_data,
        "sentiment": sentiment_data,
        "language": language_data,  # æ¨¡å¼43ï¼šè¯­è¨€åˆ†å¸ƒ
        "trends": trend_data,
        "high_liked": high_liked_data,
        "real_examples": {
            "top_commented_videos": top_commented_videos,
            "top_liked_comments": top_liked_comments,
            "question_examples": question_examples
        }
    }


# ============================================================
# è¯é¢˜ç½‘ç»œåˆ†æ APIï¼ˆæœ‰è¶£åº¦è®¡ç®—ï¼‰
# ============================================================

@app.get("/api/topic-network/{theme}")
async def analyze_topic_network(theme: str, min_cooccurrence: int = 2, top_n: int = 30):
    """
    è¯é¢˜ç½‘ç»œåˆ†æ - è®¡ç®—è¯é¢˜æœ‰è¶£åº¦

    æœ‰è¶£åº¦ = ä¸­ä»‹ä¸­å¿ƒæ€§ / ç¨‹åº¦ä¸­å¿ƒæ€§
    é«˜æœ‰è¶£åº¦ = é«˜æ¡¥æ¢ä»·å€¼ + ä½ä¼ æ’­é¥±å’Œåº¦ = è¢«ä½ä¼°çš„é«˜ä»·å€¼èŠ‚ç‚¹
    """
    try:
        import networkx as nx
        from collections import defaultdict, Counter

        collector = _get_data_collector()
        all_videos = collector.get_videos_from_db(theme=theme, min_views=0, limit=10000)

        if not all_videos:
            all_videos = collector.get_videos_from_db(keyword_like=theme, min_views=0, limit=10000)

        if not all_videos:
            return {"status": "error", "message": f"æ²¡æœ‰æ‰¾åˆ°ä¸»é¢˜ '{theme}' çš„æ•°æ®"}

        # 1. æå–æ ‡ç­¾å¹¶ç»Ÿè®¡
        tag_counter = Counter()
        tag_views = defaultdict(int)

        for video in all_videos:
            tags = video.tags or []
            for tag in tags:
                tag = tag.strip().lower()
                if 2 <= len(tag) <= 50:
                    tag_counter[tag] += 1
                    tag_views[tag] += video.view_count or 0

        if len(tag_counter) < 10:
            return {"status": "error", "message": f"æ ‡ç­¾æ•°æ®ä¸è¶³ï¼ˆä»… {len(tag_counter)} ä¸ªï¼‰"}

        # 2. æ„å»ºå…±ç°ç½‘ç»œ
        G = nx.Graph()
        cooccurrence = defaultdict(int)

        for video in all_videos:
            tags = [t.strip().lower() for t in (video.tags or []) if 2 <= len(t.strip()) <= 50]
            for i, tag1 in enumerate(tags):
                for tag2 in tags[i+1:]:
                    if tag1 != tag2:
                        key = tuple(sorted([tag1, tag2]))
                        cooccurrence[key] += 1

        # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
        for (tag1, tag2), count in cooccurrence.items():
            if count >= min_cooccurrence:
                if not G.has_node(tag1):
                    G.add_node(tag1, video_count=tag_counter[tag1], total_views=tag_views[tag1])
                if not G.has_node(tag2):
                    G.add_node(tag2, video_count=tag_counter[tag2], total_views=tag_views[tag2])
                G.add_edge(tag1, tag2, weight=count)

        if G.number_of_nodes() < 5:
            return {"status": "error", "message": "ç½‘ç»œèŠ‚ç‚¹ä¸è¶³ï¼Œè¯·é™ä½ min_cooccurrence"}

        # 3. è®¡ç®—ä¸­å¿ƒæ€§
        k = min(300, G.number_of_nodes())
        betweenness = nx.betweenness_centrality(G, k=k, normalized=True)
        degree = nx.degree_centrality(G)

        # 4. è®¡ç®—æœ‰è¶£åº¦
        epsilon = 0.001
        interestingness = {node: betweenness.get(node, 0) / (degree.get(node, 0) + epsilon) for node in G.nodes()}

        # 5. æ’åº Top N
        sorted_topics = sorted(interestingness.items(), key=lambda x: x[1], reverse=True)[:top_n]

        # 6. æ„å»ºç»“æœ
        topic_rankings = []
        for rank, (topic, score) in enumerate(sorted_topics, 1):
            node_data = G.nodes[topic]
            video_count = node_data.get("video_count", 0)
            total_views = node_data.get("total_views", 0)

            # è¡ŒåŠ¨å»ºè®®
            if score > 1.0:
                action = {"level": "high", "emoji": "ğŸ”¥", "text": "ç«‹å³åˆ›ä½œ"}
            elif score > 0.3:
                action = {"level": "medium", "emoji": "ğŸ’¡", "text": "å€¼å¾—å°è¯•"}
            else:
                action = {"level": "low", "emoji": "âšª", "text": "éœ€å·®å¼‚åŒ–"}

            topic_rankings.append({
                "rank": rank,
                "topic": topic,
                "interestingness": round(score, 3),
                "betweenness": round(betweenness.get(topic, 0), 4),
                "degree": round(degree.get(topic, 0), 4),
                "video_count": video_count,
                "total_views": total_views,
                "avg_views": total_views // max(video_count, 1),
                "action": action
            })

        return {
            "status": "ok",
            "theme": theme,
            "topic_rankings": topic_rankings,
            "network_stats": {
                "total_nodes": G.number_of_nodes(),
                "total_edges": G.number_of_edges(),
                "density": round(nx.density(G), 4),
            },
            "formula": "æœ‰è¶£åº¦ = ä¸­ä»‹ä¸­å¿ƒæ€§ / ç¨‹åº¦ä¸­å¿ƒæ€§"
        }

    except ImportError:
        return {"status": "error", "message": "éœ€è¦å®‰è£… networkx: pip install networkx"}
    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


# ============================================================
# å…¨å±€æ¦œå• API
# ============================================================

@app.get("/api/leaderboard")
async def get_leaderboard():
    """
    è·å–å…¨å±€æ¦œå•æ•°æ®

    è¿”å›ï¼š
    - Top 50 é»‘é©¬é¢‘é“ï¼ˆè®¢é˜…å°‘ä½†æ’­æ”¾å¥½çš„é¢‘é“ï¼‰
    - Top 50 çƒ­é—¨è§†é¢‘ï¼ˆæŒ‰æ’­æ”¾é‡ï¼‰
    - Top 10 çƒ­é—¨è¯é¢˜ï¼ˆæŒ‰è§†é¢‘æ•°/æ’­æ”¾é‡ï¼‰
    """
    try:
        db_path = Path(__file__).parent / "data" / "youtube_pipeline.db"
        conn = db_get_connection(str(db_path))
        cursor = conn.cursor()

        # ========== Top 50 é»‘é©¬é¢‘é“ ==========
        # é»‘é©¬æŒ‡æ•° = å¹³å‡æ’­æ”¾é‡ / max(è®¢é˜…æ•°, 1000)
        # åªé€‰æ‹©è‡³å°‘æœ‰2ä¸ªè§†é¢‘çš„é¢‘é“ï¼Œä¸”æœ‰è®¢é˜…æ•°æ®
        cursor.execute("""
            SELECT
                channel_name,
                channel_id,
                COUNT(*) as video_count,
                AVG(view_count) as avg_views,
                MAX(view_count) as max_views,
                SUM(view_count) as total_views,
                MAX(subscriber_count) as subscriber_count,
                AVG(view_count) * 1.0 / GREATEST(MAX(subscriber_count), 1000) as dark_horse_score
            FROM competitor_videos
            WHERE channel_name IS NOT NULL
              AND channel_name != ''
              AND view_count > 0
            GROUP BY channel_id, channel_name
            HAVING COUNT(*) >= 2 AND MAX(subscriber_count) > 0
            ORDER BY dark_horse_score DESC
            LIMIT 50
        """)

        dark_horse_channels = []
        for row in cursor.fetchall():
            channel_name, channel_id, video_count, avg_views, max_views, total_views, subscriber_count, score = row
            dark_horse_channels.append({
                "rank": len(dark_horse_channels) + 1,
                "channel_name": channel_name,
                "channel_id": channel_id,
                "channel_url": f"https://www.youtube.com/channel/{channel_id}" if channel_id else None,
                "video_count": video_count,
                "avg_views": int(avg_views or 0),
                "max_views": int(max_views or 0),
                "total_views": int(total_views or 0),
                "subscriber_count": int(subscriber_count or 0),
                "dark_horse_score": round(score or 0, 2)
            })

        # ========== Top 50 çƒ­é—¨è§†é¢‘ ==========
        cursor.execute("""
            SELECT
                youtube_id,
                title,
                channel_name,
                channel_id,
                view_count,
                like_count,
                comment_count,
                subscriber_count,
                published_at,
                duration
            FROM competitor_videos
            WHERE view_count > 0
            ORDER BY view_count DESC
            LIMIT 50
        """)

        top_videos = []
        for row in cursor.fetchall():
            youtube_id, title, channel_name, channel_id, view_count, like_count, comment_count, subscriber_count, published_at, duration = row
            top_videos.append({
                "rank": len(top_videos) + 1,
                "youtube_id": youtube_id,
                "title": title[:60] + "..." if title and len(title) > 60 else title,
                "full_title": title,
                "video_url": f"https://www.youtube.com/watch?v={youtube_id}",
                "channel_name": channel_name,
                "channel_id": channel_id,
                "channel_url": f"https://www.youtube.com/channel/{channel_id}" if channel_id else None,
                "view_count": int(view_count or 0),
                "like_count": int(like_count or 0),
                "comment_count": int(comment_count or 0),
                "subscriber_count": int(subscriber_count or 0),
                "published_at": published_at,
                "duration": int(duration or 0)
            })

        # ========== Top 10 çƒ­é—¨è¯é¢˜ ==========
        # ä½¿ç”¨ keyword_source ä½œä¸ºè¯é¢˜æ¥æºï¼ˆæ¯” theme æ›´ä¸°å¯Œï¼‰
        cursor.execute("""
            SELECT
                keyword_source,
                COUNT(*) as video_count,
                SUM(view_count) as total_views,
                AVG(view_count) as avg_views,
                COUNT(DISTINCT COALESCE(channel_id, channel_name)) as channel_count
            FROM competitor_videos
            WHERE keyword_source IS NOT NULL AND keyword_source != ''
            GROUP BY keyword_source
            ORDER BY video_count DESC, total_views DESC
            LIMIT 10
        """)

        top_topics = []
        for row in cursor.fetchall():
            keyword_source, video_count, total_views, avg_views, channel_count = row
            top_topics.append({
                "rank": len(top_topics) + 1,
                "topic": keyword_source,
                "video_count": int(video_count or 0),
                "total_views": int(total_views or 0),
                "avg_views": int(avg_views or 0),
                "channel_count": int(channel_count or 0)
            })

        # ========== ç»Ÿè®¡æ¦‚è§ˆ ==========
        cursor.execute("SELECT COUNT(*), COUNT(DISTINCT COALESCE(channel_id, channel_name)), SUM(view_count) FROM competitor_videos")
        stats = cursor.fetchone()

        conn.close()

        return {
            "status": "ok",
            "data": {
                "dark_horse_channels": dark_horse_channels,
                "top_videos": top_videos,
                "top_topics": top_topics,
                "stats": {
                    "total_videos": stats[0] or 0,
                    "total_channels": stats[1] or 0,
                    "total_views": stats[2] or 0
                }
            }
        }

    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


# ============================================================
# ä¸­å¿ƒæ€§åˆ†æ APIï¼ˆå¥—åˆ©åˆ†æï¼‰
# ============================================================

@app.get("/api/centrality")
async def get_centrality_analysis():
    """
    è·å–ç½‘ç»œä¸­å¿ƒæ€§åˆ†ææ•°æ®

    è¿”å›ï¼š
    - ä¸­ä»‹ä¸­å¿ƒæ€§ Top 50ï¼ˆè¯é¢˜ã€é¢‘é“ã€è§†é¢‘ã€æ ‡é¢˜è¯ï¼‰
    - ç¨‹åº¦ä¸­å¿ƒæ€§ Top 50ï¼ˆè¯é¢˜ã€é¢‘é“ã€è§†é¢‘ã€æ ‡é¢˜è¯ï¼‰
    - å¥—åˆ©æœºä¼šï¼ˆé«˜ä¸­ä»‹+ä½æ’­æ”¾/ä½ç²‰ä¸ï¼‰
    """
    try:
        from src.research.network_centrality import get_centrality_data

        db_path = Path(__file__).parent / "data" / "youtube_pipeline.db"

        if not db_exists(str(db_path)):
            return {"status": "error", "message": "æ•°æ®åº“ä¸å­˜åœ¨"}

        data = get_centrality_data(str(db_path))

        # ä¸ºæ¯ä¸ªåˆ—è¡¨æ·»åŠ  rank
        for category in ['betweenness', 'degree']:
            for list_name in ['topics', 'channels', 'videos', 'words']:
                items = data.get(category, {}).get(list_name, [])
                for i, item in enumerate(items):
                    item['rank'] = i + 1

        for list_name in ['high_betweenness_low_views_videos', 'high_betweenness_low_subs_channels']:
            items = data.get('arbitrage', {}).get(list_name, [])
            for i, item in enumerate(items):
                item['rank'] = i + 1

        return {
            "status": "ok",
            "data": data
        }

    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


# ä¸­å¿ƒæ€§æ•°æ®ç¼“å­˜ï¼ˆé¿å…æ¯æ¬¡è¯·æ±‚éƒ½é‡æ–°è®¡ç®—ï¼‰
_centrality_cache = {"data": None, "timestamp": 0, "ttl": 300}  # 5åˆ†é’Ÿç¼“å­˜


@app.get("/api/arbitrage")
async def get_arbitrage_combined():
    """
    è·å–æ•´åˆåçš„å¥—åˆ©åˆ†ææ•°æ®ï¼ˆåŒ…å«æ¦œå• + ä¸­å¿ƒæ€§åˆ†æï¼‰

    è¿”å›ï¼š
    - åŸæœ‰æ¦œå•æ•°æ®ï¼ˆé»‘é©¬é¢‘é“ã€çƒ­é—¨è§†é¢‘ã€çƒ­é—¨è¯é¢˜ï¼‰
    - ä¸­å¿ƒæ€§æ¦œå•ï¼ˆä¸­ä»‹ä¸­å¿ƒæ€§ã€ç¨‹åº¦ä¸­å¿ƒæ€§ Top 50ï¼‰
    - å¥—åˆ©æœºä¼šæ¦œå•

    ä½¿ç”¨ 5 åˆ†é’Ÿç¼“å­˜åŠ é€Ÿå“åº”
    """
    try:
        from src.research.network_centrality import get_centrality_data

        db_path = Path(__file__).parent / "data" / "youtube_pipeline.db"

        if not db_exists(str(db_path)):
            return {"status": "error", "message": "æ•°æ®åº“ä¸å­˜åœ¨"}

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        current_time = time.time()
        if _centrality_cache["data"] and (current_time - _centrality_cache["timestamp"]) < _centrality_cache["ttl"]:
            centrality_data = _centrality_cache["data"]
        else:
            # é‡æ–°è®¡ç®—å¹¶ç¼“å­˜
            centrality_data = get_centrality_data(str(db_path))
            _centrality_cache["data"] = centrality_data
            _centrality_cache["timestamp"] = current_time

        # è·å–åŸæœ‰æ¦œå•æ•°æ®
        conn = db_get_connection(str(db_path))
        cursor = conn.cursor()

        # ========== Top 50 é»‘é©¬é¢‘é“ ==========
        cursor.execute("""
            SELECT
                channel_name,
                channel_id,
                COUNT(*) as video_count,
                AVG(view_count) as avg_views,
                MAX(view_count) as max_views,
                SUM(view_count) as total_views,
                MAX(subscriber_count) as subscriber_count,
                AVG(view_count) * 1.0 / GREATEST(MAX(subscriber_count), 1000) as dark_horse_score
            FROM competitor_videos
            WHERE channel_name IS NOT NULL
              AND channel_name != ''
              AND view_count > 0
            GROUP BY channel_id, channel_name
            HAVING COUNT(*) >= 2 AND MAX(subscriber_count) > 0
            ORDER BY dark_horse_score DESC
            LIMIT 50
        """)

        dark_horse_channels = []
        for row in cursor.fetchall():
            channel_name, channel_id, video_count, avg_views, max_views, total_views, subscriber_count, score = row
            dark_horse_channels.append({
                "rank": len(dark_horse_channels) + 1,
                "channel_name": channel_name,
                "channel_id": channel_id,
                "channel_url": f"https://www.youtube.com/channel/{channel_id}" if channel_id else None,
                "video_count": video_count,
                "avg_views": int(avg_views or 0),
                "max_views": int(max_views or 0),
                "total_views": int(total_views or 0),
                "subscriber_count": int(subscriber_count or 0),
                "dark_horse_score": round(score or 0, 2)
            })

        # ========== Top 50 çƒ­é—¨è§†é¢‘ï¼ˆæŒ‰æ’­æ”¾é‡ï¼‰==========
        cursor.execute("""
            SELECT
                youtube_id,
                title,
                channel_name,
                channel_id,
                view_count,
                like_count,
                comment_count,
                subscriber_count,
                published_at,
                duration
            FROM competitor_videos
            WHERE view_count > 0
            ORDER BY view_count DESC
            LIMIT 50
        """)

        top_videos_by_views = []
        for row in cursor.fetchall():
            youtube_id, title, channel_name, channel_id, view_count, like_count, comment_count, subscriber_count, published_at, duration = row
            top_videos_by_views.append({
                "rank": len(top_videos_by_views) + 1,
                "youtube_id": youtube_id,
                "title": title[:60] + "..." if title and len(title) > 60 else title,
                "full_title": title,
                "video_url": f"https://www.youtube.com/watch?v={youtube_id}",
                "channel_name": channel_name,
                "channel_id": channel_id,
                "channel_url": f"https://www.youtube.com/channel/{channel_id}" if channel_id else None,
                "view_count": int(view_count or 0),
                "like_count": int(like_count or 0),
                "comment_count": int(comment_count or 0),
                "subscriber_count": int(subscriber_count or 0),
            })

        # ========== Top 50 é¢‘é“ï¼ˆæŒ‰ç²‰ä¸æ•°ï¼‰==========
        cursor.execute("""
            SELECT
                channel_name,
                channel_id,
                COUNT(*) as video_count,
                AVG(view_count) as avg_views,
                SUM(view_count) as total_views,
                MAX(subscriber_count) as subscriber_count
            FROM competitor_videos
            WHERE channel_name IS NOT NULL
              AND channel_name != ''
              AND subscriber_count > 0
            GROUP BY channel_id, channel_name
            ORDER BY subscriber_count DESC
            LIMIT 50
        """)

        top_channels_by_subs = []
        for row in cursor.fetchall():
            channel_name, channel_id, video_count, avg_views, total_views, subscriber_count = row
            top_channels_by_subs.append({
                "rank": len(top_channels_by_subs) + 1,
                "channel_name": channel_name,
                "channel_id": channel_id,
                "channel_url": f"https://www.youtube.com/channel/{channel_id}" if channel_id else None,
                "video_count": video_count,
                "avg_views": int(avg_views or 0),
                "total_views": int(total_views or 0),
                "subscriber_count": int(subscriber_count or 0),
            })

        # ========== Top 10 çƒ­é—¨è¯é¢˜ ==========
        cursor.execute("""
            SELECT
                keyword_source,
                COUNT(*) as video_count,
                SUM(view_count) as total_views,
                AVG(view_count) as avg_views,
                COUNT(DISTINCT COALESCE(channel_id, channel_name)) as channel_count
            FROM competitor_videos
            WHERE keyword_source IS NOT NULL AND keyword_source != ''
            GROUP BY keyword_source
            ORDER BY video_count DESC, total_views DESC
            LIMIT 50
        """)

        top_topics = []
        for row in cursor.fetchall():
            keyword_source, video_count, total_views, avg_views, channel_count = row
            top_topics.append({
                "rank": len(top_topics) + 1,
                "topic": keyword_source,
                "video_count": int(video_count or 0),
                "total_views": int(total_views or 0),
                "avg_views": int(avg_views or 0),
                "channel_count": int(channel_count or 0)
            })

        conn.close()

        # ä¸ºä¸­å¿ƒæ€§æ•°æ®æ·»åŠ  rank
        for category in ['betweenness', 'degree']:
            for list_name in ['topics', 'channels', 'videos', 'words']:
                items = centrality_data.get(category, {}).get(list_name, [])
                for i, item in enumerate(items):
                    item['rank'] = i + 1

        for list_name in ['high_betweenness_low_views_videos', 'high_betweenness_low_subs_channels']:
            items = centrality_data.get('arbitrage', {}).get(list_name, [])
            for i, item in enumerate(items):
                item['rank'] = i + 1

        return {
            "status": "ok",
            "data": {
                # åŸæœ‰æ¦œå•
                "traditional_leaderboard": {
                    "dark_horse_channels": dark_horse_channels,
                    "top_videos_by_views": top_videos_by_views,
                    "top_channels_by_subs": top_channels_by_subs,
                    "top_topics": top_topics
                },
                # ä¸­å¿ƒæ€§æ¦œå•
                "centrality_leaderboard": {
                    "betweenness": centrality_data.get('betweenness', {}),
                    "degree": centrality_data.get('degree', {})
                },
                # å¥—åˆ©æœºä¼š
                "arbitrage_opportunities": centrality_data.get('arbitrage', {}),
                # ç»Ÿè®¡ä¿¡æ¯
                "stats": centrality_data.get('stats', {})
            }
        }

    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


# ============================================================
# é¢‘é“è¯¦æƒ… APIï¼ˆå‘é«˜æ‰‹å­¦ä¹ æ¨¡å—ï¼‰
# ============================================================

@app.get("/api/channel-detail/{channel_id}")
async def get_channel_detail(channel_id: str):
    """
    è·å–é¢‘é“è¯¦æƒ…æ•°æ®ï¼ˆç”¨äº"å‘é«˜æ‰‹å­¦ä¹ "æ¨¡å—ï¼‰

    è¿”å›ï¼š
    - é¢‘é“åŸºæœ¬ä¿¡æ¯ï¼ˆè®¢é˜…æ•°ã€æ€»æ’­æ”¾ç­‰ï¼‰
    - è§†é¢‘åˆ—è¡¨ï¼ˆæŒ‰æ’­æ”¾é‡æ’åºï¼‰
    - è¯é¢˜åˆ†å¸ƒï¼ˆæŒ‰ keyword_source åˆ†ç»„ç»Ÿè®¡ï¼‰
    - æ—¶é•¿åˆ†å¸ƒ
    - æˆé•¿è½¨è¿¹ï¼ˆåŸºäºé¢‘é“åˆ›å»ºæ—¶é—´ï¼‰
    """
    try:
        db_path = Path(__file__).parent / "data" / "youtube_pipeline.db"

        if not db_exists(str(db_path)):
            return {"status": "error", "message": "æ•°æ®åº“ä¸å­˜åœ¨"}

        conn = db_get_connection(str(db_path), row_factory=sqlite3.Row)
        cursor = conn.cursor()

        # 1. è·å–é¢‘é“çš„æ‰€æœ‰è§†é¢‘
        cursor.execute("""
            SELECT
                youtube_id,
                title,
                channel_name,
                view_count,
                like_count,
                comment_count,
                duration,
                published_at,
                keyword_source,
                thumbnail_url
            FROM competitor_videos
            WHERE channel_id = ? OR channel_name = (
                SELECT channel_name FROM competitor_videos WHERE channel_id = ? LIMIT 1
            )
            ORDER BY view_count DESC
        """, (channel_id, channel_id))

        videos = cursor.fetchall()

        if not videos:
            conn.close()
            return {"status": "error", "message": f"æœªæ‰¾åˆ°é¢‘é“ {channel_id} çš„æ•°æ®"}

        # 2. è·å–é¢‘é“ä¿¡æ¯ï¼ˆä» channels è¡¨ï¼‰
        cursor.execute("""
            SELECT channel_name, subscriber_count, video_count, total_views,
                   country, description, created_at, canonical_url
            FROM channels
            WHERE channel_id = ?
        """, (channel_id,))
        channel_row = cursor.fetchone()

        # åŸºæœ¬ç»Ÿè®¡
        channel_name = videos[0]['channel_name'] if videos else 'æœªçŸ¥é¢‘é“'
        total_videos = len(videos)
        total_views = sum(v['view_count'] or 0 for v in videos)
        total_likes = sum(v['like_count'] or 0 for v in videos)
        avg_views = total_views // total_videos if total_videos > 0 else 0

        # ä» channels è¡¨è·å–è®¢é˜…æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»è§†é¢‘è¡¨ä¼°ç®—
        if channel_row:
            subscriber_count = channel_row['subscriber_count'] or 0
            created_at = channel_row['created_at']
            country = channel_row['country']
            description = channel_row['description']
        else:
            # ä»è§†é¢‘è¡¨è·å–æœ€å¤§è®¢é˜…æ•°
            cursor.execute("""
                SELECT MAX(subscriber_count) FROM competitor_videos WHERE channel_id = ?
            """, (channel_id,))
            sub_row = cursor.fetchone()
            subscriber_count = sub_row[0] if sub_row and sub_row[0] else 0
            created_at = None
            country = None
            description = None

        # 3. è§†é¢‘åˆ—è¡¨ï¼ˆTop 20ï¼‰
        video_list = []
        for v in videos[:20]:
            video_list.append({
                "youtube_id": v['youtube_id'],
                "title": v['title'],
                "view_count": v['view_count'] or 0,
                "like_count": v['like_count'] or 0,
                "duration": v['duration'] or 0,
                "published_at": v['published_at'],
                "keyword_source": v['keyword_source'],
                "thumbnail_url": v['thumbnail_url'],
                "video_url": f"https://www.youtube.com/watch?v={v['youtube_id']}"
            })

        # 4. è¯é¢˜åˆ†å¸ƒï¼ˆæŒ‰ keyword_source åˆ†ç»„ï¼‰
        topic_stats = {}
        for v in videos:
            topic = v['keyword_source'] or 'å…¶ä»–'
            if topic not in topic_stats:
                topic_stats[topic] = {
                    "count": 0,
                    "total_views": 0,
                    "videos": []
                }
            topic_stats[topic]["count"] += 1
            topic_stats[topic]["total_views"] += v['view_count'] or 0
            topic_stats[topic]["videos"].append(v['view_count'] or 0)

        # è®¡ç®—æ¯ä¸ªè¯é¢˜çš„ç»Ÿè®¡
        topic_distribution = []
        for topic, stats in topic_stats.items():
            count = stats["count"]
            total_topic_views = stats["total_views"]
            topic_distribution.append({
                "topic": topic,
                "count": count,
                "percentage": round(count / total_videos * 100, 1) if total_videos > 0 else 0,
                "total_views": total_topic_views,
                "avg_views": total_topic_views // count if count > 0 else 0,
                "contribution": round(total_topic_views / total_views * 100, 1) if total_views > 0 else 0
            })

        # æŒ‰è§†é¢‘æ•°æ’åº
        topic_distribution.sort(key=lambda x: x["count"], reverse=True)

        # æ ‡è®°ä¸»åŠ›è¯é¢˜å’Œé«˜æ•ˆè¯é¢˜
        if topic_distribution:
            topic_distribution[0]["badge"] = "ä¸»åŠ›"
            # æ‰¾å‡ºæ•ˆç‡æœ€é«˜çš„ï¼ˆå¹³å‡æ’­æ”¾æœ€é«˜ï¼‰
            best_efficiency = max(topic_distribution, key=lambda x: x["avg_views"])
            if best_efficiency != topic_distribution[0]:
                for t in topic_distribution:
                    if t["topic"] == best_efficiency["topic"]:
                        t["badge"] = "é«˜æ•ˆ"
                        break

        # 5. æ—¶é•¿åˆ†å¸ƒ
        duration_stats = {
            "short": {"count": 0, "total_views": 0, "label": "çŸ­è§†é¢‘(<5åˆ†é’Ÿ)"},
            "medium": {"count": 0, "total_views": 0, "label": "ä¸­ç­‰(5-15åˆ†é’Ÿ)"},
            "long": {"count": 0, "total_views": 0, "label": "é•¿è§†é¢‘(>15åˆ†é’Ÿ)"}
        }
        for v in videos:
            d = v['duration'] or 0
            views = v['view_count'] or 0
            if d < 300:
                duration_stats["short"]["count"] += 1
                duration_stats["short"]["total_views"] += views
            elif d < 900:
                duration_stats["medium"]["count"] += 1
                duration_stats["medium"]["total_views"] += views
            else:
                duration_stats["long"]["count"] += 1
                duration_stats["long"]["total_views"] += views

        duration_distribution = []
        for key, stats in duration_stats.items():
            count = stats["count"]
            if count > 0:
                duration_distribution.append({
                    "type": key,
                    "label": stats["label"],
                    "count": count,
                    "percentage": round(count / total_videos * 100, 1) if total_videos > 0 else 0,
                    "avg_views": stats["total_views"] // count
                })

        # 6. æˆé•¿è½¨è¿¹ï¼ˆåŸºäºé¢‘é“åˆ›å»ºæ—¶é—´ï¼‰
        growth_trajectory = []
        if created_at:
            try:
                from datetime import datetime
                created = datetime.strptime(created_at, "%Y-%m-%d")
                days = (datetime.now() - created).days
                years = days // 365

                growth_trajectory.append({
                    "phase": "åˆ›å»º",
                    "date": created_at,
                    "milestone": "é¢‘é“åˆ›å»º",
                    "data": f"å¼€å§‹äº {created_at}"
                })

                if subscriber_count > 0:
                    subs_per_day = subscriber_count / days if days > 0 else 0
                    growth_trajectory.append({
                        "phase": "å½“å‰",
                        "date": datetime.now().strftime("%Y-%m-%d"),
                        "milestone": f"ç´¯è®¡ {subscriber_count:,} è®¢é˜…",
                        "data": f"æ—¥å‡æ¶¨ç²‰ {subs_per_day:.1f}"
                    })
            except:
                pass

        # 7. è®¡ç®—æ’­æ”¾æ•ˆç‡
        efficiency = (avg_views / subscriber_count * 100) if subscriber_count > 0 else 0

        conn.close()

        return {
            "status": "ok",
            "data": {
                "channel_info": {
                    "channel_id": channel_id,
                    "channel_name": channel_name,
                    "subscriber_count": subscriber_count,
                    "total_views": total_views,
                    "total_videos": total_videos,
                    "avg_views": avg_views,
                    "efficiency": round(efficiency, 1),
                    "country": country,
                    "description": description,
                    "created_at": created_at,
                    "channel_url": f"https://www.youtube.com/channel/{channel_id}"
                },
                "videos": video_list,
                "topic_distribution": topic_distribution,
                "duration_distribution": duration_distribution,
                "growth_trajectory": growth_trajectory,
                # ç”Ÿæˆå­¦ä¹ å»ºè®®
                "learning_insights": _generate_channel_learning_insights(
                    channel_name, subscriber_count, avg_views, efficiency,
                    topic_distribution, duration_distribution, video_list
                )
            }
        }

    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


def _generate_channel_learning_insights(
    channel_name: str,
    subscriber_count: int,
    avg_views: int,
    efficiency: float,
    topic_distribution: list,
    duration_distribution: list,
    videos: list
) -> dict:
    """
    æ ¹æ®é¢‘é“æ•°æ®ç”Ÿæˆå­¦ä¹ æ´å¯Ÿ
    """
    # ä¸ºä»€ä¹ˆå€¼å¾—ç ”ç©¶
    why_study = []
    if subscriber_count > 100000:
        why_study.append(f"æ‹¥æœ‰ {subscriber_count:,} è®¢é˜…ï¼Œæ˜¯è¯¥é¢†åŸŸçš„å¤´éƒ¨ç©å®¶")
    elif subscriber_count > 10000:
        why_study.append(f"æ‹¥æœ‰ {subscriber_count:,} è®¢é˜…ï¼Œæ­£åœ¨å¿«é€Ÿå¢é•¿")

    if efficiency > 20:
        why_study.append(f"æ’­æ”¾æ•ˆç‡è¾¾ {efficiency:.1f}%ï¼ˆé«˜äºè¡Œä¸šå¹³å‡ï¼‰ï¼Œå†…å®¹å¸å¼•åŠ›å¼º")
    elif efficiency > 10:
        why_study.append(f"æ’­æ”¾æ•ˆç‡ {efficiency:.1f}%ï¼Œç”¨æˆ·ç²˜æ€§è‰¯å¥½")

    if avg_views > 100000:
        why_study.append(f"å‡æ’­ {avg_views:,}ï¼Œå†…å®¹æœ‰è¾ƒå¼ºçš„ä¼ æ’­åŠ›")

    if not why_study:
        why_study.append("ç ”ç©¶å…¶å†…å®¹ç­–ç•¥å’Œå¢é•¿è·¯å¾„ï¼Œå¯ä»¥ä¸ºæ–°äººæä¾›å‚è€ƒç»éªŒ")

    # æˆåŠŸæ¨¡å¼
    success_patterns = []

    # åˆ†æè¯é¢˜ç­–ç•¥
    if topic_distribution:
        main_topic = topic_distribution[0]
        if main_topic["percentage"] > 50:
            success_patterns.append({
                "title": "å‚ç›´æ·±è€•",
                "desc": f"ä¸»åŠ›è¯é¢˜ã€Œ{main_topic['topic']}ã€å æ¯” {main_topic['percentage']}%ï¼Œä¸“æ³¨å•ä¸€é¢†åŸŸ",
                "evidence": f"{main_topic['count']} ä¸ªè§†é¢‘èšç„¦è¯¥æ–¹å‘"
            })
        else:
            success_patterns.append({
                "title": "å¤šå…ƒå¸ƒå±€",
                "desc": "å†…å®¹è¦†ç›–å¤šä¸ªè¯é¢˜ï¼Œé™ä½å•ä¸€è¯é¢˜é£é™©",
                "evidence": f"æ¶‰åŠ {len(topic_distribution)} ä¸ªä¸åŒè¯é¢˜"
            })

    # åˆ†ææ—¶é•¿ç­–ç•¥
    if duration_distribution:
        main_duration = max(duration_distribution, key=lambda x: x["count"])
        success_patterns.append({
            "title": f"{main_duration['label']}ç­–ç•¥",
            "desc": f"ä¸»è¦å‘å¸ƒ{main_duration['label']}ï¼Œå æ¯” {main_duration['percentage']}%",
            "evidence": f"è¿™ç±»è§†é¢‘å‡æ’­ {main_duration['avg_views']:,}"
        })

    # åˆ†æçˆ†æ¬¾è§„å¾‹
    if videos and len(videos) >= 3:
        top_video = videos[0]
        avg_top3 = sum(v['view_count'] for v in videos[:3]) / 3
        success_patterns.append({
            "title": "çˆ†æ¬¾èƒ½åŠ›",
            "desc": f"æœ€é«˜æ’­æ”¾ {top_video['view_count']:,}ï¼Œå‰3è§†é¢‘å‡æ’­ {int(avg_top3):,}",
            "evidence": f"çˆ†æ¬¾æ ‡é¢˜å‚è€ƒï¼šã€Œ{top_video['title'][:30]}...ã€"
        })

    # å­¦ä¹ è·¯å¾„
    learning_path = [
        f"ç ”ç©¶ã€Œ{topic_distribution[0]['topic'] if topic_distribution else 'ä¸»åŠ›è¯é¢˜'}ã€æ–¹å‘çš„é€‰é¢˜è§„å¾‹",
        "å­¦ä¹ è¯¥é¢‘é“çš„æ ‡é¢˜å†™æ³•å’Œå°é¢é£æ ¼",
        f"ä¿æŒç¨³å®šæ›´æ–°ï¼Œå‚è€ƒå…¶{main_duration['label'] if duration_distribution else 'ä¸­ç­‰æ—¶é•¿'}ç­–ç•¥" if duration_distribution else "ä¿æŒç¨³å®šæ›´æ–°é¢‘ç‡",
        "åˆ†æé«˜æ’­æ”¾è§†é¢‘çš„å†…å®¹ç»“æ„",
        "æ‰¾åˆ°å·®å¼‚åŒ–è§’åº¦ï¼Œé¿å…å®Œå…¨æ¨¡ä»¿"
    ]

    # è¡ŒåŠ¨å»ºè®®
    actions = {
        "do": [
            f"ä»ã€Œ{topic_distribution[0]['topic'] if topic_distribution else 'éªŒè¯è¿‡çš„è¯é¢˜'}ã€åˆ‡å…¥",
            "ä¿æŒç¨³å®šæ›´æ–°é¢‘ç‡",
            "å­¦ä¹ æ ‡é¢˜å’Œå°é¢é£æ ¼"
        ],
        "avoid": [
            "ä¸è¦ä¸€å¼€å§‹å°±åˆ†æ•£ç²¾åŠ›",
            "ä¸è¦å®Œå…¨å¤åˆ¶ï¼Œè¦æœ‰å·®å¼‚åŒ–"
        ]
    }

    return {
        "why_study": why_study,
        "success_patterns": success_patterns,
        "learning_path": learning_path,
        "actions": actions
    }


# ============================================================
# ç½‘ç»œå›¾å¯è§†åŒ– API
# ============================================================

@app.get("/api/network-graph")
async def get_network_graph(graph_type: str = "topic", max_nodes: int = 50):
    """
    è·å–ç½‘ç»œå›¾å¯è§†åŒ–æ•°æ®ï¼ˆèŠ‚ç‚¹ + è¾¹ï¼‰

    å‚æ•°:
    - graph_type: 'topic' (è¯é¢˜å…±ç°ç½‘ç»œ) æˆ– 'channel' (é¢‘é“-é¢‘é“ç½‘ç»œ)
    - max_nodes: æœ€å¤§èŠ‚ç‚¹æ•°é‡ (10-100)

    è¿”å›:
    - nodes: èŠ‚ç‚¹åˆ—è¡¨ï¼ŒåŒ…å« id, label, betweenness, degree, size, color_intensity ç­‰
    - edges: è¾¹åˆ—è¡¨ï¼ŒåŒ…å« from, to, weight, width
    - stats: ç»Ÿè®¡ä¿¡æ¯

    ç”¨äºå‰ç«¯ vis.js ç½‘ç»œå›¾å¯è§†åŒ–ï¼Œæ”¯æŒä¸‰ç§ç€è‰²æ¨¡å¼ï¼š
    1. ä¸­ä»‹ä¸­å¿ƒæ€§ç€è‰² - æ¡¥æ¢èŠ‚ç‚¹é«˜äº®
    2. ç¨‹åº¦ä¸­å¿ƒæ€§ç€è‰² - æ¢çº½èŠ‚ç‚¹é«˜äº®
    3. å¥—åˆ©æœºä¼šç€è‰² - é«˜ä¸­ä»‹+ä½çƒ­åº¦èŠ‚ç‚¹é«˜äº®
    """
    try:
        from src.research.network_centrality import get_network_graph_data

        db_path = Path(__file__).parent / "data" / "youtube_pipeline.db"

        if not db_exists(str(db_path)):
            return {"status": "error", "message": "æ•°æ®åº“ä¸å­˜åœ¨"}

        # é™åˆ¶èŠ‚ç‚¹æ•°é‡èŒƒå›´
        max_nodes = max(10, min(100, max_nodes))

        data = get_network_graph_data(str(db_path), graph_type=graph_type, max_nodes=max_nodes)

        return data

    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


# ============================================================
# å¯åŠ¨
# ============================================================

if __name__ == "__main__":
    print("=" * 50)
    print("YouTube Content Assistant API Server")
    print("=" * 50)
    print("å¯åŠ¨æœåŠ¡å™¨: http://localhost:8000")
    print("WebSocket: ws://localhost:8000/ws/{task_id}")
    print("API æ–‡æ¡£: http://localhost:8000/docs")
    print("=" * 50)

    import os
    # å¼€å‘æ—¶è®¾ç½® DEV=1 å¯ç”¨çƒ­é‡è½½ï¼Œç”Ÿäº§ç¯å¢ƒé»˜è®¤å…³é—­
    dev_mode = os.environ.get("DEV", "0") == "1"

    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=dev_mode,  # é»˜è®¤å…³é—­çƒ­é‡è½½ï¼Œé¿å…ä¸­æ–­ WebSocket è¿æ¥
        log_level="info"
    )

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€è¿½è¸ªè°ƒåº¦å™¨
å®šæœŸæ‰§è¡ŒåŠ¨æ€è¿½è¸ªä»»åŠ¡ï¼Œä¸é•¿æœŸæ¨¡å¼åˆ†æç»“åˆ
"""

import sys
import schedule
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.logger import setup_logger
from utils.config import get_config
from utils.file_utils import ensure_dir, write_json
from dynamic_tracker import DynamicTracker
from analysis.pattern_analyzer import PatternAnalyzer

logger = setup_logger('scheduler')

class TrackingScheduler:
    """åŠ¨æ€è¿½è¸ªè°ƒåº¦å™¨"""

    def __init__(self, config):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨

        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.tracker = DynamicTracker(config)
        self.analyzer = PatternAnalyzer(config)

    def setup_schedule(self):
        """è®¾ç½®è°ƒåº¦ä»»åŠ¡"""
        logger.info("è®¾ç½®åŠ¨æ€è¿½è¸ªè°ƒåº¦ä»»åŠ¡")

        # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡çƒ­ç‚¹
        schedule.every().hour.do(self._hourly_trend_check)

        # æ¯å¤©æ—©ä¸Š8ç‚¹ç”Ÿæˆæ¯æ—¥æ‘˜è¦
        schedule.every().day.at("08:00").do(self._daily_digest)

        # æ¯å‘¨ä¸€æ—©ä¸Š9ç‚¹ç”Ÿæˆå‘¨æŠ¥
        schedule.every().monday.at("09:00").do(self._weekly_report)

        # æ¯3å¤©æ›´æ–°ä¸€æ¬¡æ¨¡å¼åº“
        schedule.every(3).days.do(self._update_pattern_library)

        logger.info("è°ƒåº¦ä»»åŠ¡è®¾ç½®å®Œæˆ")

    def _hourly_trend_check(self):
        """æ¯å°æ—¶è¶‹åŠ¿æ£€æŸ¥"""
        logger.info("æ‰§è¡Œæ¯å°æ—¶è¶‹åŠ¿æ£€æŸ¥")

        try:
            keywords = self.config.get('monitoring.keywords', [])
            if not keywords:
                logger.warning("æœªé…ç½®ç›‘æ§å…³é”®è¯")
                return

            # å¿«é€Ÿè¶‹åŠ¿æ£€æŸ¥
            trends = self.tracker.track_daily_trends(keywords)

            # ä¿å­˜è¶‹åŠ¿æ•°æ®
            output_dir = Path('output/trends/hourly')
            ensure_dir(output_dir)

            timestamp = datetime.now().strftime('%Y%m%d_%H')
            trend_file = output_dir / f"trends_{timestamp}.json"
            write_json(trend_file, trends)

            # æ£€æŸ¥æ˜¯å¦å‘ç°å¼‚å¸¸è¶‹åŠ¿
            self._check_urgent_trends(trends)

            logger.info(f"æ¯å°æ—¶è¶‹åŠ¿æ£€æŸ¥å®Œæˆ: {trend_file}")

        except Exception as e:
            logger.error(f"æ¯å°æ—¶è¶‹åŠ¿æ£€æŸ¥å¤±è´¥: {e}")

    def _daily_digest(self):
        """ç”Ÿæˆæ¯æ—¥æ‘˜è¦"""
        logger.info("ç”Ÿæˆæ¯æ—¥æ‘˜è¦")

        try:
            digest = self.tracker.generate_daily_digest()

            # ä¿å­˜æ‘˜è¦
            output_dir = Path('output/daily_digest')
            ensure_dir(output_dir)

            digest_file = output_dir / f"digest_{datetime.now().strftime('%Y%m%d')}.md"
            with open(digest_file, 'w', encoding='utf-8') as f:
                f.write(digest)

            logger.info(f"æ¯æ—¥æ‘˜è¦å·²ç”Ÿæˆ: {digest_file}")

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¯æ—¥æ‘˜è¦å¤±è´¥: {e}")

    def _weekly_report(self):
        """ç”Ÿæˆå‘¨æŠ¥"""
        logger.info("ç”Ÿæˆå‘¨æŠ¥")

        try:
            # è·å–è¿‡å»ä¸€å‘¨çš„æ•°æ®
            week_data = self._collect_weekly_data()

            # ç”Ÿæˆå‘¨æŠ¥
            report = self._generate_weekly_report(week_data)

            # ä¿å­˜å‘¨æŠ¥
            output_dir = Path('output/weekly_reports')
            ensure_dir(output_dir)

            week_start = datetime.now() - timedelta(days=7)
            report_file = output_dir / f"weekly_report_{week_start.strftime('%Y%m%d')}.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)

            logger.info(f"å‘¨æŠ¥å·²ç”Ÿæˆ: {report_file}")

        except Exception as e:
            logger.error(f"ç”Ÿæˆå‘¨æŠ¥å¤±è´¥: {e}")

    def _update_pattern_library(self):
        """æ›´æ–°æ¨¡å¼åº“"""
        logger.info("æ›´æ–°æ¨¡å¼åº“")

        try:
            # è·å–æœ€è¿‘çš„æ•°æ®
            recent_trends = self._load_recent_trends(days=7)
            recent_videos = self._load_recent_videos(days=7)

            # é‡æ–°åˆ†ææ¨¡å¼
            if recent_videos:
                new_patterns = self.analyzer.analyze_videos(recent_videos)

                # ä¸ç°æœ‰æ¨¡å¼å¯¹æ¯”
                existing_patterns = self._load_existing_patterns()
                updated_patterns = self._merge_patterns(existing_patterns, new_patterns)

                # ä¿å­˜æ›´æ–°çš„æ¨¡å¼
                output_dir = Path('output/patterns')
                ensure_dir(output_dir)

                pattern_file = output_dir / f"patterns_{datetime.now().strftime('%Y%m%d')}.json"
                write_json(pattern_file, updated_patterns)

                logger.info(f"æ¨¡å¼åº“å·²æ›´æ–°: {pattern_file}")
                logger.info(f"æ–°å¢æ¨¡å¼: {len(new_patterns)} ä¸ª")

        except Exception as e:
            logger.error(f"æ›´æ–°æ¨¡å¼åº“å¤±è´¥: {e}")

    def _check_urgent_trends(self, trends: Dict[str, Any]):
        """æ£€æŸ¥ç´§æ€¥è¶‹åŠ¿"""
        urgent_flags = []

        # æ£€æŸ¥æ–°å…´è¯é¢˜
        for topic in trends.get('emerging_topics', []):
            if topic['growth_rate'] > 5.0:  # å¢é•¿ç‡è¶…è¿‡500%
                urgent_flags.append(f"ğŸ”¥ ç´§æ€¥: {topic['keyword']} å¢é•¿ {topic['growth_rate']:.1%}")

        # æ£€æŸ¥ç—…æ¯’è§†é¢‘
        for video in trends.get('viral_videos', []):
            if video.get('velocity', 0) > 5000:  # æ¯å°æ—¶è¶…è¿‡5000è§‚çœ‹
                urgent_flags.append(f"ğŸš€ ç—…æ¯’è§†é¢‘: {video['title'][:30]}...")

        if urgent_flags:
            logger.warning("å‘ç°ç´§æ€¥è¶‹åŠ¿:")
            for flag in urgent_flags:
                logger.warning(f"  {flag}")

            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é€šçŸ¥é€»è¾‘ï¼ˆå¦‚å‘é€é‚®ä»¶ã€Slackæ¶ˆæ¯ç­‰ï¼‰
            self._send_urgent_alert(urgent_flags)

    def _collect_weekly_data(self) -> Dict[str, Any]:
        """æ”¶é›†å‘¨æ•°æ®"""
        week_data = {
            'start_date': (datetime.now() - timedelta(days=7)).isoformat(),
            'end_date': datetime.now().isoformat(),
            'daily_trends': [],
            'competitor_activity': [],
            'platform_changes': [],
            'summary': {}
        }

        # åŠ è½½æ¯æ—¥è¶‹åŠ¿æ•°æ®
        trend_dir = Path('output/trends/hourly')
        if trend_dir.exists():
            # è·å–è¿‡å»7å¤©çš„æ•°æ®æ–‡ä»¶
            import glob
            pattern = str(trend_dir / "trends_*.json")
            files = glob.glob(pattern)

            for file_path in sorted(files, reverse=True)[:7*24]:  # æœ€å¤š7å¤©*24å°æ—¶
                try:
                    import json
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        week_data['daily_trends'].append(data)
                except Exception as e:
                    logger.error(f"åŠ è½½è¶‹åŠ¿æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        return week_data

    def _generate_weekly_report(self, week_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆå‘¨æŠ¥"""
        lines = [
            "# YouTubeåŠ¨æ€è¿½è¸ªå‘¨æŠ¥",
            "",
            f"**æŠ¥å‘Šå‘¨æœŸ**: {week_data['start_date'][:10]} è‡³ {week_data['end_date'][:10]}",
            "",
            "## ğŸ“Š æœ¬å‘¨æ¦‚å†µ",
            ""
        ]

        # ç»Ÿè®¡æ¦‚è§ˆ
        total_trends = len(week_data.get('daily_trends', []))
        if total_trends > 0:
            # è®¡ç®—å¹³å‡å¢é•¿ç‡
            growth_rates = []
            for trend_data in week_data['daily_trends']:
                for keyword, stats in trend_data.get('daily_stats', {}).items():
                    growth_rates.append(stats.get('growth_rate', 0))

            avg_growth = sum(growth_rates) / len(growth_rates) if growth_rates else 0

            lines.extend([
                f"- **è¿½è¸ªå¤©æ•°**: {total_trends // 24} å¤©",
                f"- **å¹³å‡å¢é•¿ç‡**: {avg_growth:.1%}",
                f"- **æ–°å…´è¯é¢˜**: {len([t for t in week_data['daily_trends'] for topic in t.get('emerging_topics', [])])} ä¸ª",
                f"- **ç—…æ¯’è§†é¢‘**: {len([t for t in week_data['daily_trends'] for video in t.get('viral_videos', [])])} ä¸ª",
                ""
            ])

        # çƒ­ç‚¹è¯é¢˜æ’è¡Œ
        lines.extend([
            "## ğŸ”¥ æœ¬å‘¨çƒ­ç‚¹è¯é¢˜",
            ""
        ])

        # ç»Ÿè®¡å„è¯é¢˜å‡ºç°æ¬¡æ•°
        topic_counts = {}
        for trend_data in week_data['daily_trends']:
            for topic in trend_data.get('emerging_topics', []):
                keyword = topic['keyword']
                topic_counts[keyword] = topic_counts.get(keyword, 0) + 1

        # æ’åºå¹¶æ˜¾ç¤º
        sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        for i, (topic, count) in enumerate(sorted_topics, 1):
            lines.append(f"{i}. **{topic}**: å‡ºç° {count} æ¬¡")

        lines.append("")

        # æ¨¡å¼å˜åŒ–åˆ†æ
        lines.extend([
            "## ğŸ”„ æ¨¡å¼å˜åŒ–åˆ†æ",
            "",
            "### é•¿æœŸç¨³å®šæ¨¡å¼",
            "- çŸ¥è¯†ä»˜è´¹çš„æ ¸å¿ƒéœ€æ±‚æœªå˜",
            "- ç”¨æˆ·å¯¹"å®ç”¨æŠ€èƒ½"å†…å®¹éœ€æ±‚ç¨³å®š",
            "- é«˜è´¨é‡æ•™ç¨‹ä»æœ‰æŒç»­å¸å¼•åŠ›",
            "",
            "### æ–°å…´æ¨¡å¼",
            "- AIç›¸å…³è¯é¢˜çƒ­åº¦æŒç»­ä¸Šå‡",
            "- çŸ­è§†é¢‘+é•¿è§†é¢‘ç»„åˆæ¨¡å¼å…´èµ·",
            "- äº’åŠ¨å¼æ•™å­¦å†…å®¹æ›´å—æ¬¢è¿",
            "",
        ])

        # ä¸‹å‘¨é¢„æµ‹
        lines.extend([
            "## ğŸ”® ä¸‹å‘¨é¢„æµ‹",
            "",
            "### æ½œåœ¨çƒ­ç‚¹",
            "1. **æŠ€æœ¯ç±»æ•™ç¨‹**: é¢„è®¡æŒç»­çƒ­åº¦",
            "2. **AIåº”ç”¨åˆ†äº«**: å¯èƒ½æˆä¸ºæ–°å¢é•¿ç‚¹",
            "3. **æ•ˆç‡å·¥å…·**: å¯èƒ½æœ‰å°å¹…å¢é•¿",
            "",
            "### é£é™©æç¤º",
            "1. é¿å…è¿‡åº¦ä¾èµ–å•ä¸€è¯é¢˜",
            "2. å…³æ³¨å¹³å°æ”¿ç­–å˜åŒ–",
            "3. ç«å“å¯èƒ½åŠ å¤§æŠ•å…¥",
            "",
        ])

        # è¡ŒåŠ¨å»ºè®®
        lines.extend([
            "## ğŸ’¡ è¡ŒåŠ¨å»ºè®®",
            "",
            "### å†…å®¹åˆ›ä½œ",
            "- ç»§ç»­æ·±è€•æŠ€æœ¯æ•™ç¨‹é¢†åŸŸ",
            "- å°è¯•AI+ä¼ ç»ŸæŠ€èƒ½çš„ç»„åˆ",
            "- å¢åŠ äº’åŠ¨å¼å†…å®¹æ¯”ä¾‹",
            "",
            "### ç­–ç•¥è°ƒæ•´",
            "- ä¿æŒæ¯æ—¥å‘å¸ƒé¢‘ç‡",
            "- å…³æ³¨ç«å“åˆ›æ–°ç‚¹",
            "- å‡†å¤‡2-3ä¸ªå¤‡ç”¨è¯é¢˜",
            "",
            "### æ¨¡å¼ä¼˜åŒ–",
            "- åŸºäºæ–°æ•°æ®æ›´æ–°å†…å®¹æ¨¡æ¿",
            "- è°ƒæ•´å‘å¸ƒæ—¶é—´ç­–ç•¥",
            "- ä¼˜åŒ–æ ‡ç­¾å’Œå…³é”®è¯ä½¿ç”¨",
            ""
        ])

        return '\n'.join(lines)

    def _load_recent_trends(self, days: int = 7) -> List[Dict[str, Any]]:
        """åŠ è½½æœ€è¿‘è¶‹åŠ¿æ•°æ®"""
        trends_dir = Path('output/trends/hourly')
        if not trends_dir.exists():
            return []

        import glob
        import json

        pattern = str(trends_dir / "trends_*.json")
        files = glob.glob(pattern)

        # è·å–æœ€è¿‘Nå¤©çš„æ–‡ä»¶
        cutoff = datetime.now() - timedelta(days=days)
        recent_files = []

        for file_path in files:
            try:
                # ä»æ–‡ä»¶åæå–æ—¥æœŸ
                filename = Path(file_path).stem  # å¦‚ trends_20241209_14
                date_str = filename.split('_')[1] + '_' + filename.split('_')[2]
                file_date = datetime.strptime(date_str, '%Y%m%d_%H')

                if file_date >= cutoff:
                    recent_files.append(file_path)
            except Exception:
                continue

        # åŠ è½½æ•°æ®
        recent_trends = []
        for file_path in sorted(recent_files, reverse=True):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    recent_trends.append(data)
            except Exception as e:
                logger.error(f"åŠ è½½è¶‹åŠ¿æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        return recent_trends

    def _load_recent_videos(self, days: int = 7) -> List[Dict[str, Any]]:
        """åŠ è½½æœ€è¿‘è§†é¢‘æ•°æ®"""
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æˆ–æ–‡ä»¶ä¸­åŠ è½½
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨
        return []

    def _load_existing_patterns(self) -> List[Dict[str, Any]]:
        """åŠ è½½ç°æœ‰æ¨¡å¼"""
        pattern_dir = Path('output/patterns')
        if not pattern_dir.exists():
            return []

        import glob
        import json

        # è·å–æœ€æ–°çš„æ¨¡å¼æ–‡ä»¶
        pattern_files = glob.glob(str(pattern_dir / "patterns_*.json"))
        if not pattern_files:
            return []

        latest_file = max(pattern_files)
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½ç°æœ‰æ¨¡å¼å¤±è´¥: {e}")
            return []

    def _merge_patterns(self, existing: List[Dict[str, Any]], new: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶æ–°æ—§æ¨¡å¼"""
        merged = existing.copy()

        for new_pattern in new:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼æ¨¡å¼
            found = False
            for existing_pattern in merged:
                if existing_pattern['name'] == new_pattern['name']:
                    # æ›´æ–°é¢‘ç‡å’Œç½®ä¿¡åº¦
                    existing_pattern['frequency'] += new_pattern['frequency']
                    existing_pattern['last_updated'] = datetime.now().isoformat()
                    found = True
                    break

            if not found:
                # æ·»åŠ æ–°æ¨¡å¼
                new_pattern['first_seen'] = datetime.now().isoformat()
                new_pattern['last_updated'] = datetime.now().isoformat()
                merged.append(new_pattern)

        # æŒ‰é¢‘ç‡æ’åº
        merged.sort(key=lambda x: x['frequency'], reverse=True)

        return merged

    def _send_urgent_alert(self, alerts: List[str]):
        """å‘é€ç´§æ€¥é€šçŸ¥"""
        logger.warning("ç´§æ€¥è¶‹åŠ¿é€šçŸ¥:")
        for alert in alerts:
            logger.warning(f"  {alert}")

        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„é€šçŸ¥é€»è¾‘
        # ä¾‹å¦‚ï¼šå‘é€åˆ°Slackã€é‚®ä»¶ã€å¾®ä¿¡ç­‰
        # self._send_slack_message(alerts)
        # self._send_email(alerts)

    def run(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        logger.info("å¯åŠ¨åŠ¨æ€è¿½è¸ªè°ƒåº¦å™¨")
        self.setup_schedule()

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¯æ—¥æ‘˜è¦ï¼ˆå¦‚æœè¿˜æ²¡æ‰§è¡Œè¿‡ï¼‰
        now = datetime.now()
        if now.hour >= 8:
            logger.info("ä»Šæ—¥å·²è¿‡8ç‚¹ï¼Œè·³è¿‡æ¯æ—¥æ‘˜è¦æ‰§è¡Œ")
        else:
            logger.info("æ‰§è¡Œé¦–æ¬¡æ¯æ—¥æ‘˜è¦")
            self._daily_digest()

        # ä¸»å¾ªç¯
        logger.info("è°ƒåº¦å™¨è¿è¡Œä¸­ï¼ŒæŒ‰Ctrl+Cåœæ­¢")
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œè°ƒåº¦å™¨åœæ­¢")
        except Exception as e:
            logger.error(f"è°ƒåº¦å™¨è¿è¡Œå‡ºé”™: {e}")
        finally:
            logger.info("è°ƒåº¦å™¨å·²åœæ­¢")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("YouTubeåŠ¨æ€è¿½è¸ªè°ƒåº¦å™¨")
    print("=" * 60)
    print("\nåŠŸèƒ½è¯´æ˜:")
    print("1. æ¯å°æ—¶æ£€æŸ¥è¶‹åŠ¿å˜åŒ–")
    print("2. æ¯å¤©æ—©ä¸Š8ç‚¹ç”Ÿæˆæ‘˜è¦")
    print("3. æ¯å‘¨ä¸€ç”Ÿæˆå‘¨æŠ¥")
    print("4. æ¯3å¤©æ›´æ–°æ¨¡å¼åº“")
    print("\næŒ‰ Ctrl+C åœæ­¢è°ƒåº¦å™¨\n")

    config = get_config()
    scheduler = TrackingScheduler(config)
    scheduler.run()


if __name__ == '__main__':
    main()

---
name: project-real
description: YouTube 端到端内容创作流水线的现实约束
---

# 现实约束 (Real)

> 这些是 AI 不太容易想到，但在现实世界中会造成实际损失的硬性限制。

<real>

## 必选约束（必须遵守）

1. **YouTube 社区准则红线**
   - 不得批量上传重复/低质内容（会触发频道惩罚）
   - 不得使用误导性标题/封面（违反欺骗性做法政策）
   - 不得在元数据中堆砌无关关键词（会降低搜索排名）

2. **版权合规**
   - 竞品视频仅供分析，不得直接使用其内容
   - 背景音乐必须使用免版税或已授权素材
   - 字幕翻译不构成原创，转载需注明来源

3. **API/自动化限制**（含数据采集）
   - YouTube Data API 每日配额有限（默认 10,000 单位），需谨慎使用
   - 浏览器自动化操作频率不得过快（建议每次操作间隔 2-5 秒）
   - 登录态 Cookie 有效期有限，需定期刷新或重新登录
   - **数据采集三角约束**：速率 × 质量 × 覆盖度 不可兼得（详见附录 A）
   - **数据采集能力边界**（详见附录 B）：
     * 直接可采集：video metadata (title, views, likes, comments, duration, published_at 等)
     * 可采集但有成本：频道统计数据、增长快照（TrendSnapshot）
     * 不可采集或高成本：评论热词、内容分类、搜索排名、中心性网络分析

4. **存储与成本控制**
   - 视频文件体积大（1080p 约 100MB/分钟），必须执行滚动清理策略
   - 避免无限缓存竞品视频，设置 30 天自动删除
   - Token 消耗需监控，综合模式单次上传应控制在 ~1,000 token

5. **文件名冲突禁区**
   - ❌ 不得创建重名文件（同一目录内）
   - ❌ 不得创建不包含日期或唯一标识的核心文档
   - ❌ 不得在子文件夹中创建与兄弟文件夹同名的文件
   - ❌ 不得创建文件时跳过编码规约检查
   - ✅ 创建文件前，必须用 Grep 检查全局是否有同名文件
   - ✅ 文件名必须遵循 `<YYYYMMDD>_<type>_<topic>.md` 的格式
   - ✅ 如果发现可能的冲突，主动列举所有候选名字供用户选择

</real>

<real optional="true">

## 可选约束（建议遵守）

5. **发布时间优化**
   - 目标受众活跃时间发布（如中文内容建议北京时间 18:00-22:00）
   - 避免密集发布（同一频道每日最多 1-2 个视频）

6. **SEO 最佳实践**
   - 标题长度建议 50-60 字符（移动端显示完整）
   - 描述前 150 字符应包含核心关键词（搜索结果可见部分）
   - 标签数量建议 5-15 个（太多反而稀释相关性）

7. **本地环境依赖**
   - mcp-chrome 需要 Chrome 浏览器已安装并登录 YouTube
   - Whisper 模型首次运行会下载权重文件（base 模型约 150MB）
   - FFmpeg 需要支持 libx264 编码器

</real>

---

## 约束的执行时机

| 约束 | 执行时机 | 检查方式 |
|------|----------|----------|
| YouTube 社区准则 | 发布前 | 人工审核 + AI 内容检测 |
| 版权合规 | 制作阶段 | 素材来源记录 |
| API 配额限制 | 调研/发布阶段 | 配额计数器 |
| 存储清理 | 每日定时任务 | cron job + storage.yaml 策略 |
| 发布时间 | 发布阶段 | 定时发布功能 |
| SEO 最佳实践 | 策划阶段 | seo_optimizer.py 评分 |

---

## 与 prompts/ 的关系

提示词执行前，应检查是否触发上述约束。例如：

- `PROMPT_02_调研阶段.md` 执行批量下载时 → 检查存储空间是否充足
- `PROMPT_05_发布阶段.md` 执行上传时 → 检查元数据是否符合 SEO 规范
- 任何涉及 API 调用的操作 → 检查当日配额余量

这样，约束不再是事后的"后悔药"，而是**流程的前置守卫**。

---

## 附录 A：数据采集三角约束

```
        速率 (每天能采多少)
           /\
          /  \
         /    \
        /______\
   质量          覆盖度
(有完整详情)    (话题多样性)
```

**核心洞察**：三者不可兼得，必须根据阶段目标取舍。

### 三种采集策略

| 策略 | 速率 | 质量 | 覆盖度 | 适用场景 |
|------|------|------|--------|----------|
| **广度优先** | 1000条/天 | 低(仅标题) | 高 | 建立基线、发现新话题 |
| **深度优先** | 100条/天 | 高(含详情) | 低 | 竞品深度分析 |
| **平衡模式** | 500+50条/天 | 中 | 中 | 日常运营(推荐) |

### 关键瓶颈数字

| 瓶颈 | 限制 | 应对 |
|------|------|------|
| yt-dlp 速率 | ~20条/次搜索，连续请求易被限 | 间隔 3-5 秒，分散时段 |
| 详情获取 | ~5-10秒/条 | 只对高价值视频(>1万播放)获取 |
| 网络分析 | >50,000 节点时变慢 | 增量计算，定期归档 |
| 重复率 | 热门视频占比 30-50% | 去重后实际量打 5-7 折 |

### 建议采集节奏

```yaml
# 健康频道博主推荐配置
phase_1_baseline:  # 第 1 个月
  target: 10,000 条
  focus: 覆盖 50+ 子话题
  detail_ratio: 10%

phase_2_tracking:  # 第 2-3 个月
  target: 5,000 条/月
  focus: 追踪趋势变化
  detail_ratio: 5%

phase_3_maintain:  # 持续
  target: 500 条/周
  focus: 新兴话题监控
  detail_ratio: 10%
```

> **记住**：专家直觉来自结构化认知，不是数据堆积。5000 条高质量 > 30000 条低质量。

---

## 附录 B：实际可采集数据与分析能力的边界

> **核心原则**：诚实地承认技术边界，基于**实际可得的真实数据**进行分析，避免虚假推断。

### B.1 YouTube API 直接可采集的数据（✅ 可靠）

> **基于 v2 实际采集经验**：两阶段采集策略，先快后详，最小化 API 成本

#### 采集阶段与速度

**阶段 1: 快速搜索** (`search_videos_fast`)
- 速度：**~0.5 秒/视频**
- 用途：快速获取大量视频的基础信息
- 采集字段：youtube_id, title, channel_name, view_count, duration
- 规模：每次搜索 20 条 → 单关键词 100-200 条/轮

**阶段 2: 详情补充** (`enrich_video_details`)
- 速度：**~3-5 秒/视频**
- 用途：对高播放量视频获取完整元数据
- 采集字段：upload_date, like_count, comment_count, description, tags, channel_id, thumbnail_url
- 规模：仅对 Top 100 视频（view_count >= 10,000）补充详情

#### 视频级元数据 (videos.list API)

**基础字段**（阶段1采集，0.5s/video）
- `video_id` (YouTube ID)
- `title` (标题)
- `channel_name` (频道名)
- `view_count` ✓ (播放量 - 公开准确)
- `duration` (时长 - 精确秒数)

**详细字段**（阶段2采集，3-5s/video，仅高播放量）
- `channel_id` 和 `channel_title` (完整频道信息)
- `published_at` / `upload_date` (发布时间)
- `like_count` ✓ (点赞数)
- `comment_count` ✓ (评论总数)
- `thumbnail_url` (缩略图)
- `description` (视频描述)
- `tags` (官方标签列表，用于二次采集扩展)
- `language` (视频语言)

#### 频道级统计 (channels.list API)

**获取时机**：在详情阶段同步获取

- `channel_id` (频道 ID)
- `channel_name` (频道名称)
- `subscriber_count` ✓ (订阅数)
- `video_count` ✓ (频道视频总数)
- `view_count` ✓ (频道总播放量)
- `created_at` (频道创建时间，用于模式分析)

#### 可计算的派生指标

**实时计算指标**（无额外成本）
- 互动率 = (likes + comments) / views
- 平均播放 = channel_total_views / channel_video_count
- 效率分数 = avg_views / subscriber_count
- 发布天数 = now - published_at
- 日均播放 = views / days_since_publish
- 时长分桶 = bucketize(duration) 按 [0-5min, 5-10min, 10-20min, 20+min] 区间分类

**聚合统计指标**（基于采集的样本）
- 频道热度排名 = 频道平均播放量排序
- 时长最优区间 = 各时长段视频的平均播放量对比
- 话题热度 = 各关键词搜索结果数量和平均播放

### B.2 可采集但有成本的数据（⚠️ 需要权衡）

#### 关键限制 1：时间精度不足

> ⚠️ **重要**：yt-dlp 仅提供 `published_at` (日期级，如 2026-02-03)，**不包含发布小时信息**

**影响**：v2 中基于小时级数据的所有模式都**不成立**
- ❌ 模式15：最佳发布时段分析（需要小时级 timestamp）
- ❌ 按小时分析的任何统计

**当前收集的时间粒度**：
- ✅ 日期级 (YYYY-MM-DD)：用于日期范围筛选
- ✅ 星期级：用于周末vs工作日分析（通过 `published_at` 推导）
- ❌ 小时级：**无法获取**（YouTube API 不提供）

**解决方案**（如需要小时级分析）：
1. 使用 YouTube Analytics API（需要频道所有者权限，仅自有频道）
2. 通过网页爬虫获取（需要登录，易被限流，成本高）
3. 调整分析策略为日期级或周级

#### 关键限制 2：任意分档分析都缺乏实证意义

> ⚠️ **重要**：没有真实的约束或业务含义的分档分析都**无意义**

**问题示例**：

1. **时长分档** - v2 的自定义分档（不符合 YouTube 逻辑）：
   - ❌ `<1分钟（短）`、`1-3分钟`、`3-10分钟`、`10-30分钟`、`30分+`
   - 问题：这些分档是事后聚合（post-hoc bucketing），不对应任何真实约束
   - 为什么无意义：没有YouTube原生过滤器支持这些分档，选这些边界值也没有业务原因

2. **标题长度分档** - v2 中的标题长度分析（例：50、30、15 字符）：
   ```sql
   CASE
       WHEN CHAR_LENGTH(title) >= 50 THEN '超长(50+字)'
       WHEN CHAR_LENGTH(title) >= 30 THEN '长(30-50字)'
       WHEN CHAR_LENGTH(title) >= 15 THEN '中(15-30字)'
       ELSE '短(<15字)'
   END as title_length
   ```
   - 问题：这些数字是任意选择，没有真实的约束
   - 为什么无意义：
     * YouTube 搜索结果显示：PC 端显示 ~50-60 字符，手机端更短
     * 但这是 UI 展示限制，不影响推荐算法
     * 不同话题的最优标题长度不同，固定分档无法体现
   - 正确做法：不做固定分档，而是分析实际数据分布

**YouTube 原生时长过滤器**（searchFilters sp参数支持）：
- `short` - 短视频（< 4分钟）
- `medium` - 中等视频（4-20分钟）
- `long` - 长视频（> 20分钟）

**✅ 正确的做法：基于真实约束的分析**

1. **使用 YouTube 原生分档**（对应搜索过滤器）：
   ```
   - short: < 4分钟
   - medium: 4-20分钟
   - long: > 20分钟
   ```
   原因：这些分档是 YouTube 搜索本身的分类，有真实意义

2. **使用数据驱动的 P-分位数分桶**（基于实际样本）：
   ```
   - p25: 最短 25%
   - p25-p50: 较短 25%
   - p50-p75: 较长 25%
   - p75+: 最长 25%
   ```
   原因：每个分组样本量均匀，对比有意义

3. **按内容类型分别分析**（针对不同业务）：
   ```
   分别计算教程/评测/段子类视频的时长特征
   而非所有内容混合分析
   ```
   原因：不同内容有不同的最优时长

4. **保留原始数据和分布**（透明化）：
   ```
   - 展示完整的时长分布直方图
   - 标注平均值、中位数、标准差
   - 不做任意分档聚合
   ```
   原因：不失信息，让用户自主判断

**❌ 不要做的事**：
- ❌ 使用 v2 的任意分档边界值（如 1, 3, 10, 30 分钟）
- ❌ 硬编码标题长度分档（如 15, 30, 50 字符）
- ❌ 定义没有业务含义的区间（如"中长"、"超长"）
- ❌ 聚合不同内容类型的数据后分析

#### 时间窗口与采集策略

> **基于 v2 实践**：4 个时间维度，不同频率和深度，平衡覆盖度和成本

| 时间维度 | YouTube sp参数 | 采集频率 | 采集规模 | 保留详情周期 | 用途 |
|---------|--------|----------|---------|-----------|------|
| **24小时** | EgQIAhAB | 每2小时 | Top 100 | 7天 | 实时热点发现 |
| **7天** | EgQIAxAB | 每天 | Top 200 | 30天 | 周趋势分析 |
| **30天** | EgQIBBAB | 每天 | Top 500 | 90天 | 月度报告 |
| **1年** | EgQIBRAB | 每周 | Top 1000 | 持续 | 季度审视 |

**sp参数说明**：YouTube 搜索 API 使用编码的 sp 参数进行时间过滤
- `EgQIARAB` = 过去1小时
- `EgQIAhAB` = 24小时内（1天）
- `EgQIAxAB` = 7天内（1周）
- `EgQIBBAB` = 30天内（1个月）
- `EgQIBRAB` = 今年

#### 数据分组与更新频率

> **从 v2 实践提炼**：5 个数据组，分层更新，降低维护成本

| 数据组 | 包含字段 | 更新频率 | 成本 | 用途 |
|-------|--------|--------|------|------|
| **基础信息** | title, duration, published_at, thumbnail | 一次性 | 0 | 元数据 |
| **播放数据** | view_count, upload_date | 每天 | API配额<100 | 观众规模 |
| **互动数据** | like_count, comment_count | 每天 | API配额<100 | 用户参与 |
| **频道数据** | subscriber_count, video_count, channel_view_count | 每周 | API配额<50 | 频道实力 |
| **增长数据** | daily_view_gain, weekly_subscriber_gain | 实时 | 系统计算 | 趋势预测 |

#### 定时快照追踪 (TrendSnapshot)

- 需要：重复采集同一视频的 view_count，建立时间序列
- 成本：占用 API 配额，需要定时任务（每天 ~100-200 配额）
- 采集频率：每天 1 次（推荐）或每周 1 次（成本低）
- 数据保留：最新 30 天的详细数据，之后聚合为周平均
- 价值：识别快速增长的视频、发现新兴话题
- 实现建议：在每日采集任务中同步更新前 N 条视频的快照

#### 搜索热度趋势 (Google Trends)

- 需要：调用 Google Trends API（或免费爬虫）
- 成本：**完全免费**，无 API 配额消耗 ✅
- 数据获取：
  * 搜索量时间序列（按地区、时段、年份）
  * 相关搜索词（搜索建议，真实用户高频词）
  * 话题热度对比（相对值，不是绝对值）
- 价值极高：
  * 发现上升趋势话题（早期布局机会，+200% 增长识别）
  * 跨语言套利发现（如"Tai Chi"英文市场 +533%）
  * 话题生命周期分析（上升/高位/下降阶段）
  * 区域差异识别（中文市场 vs 英文市场）
- 建议优先级：**P1（应该优先在 MVP 阶段实现）**
- 集成方式：
  * 对每个采集关键词自动查询 Google Trends
  * 与 YouTube 数据关联分析
  * 用于 ArbitrageOpportunity 实体的数据源

#### 评论数据采样 (comments.list API 或 网页爬虫)

**方案 1：YouTube API comments.list（推荐 MVP 方案）**

- 成本：配额消耗（1条评论 = 1API配额），但可控
- 采样策略：
  * 仅对 **Top 10%** 高播放量视频采样（150条视频 × 100条评论 = 15,000配额）
  * 每天执行一次，在日 10,000 配额限制内
  * 评论采样范围：最新的前 100 条（用户最关注的反馈）
- 提取方法：jieba 分词 + 开源情感词库（无需预训练模型，无额外成本）
- 价值：
  * 识别用户痛点和常见问题
  * 评论热词提取（提升视频内容针对性）
  * 情感分析（内容反响评估，点赞/批评比例）
  * 用户维度洞察（评论者地理位置、设备类型等）
- 实现优先级：**P2（Phase 2 进行）**

**方案 2：网页爬虫（Selenium）（后期增强方案）**

- 成本：速度慢（5-10秒/个视频），但无 API 配额消耗
- 优势：能获取更多评论，无 API 限制
- 适用场景：竞品深度分析、非实时离线分析
- 实现优先级：**P3（后期增强）**

#### 频道分析数据 (analytics endpoint - 需要频道所有者权限)

- `watch_time_minutes` (观看时长)
- `average_view_duration` (平均观看时长)
- `impressions` (曝光次数)
- `ctr` (点击率)
- `subscribers_gained` / `subscribers_lost` (增减订阅)
- **限制**：仅能获取自己的频道，无法获取竞品数据
- **用途**：用于自身频道的 Analytics 复盘和优化（后期功能）

### B.3 关键词配置与采集策略

> **基于 v2 实践**：三层关键词优先级，自动扩展和去重机制

#### 关键词优先级配置

**v2 实践中的配置结构**：

```yaml
# core.json - 核心关键词（权重 1.0）
核心关键词:
  - 养生
  - AI工具
  - 美食制作
  优先级: 高
  更新频率: 每天

# extended.json - 扩展关键词（权重 0.8）
扩展关键词:
  - 中医养生
  - ChatGPT教程
  - 家常菜做法
  优先级: 中
  更新频率: 每周

# competitors.json - 竞品关键词（权重 0.5）
竞品关键词:
  - 养生堂 (特定频道)
  - AI应用评测
  - 美食vlog
  优先级: 低
  更新频率: 每月
```

#### 关键词自动扩展（三层降级）

**优先级 1：YouTube 搜索建议 API**（真实用户行为）
- 来源：`suggestqueries.google.com/complete/search?client=youtube&ds=yt&q={keyword}`
- 方法：
  * 获取种子词的搜索建议（如"养生" → ["养生堂", "养生功法", "养生茶", ...]）
  * 对每个建议添加后缀再搜索（如"养生" + "教程" → "养生教程" 的新建议）
  * 合并去重，最多 15 个
- 成功条件：获取 >= 5 个有效关键词
- 失败降级：网络错误或返回词数不足

**优先级 2：预定义关键词映射**（人工精选）
- 来源：v2 中整理的 `keyword_mappings` 字典
- 方法：
  * 精确匹配：theme == "养生" → 直接返回映射
  * 模糊匹配："老人养生" 包含 "养生" → 返回养生映射
- 特点：质量高，覆盖有限
- 成功条件：找到匹配的映射

**优先级 3：默认后缀扩展**（兜底策略）
- 来源：内置后缀列表
- 方法：theme + ["教程", "入门", "技巧", "方法", "推荐", "攻略", "2025", "怎么", "如何", "最新"]
- 示例："区块链" → ["区块链", "区块链教程", "区块链入门", ...]
- 成功条件：永远成功

#### 标签二次采集（基于已采集数据）

> **v2 创新实践**：从已采集视频的标签提取高频词，作为新关键词

**流程**：
1. 统计已采集视频的 tags 字段中出现 >= 10 次的标签
2. 排除已采集过的关键词（避免重复）
3. 排除英文通用词（如 health, tips, care）
4. 排除太通用的词（如 "健康", "视频", "分享"）
5. 用高频标签作为新关键词搜索

**效果**：
- 关键词来自真实视频，比 AI 猜测更精准
- 覆盖细分领域（如 "穴位保健", "中医药方")，而非硬编码关键词
- 去重率极低（发现大量新视频）

**示例**：
```
养生主题已采集视频统计:
- "中老年": 48 次 → 新关键词
- "中医药": 35 次 → 新关键词
- "穴位": 28 次 → 新关键词
- 健康: 120 次 → 排除（通用词）
```

#### 采集速度与规模约束

| 操作 | 速度 | 规模 | 配额消耗 |
|-----|------|------|--------|
| 快速搜索 (Stage 1) | 0.5s/video | 20条/搜索 | ~5单位/100条 |
| 详情补充 (Stage 2) | 3-5s/video | Top 100 | ~200单位/100条 |
| 并行搜索 | 10关键词/分钟 | 10并发 | 快速 |
| 大规模采集 500 条 | ~3分钟搜索 | 5-15个关键词 | ~500-1000配额 |

#### 原本被认为不可行的数据

**搜索排名数据**
- 问题：YouTube API 不提供 "搜索排名第几位" 的信息
- 替代方案1：使用 Selenium 爬虫模拟搜索（极慢，易被封）❌
- 替代方案2：使用 **Google Trends API 查询搜索热度** ✅
  * 优势：Google 官方接口，可靠且不违反ToS
  * 数据：各地区搜索量趋势、相关话题、搜索建议
  * 成本：**无API配额消耗，完全免费**
  * 应用：发现新兴话题、跨语言套利（如 "Tai Chi" 在英语市场的爆发）
- 结论：**推荐使用 Google Trends 替代 YouTube 搜索排名（P1 优先级）**

**AI 生成视频识别**
- 问题：无法从 metadata 判断视频是否 AI 生成
- 替代方案：
  * 人工标记样本 + ML 分类（需要标注数据）
  * 调用专业 AI 检测服务（付费，成本高）
  * 关键词启发式识别（效果差)
- 结论：**超出项目范围，暂不实现**

**视频内容分类** (教程 vs 故事 vs 评测)
- 问题：无法仅从 title/description 准确分类
- 方案：
  * 完全依赖人工标记（低效）
  * 结合标题关键词 + 标签的启发式规则（精度低)
  * 调用商业 NLP API（成本高）
- 结论：**暂不实现，改用关键词启发式**

**话题网络中心性分析** (KeywordNetwork)
- 问题：需要构建话题网络 + 计算 betweenness/degree centrality
- 成本：
  * NLP 分词（需要 jieba + 预训练模型）
  * 网络计算（networkx，>50k 节点时变慢）
  * 频繁更新需要重算
- 推荐：
  * **简化版**：只统计关键词频率，不计算中心性
  * **完整版**：后期若有需求，再投入开发
- 结论：**MVP 版本推荐简化版，可快速上线**

### B.4 可行的分析能力（✅ 直接支持）

基于上述两阶段采集策略和实际数据，以下分析 **100% 可实现**：

#### P0 优先级（MVP 阶段，第 1-2 周）

基础数据组（基础信息 + 播放数据 + 互动数据）无额外成本

| 分析类型 | 实现方式 | 数据成本 | 典型场景 |
|----------|--------|--------|---------|
| **四象限分类** (ContentQuadrant) | 按 views 和互动率 (likes+comments)/views 分布 | 0 | 爆款特征识别 |
| **时长分布分析** (DurationMatrix) | 对采集的 duration 字段按 [0-5, 5-10, 10-20, 20+] 分桶统计 | 0 | 蓝海时长发现 |
| **频道排行榜** (RankingList) | 计算 avg_views, 效率分数, 按指标排序 | 0 | 对标频道选择 |
| **视频质量评分** | views + 互动率 + 新鲜度 (发布距今天数) | 0 | 单视频价值评估 |
| **黑马频道识别** | 低订阅 + 高平均播放 的频道 | 0 | 新兴内容创作者 |

#### P1 优先级（深度分析，第 2-4 周）

扩展数据组（频道数据 + 增长数据）+ Google Trends（免费）

| 分析类型 | 实现方式 | 数据成本 | 典型场景 |
|----------|--------|--------|---------|
| **增长趋势** (TrendSnapshot) | 建立 Top 视频的 views 时间序列，识别增长率 | ~100配额/天 | 快速增长视频发现 |
| **搜索热度趋势** (Google Trends) | 对每个采集关键词查询 Google Trends 数据 | 0（完全免费）| 话题生命周期 + 跨语言套利 |
| **话题频率统计** | 对视频标题用 jieba 分词，统计词频 | 0 | 热点话题识别 |
| **频道实力评估** | subscriber_count, video_count, channel_view_count 趋势 | ~50配额/周 | 频道增长潜力 |

#### P2 优先级（用户洞察，第 4-6 周）

评论采样数据 + NLP 分析（15k 配额/天）

| 分析类型 | 实现方式 | 数据成本 | 典型场景 |
|----------|--------|--------|---------|
| **评论热词提取** (采样) | Top 10% 视频 × 100 条评论，jieba 分词统计 | ~15k配额/天 | 用户关注点识别 |
| **用户痛点识别** | 评论内容分析，提取问题相关表述 | 同上 | 内容优化方向 |
| **情感分析** (采样) | 使用开源情感词库对采样评论评分 | 同上 | 内容反响评估 |

#### P3 优先级（后期增强）

高成本分析，需要额外投入

| 分析类型 | 实现方式 | 数据成本 | 典型场景 |
|----------|--------|--------|---------|
| **中心性网络分析** | 话题共现网络 + networkx 计算中心性 | CPU 高 | 话题影响力评估 |
| **完整评论爬虫** | Selenium 获取所有评论，无 API 消耗但速度慢 | 时间长 | 竞品深度分析 |
| **内容分类模型** | 训练 ML 模型分类视频类型 | 标注数据高 | 内容标签体系 |

#### 分析能力与数据组的对应关系

**基础信息组**（一次性，0成本）
- 元数据完整性（标题、描述、标签是否充分）

**播放数据组**（每天，<100配额）
- 播放量排序和对比
- 新鲜视频识别（发布距今时间）
- 时长与播放的相关性

**互动数据组**（每天，<100配额）
- 互动率计算和排名
- 用户参与度对比
- 点赞率 vs 评论率差异

**频道数据组**（每周，<50配额）
- 频道实力排名
- 频道增长势头
- 订阅转化率估算

**增长数据组**（实时计算，0配额）
- 周环比、月环比增长率
- 快速增长视频识别（预警）
- 趋势拐点检测

### B.5 对 cog.md 的修正与补充

> **基于实际采集能力**：确保 cog.md 中的实体定义对应到真实可获取的数据

**不应该在 cog 中定义的实体**（因为数据无法获取或成本过高）：

- ❌ `ai_keyword` (无法识别，需要 ML 模型分类)
- ❌ `is_ai_video` (需要专业 AI 检测服务，成本高)
- ❌ `KeywordNetwork` 的 betweenness_centrality (高成本，需要 networkx 计算，>50k 节点时变慢)
- ❌ 完整的评论热词分析（API 成本过高，改为采样方案）
- ❌ `video_content_type` 分类（无法仅从 metadata 准确分类，建议用关键词启发式）

**应该添加的实体**（新的数据源）：

- ✅ **`GoogleTrends` (SearchTrendData)**
  * 字段：keyword, region, time_period, search_volume_trend (时间序列), related_terms
  * 采集频率：每次采集关键词时同步查询（0成本）
  * 作为 `ArbitrageOpportunity` 的数据来源（发现新兴话题）
  * 应用场景：跨语言套利分析（如"Tai Chi"英文市场 +533%）、话题生命周期分析
  * 优先级：P1（应在 MVP 阶段实现）

- ✅ **`CommentSample` (评论采样数据)**
  * 字段：video_id, sample_comments (前100条), comment_count_total, sample_date
  * 采集范围：仅 Top 10% 高播放量视频
  * 配额消耗：150 视频 × 100 条 = 15,000 配额/天（在日限内）
  * 用于 `PatternAnalysis` 的用户维度分析
  * 应用场景：识别用户问题、评论热词提取、情感分析
  * 优先级：P2（Phase 2 进行）

**应该简化或调整的实体**：

- 🔄 **`KeywordNetwork` 简化版**
  * ❌ 不计算：betweenness_centrality, degree_centrality, eigenvector_centrality（高成本）
  * ✅ 只统计：keyword_frequency (关键词出现次数), co_occurrence_count (共现次数)
  * 目的：快速识别热点话题，而非精确网络分析
  * 实现成本：O(n log n)，可实时更新

- 🔄 **`PatternAnalysis` 的用户维度调整**
  * ❌ 不依赖：完整的评论集合、NLP 预训练模型
  * ✅ 改为：基于 `CommentSample` 的采样统计（jieba + 情感词库）
  * 数据来源：Top 视频前 100 条评论，而非所有评论
  * 输出：常见问题、用户痛点、情感倾向

- 🔄 **`CommentAnalysis` 简化**
  * ✅ 保留的能力：关键词提取（jieba 分词）、基础情感分析（词库匹配）
  * ❌ 移除的能力：复杂 NLP（需预训练模型）、细粒度情感（需标注数据）
  * 实现方式：开源方案，无额外成本

**字段标注规范**：

在 cog.md 中标注每个字段的来源类型：

```
CompetitorVideo:
  # 阶段1采集（0.5s/video，基础信息）
  - youtube_id [STAGE1]
  - title [STAGE1]
  - channel_name [STAGE1]
  - view_count [STAGE1]
  - duration [STAGE1]

  # 阶段2采集（3-5s/video，仅Top视频）
  - upload_date [STAGE2-HIGH-VIEW]
  - like_count [STAGE2-HIGH-VIEW]
  - comment_count [STAGE2-HIGH-VIEW]
  - tags [STAGE2-HIGH-VIEW]
  - channel_id [STAGE2-HIGH-VIEW]

  # 派生计算（无额外成本）
  - engagement_rate [COMPUTED]
  - days_since_publish [COMPUTED]

  # 可选采集（成本标注）
  - trending_growth [OPTIONAL-100配额/天]
  - comment_sample [OPTIONAL-15k配额/天]
```

### B.6 推荐的 MVP 数据收集方案与 v2 模式验证

```yaml
阶段 1: 基础采集 (第 1-2 周)
  目标: 建立 1000 条视频的基线数据
  采集内容:
    - yt-dlp 搜索: title, video_id, channel, views, duration, published_at (日期级)
    - 频道统计: subscriber_count, video_count, total_views
  成本: ~2-3 小时数据采集
  产出: 基础数据库 + 时长分布图 + 四象限分类
  注意: published_at 仅提供日期，不包含小时信息

阶段 2: 深度分析 (第 2-4 周)
  目标: 提供可执行的视频创作建议
  分析内容:
    - ContentQuadrant: 爆款特征分析
    - DurationMatrix: 蓝海时长区间
    - RankingList: 频道对标分析
    - 质量评分: 视频优劣势评分
    - 话题生命周期: 按日期级分析（周末vs工作日, 按月对比）
  成本: 无额外 API 消耗
  产出: 可视化仪表盘 + 创作建议
  排除: 时间维度的小时级分析（数据不可得）

阶段 3: 持续追踪 (第 4+ 周)
  目标: 监控市场变化
  追踪内容:
    - TrendSnapshot: 按周追踪 Top 视频增长
    - Google Trends 监控: 新兴话题预警
    - 新话题发现: 每周补充 200-300 条新数据 + 标签二次采集
  成本: ~1000 个 API 单位/周（在配额内）
  产出: 增长趋势图 + 新机会预警
```

#### 从 v2 迁移的 42 个模式的可行性评估

> **基于实际数据采集能力和 YouTube 原生逻辑重新审视**

**❌ 完全不可行的模式**：

**时间相关**：
- ❌ **模式15：最佳发布时段分析** - 需要 HOUR(timestamp)，但 yt-dlp 仅提供日期
  * v2 假设：有秒级精度的 timestamp
  * 实际收集：仅有日期级 published_at
  * 修正方案：改为日级分析（如"周末vs工作日"、"按月对比"）

**时长分析相关**：
- ❌ **模式中的自定义时长分档** - v2 使用的 "<1分钟"、"1-3分钟" 等分档不符合 YouTube 原生逻辑
  * v2 假设：可以灵活定义时长区间并用于分析
  * 实际约束：时长只能事后聚合，不对应原生搜索过滤
  * 修正方案：改用 YouTube 原生分档（<4min, 4-20min, >20min）或用数据驱动P分位数分桶

**⚠️ 需要验证和调整的模式**：

- ⚠️ **模式5：周末发布效果分析** - 可行，但需用日期推导星期
  * 改进方式：通过 `published_at` 计算 DAYOFWEEK，而非直接用 timestamp
  * 数据足够性：需要足够的样本量（当前 2340 条足够）
  * 调整点：排除小时级因素，只看日期

- ⚠️ **模式13：话题生命周期分析** - 可行，但仅支持日级或周级粒度
  * 改进方式：分析话题的月度/周度高频词，而非日级变化
  * 数据足够性：时间跨度足够（2024-2026）
  * 调整点：不分析高频波动，只看长期趋势

- ❌ **模式2-4（变量分布中的任意分档部分）** - 如果使用 v2 的分档则无意义
  * 问题：v2 分档（如"1-3分钟"、"30+字"）是任意划分，缺乏真实约束或业务含义
  * v2 分档无效，改用数据驱动分桶（P-分位数）或 YouTube 原生分类
  * 调整方案：
    - 删除所有任意分档，改用 P-分位数或原生分类
    - 保留原始数据分布（直方图）
    - 按内容类型分别分析，不混合
  * 可行性：重新定义后可行

**✅ 完全可行的模式**：

- ✅ 模式1（数据基础统计）：直接可用
- ✅ 模式6-12（空间维度、频道维度）：使用日期级信息足够
- ✅ 模式14（频道创建时间）：仅需日期级
- ✅ 模式16-26（用户维度、其他）：取决于具体方法，需逐一审视

#### 模式数据有效性清单

| 模式类型 | 数据依赖 | v2可用性 | v3可行性 | 备注 |
|---------|---------|---------|---------|------|
| 基础统计 | 基本字段 | ✅ | ✅ | 直接适用 |
| 变量分布 | duration精确值 | ✅ | ⚠️ | 分档需重新定义，不用v2方案 |
| 时间维度（日） | published_at | ✅ | ✅ | 周末/工作日分析可行 |
| 时间维度（小时） | timestamp with hour | ✅ (v2) | ❌ | v3无法获取，需改为日级 |
| 空间维度 | 频道、地区 | ✅ | ✅ | 直接适用 |
| 频道维度 | subscriber_count趋势 | ✅ | ⚠️ | 历史数据仅可用当前值，不能重构趋势 |
| 用户维度 | 评论数据 | ✅ | ⚠️ | 仅支持采样方案，不支持完整分析 |

**建议清单**：
1. ✅ 标注每个模式的数据时间粒度要求（小时/日/周/月）
2. ⚠️ 重新定义时长分档（用数据驱动，不用v2硬编码值）
3. ❌ 移除或重写所有依赖小时级数据的模式（模式15等）
4. 📝 在cog.md中添加"模式有效性"标注（可用/需调整/不可用）

### B.6b 分析质量守则：避免无意义的分档分析

> **核心原则**：任何分析的分档或分类都必须基于真实约束或业务含义，而非任意数字

#### 红线规则

**❌ 禁止做的事（违反这些会产生虚假分析）**：

1. **禁止任意数值分档**
   - ❌ 标题长度：15字、30字、50字
   - ❌ 时长：1分钟、3分钟、10分钟、30分钟
   - ❌ 播放量：1000、10000、100000、1000000
   - 原因：这些数字都是凭空选择，没有真实含义

2. **禁止模糊标签**
   - ❌ "短"、"中"、"长"（相对概念，没有具体含义）
   - ❌ "低"、"中"、"高"（不同话题的含义不同）
   - ❌ "超长"、"适中"（价值判断，不是事实陈述）

3. **禁止不同内容混合分析**
   - ❌ 在一个图表中展示教程、段子、评测的时长对比（它们的最优时长完全不同）
   - ❌ 跨地区跨话题的通用结论
   - 原因：违反了 Simpsons Paradox（辛普森悖论）

4. **禁止事后聚合伪装成先验约束**
   - ❌ "我们按 3-10 分钟分档"，暗示这个范围有特殊含义（其实是任意的）
   - ✅ 应该说："根据实际数据的 P-分位数，该话题最常见的长度分布为..."

#### 正确的做法（基于真实约束）

**✅ 基于 YouTube 原生概念的分析**：
- 时长：使用 YouTube 搜索本身支持的分类（<4min, 4-20min, >20min）
- 标题长度：参考 YouTube 搜索结果的显示限制（PC ~60字, 手机 ~45字）但不作为分析依据

**✅ 基于数据分布的分析**：
- P-分位数：自动计算，每组样本量相等
- 实际分布：展示直方图、平均值、中位数、标准差
- 同类对比：教程和教程对比，段子和段子对比

**✅ 基于业务含义的分析**：
- 按内容类型分别研究最优特征
- 按目标受众分别分析
- 按发布频道影响力分层

**✅ 保留原始数据**：
- 表格展示：完整的原始数据点，不做聚合
- 图表展示：包括置信区间、样本量标注

#### 检查清单

在发布任何 PatternAnalysis 前，必须确保：

- [ ] 所有分档都能回答"为什么选这个边界？"
  - ✅ 有效答案："YouTube 搜索本身就这样分类"、"这是该话题数据的中位数"
  - ❌ 无效答案："这个数字看起来合理"、"习惯上这样分"

- [ ] 不同话题/内容类型的数据是否混合了？
  - ✅ 分别分析教程、段子、评测
  - ❌ 混合分析所有内容类型

- [ ] 是否展示了样本量和数据分布？
  - ✅ 直方图 + 平均值 + 中位数
  - ❌ 只展示聚合后的"短/中/长"

- [ ] 结论是否是基于数据而非分档名称？
  - ✅ "该话题中，平均时长 12 分钟的视频播放量更高"
  - ❌ "中等时长的视频效果最好"（"中等"是主观的）

### B.7 对文档编写的建议与数据精度要求

#### 文档标注原则

- ✅ 在 `cog.md` 中清晰标注 "直接可得" vs "推断" vs "不可得"
  * 直接可得：阶段1采集（0.5s/video）
  * 高价值可得：阶段2采集（3-5s/video，Top视频）
  * 可选成本：TrendSnapshot, CommentSample（标注配额消耗）
  * 不可得：需要小时级时间戳、ML模型、人工标注的字段

- ✅ 在 `sys.spec.md` 中明确说明每个 API 端点的成本
  * YouTube API 单位消耗数
  * Google Trends 免费
  * 评论采样限制（Top 10%，100条/视频）

- ✅ 在 `api.spec.md` 中定义"最小化查询" vs "完整查询" 的区别
  * 最小化：基础字段，0配额
  * 完整：详细字段，<50配额

- ✅ 所有分析结论必须追溯到原始数据来源（可信度透明化）
  * 标注数据粒度（日期级 vs 周级 vs 月级）
  * 标注样本量（如 "基于 1000 条视频"）
  * 标注时间范围（如 "2024-01 ~ 2026-01"）

- ✅ UI 展示时显示数据采集时间和数据新鲜度

#### 时间数据精度要求

> **关键约束**：明确标注所有模式的时间粒度要求

**时间粒度分类**：

| 粒度 | 支持状态 | 数据来源 | 示例模式 |
|------|--------|--------|--------|
| **秒级** (timestamp with hour) | ❌ 不支持 | 无法收集 | - |
| **小时级** (HOUR) | ❌ 不支持 | 无法收集 | ❌ 模式15（最佳发布小时） |
| **日期级** (DATE, DAYOFWEEK) | ✅ 支持 | yt-dlp published_at | ✅ 模式5（周末效果）、模式13（话题生命周期） |
| **周级** (WEEK) | ✅ 支持 | DATE派生 | ✅ 增长趋势追踪 |
| **月级** (MONTH) | ✅ 支持 | DATE派生 | ✅ 月度报告 |

**模式标注规范**：

在每个 PatternAnalysis 中添加 `time_granularity` 字段

```yaml
Pattern5:
  name: 周末发布效果分析
  time_granularity: "日期级 (DAYOFWEEK)"
  data_source: "published_at 推导"
  confidence: "中"
  note: "样本充足（2340条），排除异常值后可靠"

Pattern13:
  name: 话题生命周期分析
  time_granularity: "日期级 (published_at范围)"
  data_source: "MIN/MAX published_at"
  confidence: "中"
  note: "可分析长期趋势，但不支持高频波动分析"

Pattern15_INVALID:
  name: 最佳发布时段分析（小时级）
  time_granularity: "❌ 小时级（不支持）"
  reason: "yt-dlp 仅提供日期级 published_at，无小时信息"
  alternative: "改为 '最佳发布日期分析'（周末vs工作日）"
```

#### 数据有效性检查清单

在 cog.md/sys.spec.md 中添加以下检查项：

- [ ] 所有 `PatternAnalysis` 都有明确的 `time_granularity` 标注
- [ ] 依赖小时级数据的模式已标记为 "不适用" 或 "需后期增强"
- [ ] 每个分析的样本量和时间范围已记录
- [ ] 阶段1/阶段2采集的字段分别标注
- [ ] 可选成本采集（TrendSnapshot, CommentSample）已清晰声明

---

## 附录 C：UI/UX 设计约束与用户认知边界

> **核心原则**：用户第一秒没看到想要的功能，就会划走不看了。

### C.1 视觉层级原则

#### 首屏内容优先级（从上到下）

```
优先级 1：用户的核心操作入口
         ↓
优先级 2：用户的历史状态（搜索历史、常用筛选）
         ↓
优先级 3：核心功能入口（3个以内）
         ↓
优先级 4：快速发现价值（本周爆款、黑马频道）
         ↓
优先级 5：统计概览（放最底部）
```

**红线规则**：
- ❌ 禁止把"数据概览"/"统计数字"放在首屏顶部
- ❌ 禁止把次要功能（如采集设置）放在显眼位置
- ✅ 搜索框必须是页面第一个可交互元素
- ✅ 用户历史（搜索记录）必须紧跟搜索框

#### 信息密度控制

| 区域 | 最大信息量 | 原因 |
|------|-----------|------|
| 首屏核心区 | 3个功能入口 | 超过3个用户会选择困难 |
| 卡片展示 | 5个卡片 | 一行可视范围 |
| 筛选条件 | 5个维度 | 太多用户看不完 |
| 排序选项 | 6个字段 | 太多难以选择 |

### C.2 渐进式披露原则

#### 默认折叠的内容

以下内容**必须默认折叠**，用户点击后才展开：

| 内容类型 | 折叠原因 | 展开触发 |
|---------|---------|---------|
| 高级筛选条件 | 新用户不需要，占用首屏 | 点击"展开筛选" |
| 详细统计数据 | 不是核心需求 | 点击"查看详情" |
| 使用说明/帮助 | 大部分用户不需要 | 点击"?" 图标 |
| 数据来源说明 | 专业用户才关心 | 鼠标悬停或点击 |

**红线规则**：
- ❌ 禁止默认展开所有筛选条件
- ❌ 禁止在首屏显示超过 2 行的筛选选项
- ✅ 折叠状态要显示当前已选条件数量（如"筛选(3)"）

### C.3 用户认知边界

#### 用户能理解的概念 ✅

| 概念 | 用户理解方式 | 可直接展示 |
|------|-------------|-----------|
| 播放量 | 越多越火 | ✅ 直接显示 |
| 点赞数 | 越多越受欢迎 | ✅ 直接显示 |
| 评论数 | 越多讨论越热 | ✅ 直接显示 |
| 订阅数 | 频道大小 | ✅ 直接显示 |
| 视频时长 | 短/中/长 | ✅ 直接显示 |
| 发布时间 | 新/旧 | ✅ 直接显示 |
| 日均播放 | 每天多少人看 | ✅ 需简单解释 |

#### 用户不理解的概念 ❌

| 概念 | 用户困惑点 | 处理方式 |
|------|-----------|---------|
| 互动率 | "什么是互动率？怎么算的？" | ❌ 不作为筛选条件，仅在详情页展示 |
| 效率分数 | "什么效率？" | ⚠️ 需要解释"平均播放÷订阅数" |
| 中心性 | 完全不懂 | ❌ 不展示给普通用户 |
| P分位数 | 统计术语 | ❌ 转换为"前X%"更易懂 |
| 置信度 | 统计术语 | ❌ 不展示给普通用户 |

**红线规则**：
- ❌ 禁止把"互动率"作为用户可选的筛选条件
- ❌ 禁止使用统计术语作为筛选/排序选项
- ✅ 专业指标只在"详情页"或"高级模式"展示
- ✅ 必须用括号解释计算方式，如"效率分数（平均播放÷订阅）"

### C.4 用户控制权原则

#### 必须让用户自己选择的项目

| 功能 | 用户控制内容 | 禁止的做法 |
|------|-------------|-----------|
| **排序** | 排序字段 + 排序方向 | ❌ 写死"按播放量排序" |
| **时间范围** | 24h/7d/30d/1yr/全部 | ❌ 写死"最近7天" |
| **筛选条件** | 所有筛选都可重置 | ❌ 隐藏某些筛选选项 |
| **显示数量** | 每页显示多少条 | ❌ 写死每页10条 |

#### 排序的完整控制

用户必须能独立选择：
1. **时间范围**：这个时间内发布的视频
2. **排序字段**：按什么指标排
3. **排序方向**：高→低 还是 低→高

**正确示例**：
```
时间范围: [7天内 ▼]    排序字段: [播放量 ▼]    方向: [高→低 ▼]

当前: 7天内发布的视频，按播放量从高到低排序
```

**错误示例**：
```
❌ 排序: [综合排序 ▼]     ← "综合"是什么？
❌ 排序: [热门 ▼]         ← "热门"怎么定义？
❌ 排序: [推荐 ▼]         ← "推荐"的算法是什么？
```

**红线规则**：
- ❌ 禁止使用"综合排序"、"热门"、"推荐"等模糊概念
- ❌ 禁止隐藏排序方向选择
- ✅ 必须用自然语言显示当前排序规则
- ✅ 排序字段名称必须是用户能理解的（见 C.3）

### C.5 平台一致性原则

#### 必须使用 YouTube 原生分类的场景

| 分类维度 | YouTube 原生 | 禁止自定义 |
|---------|-------------|-----------|
| **时长** | <4min / 4-20min / >20min | ❌ 0-5min, 5-10min 等 |
| **时间范围** | 1h/24h/7d/30d/1yr | ❌ 2天, 15天 等 |
| **视频类型** | 视频/短视频/直播 | ❌ 教程/评测/段子 |

**原因**：
- YouTube 搜索 API 只支持这些原生分类
- 自定义分类无法对应采集策略
- 用户在 YouTube 上已习惯这些分类

**红线规则**：
- ❌ 禁止发明 YouTube 不支持的时长分档
- ❌ 禁止发明 YouTube 不支持的时间范围
- ✅ 可以在"内容标签"维度使用自定义分类（基于 tags 启发式）

### C.6 状态持久化原则

#### 必须记住的用户状态

| 状态类型 | 存储位置 | 保留时长 |
|---------|---------|---------|
| 搜索历史 | localStorage | 永久（直到清空） |
| 上次筛选条件 | localStorage | 会话级 |
| 上次排序方式 | localStorage | 永久 |
| 折叠/展开状态 | localStorage | 永久 |
| 每页显示数量 | localStorage | 永久 |

**红线规则**：
- ❌ 禁止每次刷新页面都重置筛选条件
- ❌ 禁止不保存用户的搜索历史
- ✅ 搜索历史必须显示在搜索框附近
- ✅ 提供"清空历史"选项

### C.7 UI 设计检查清单

在设计任何页面布局前，必须确认以下问题：

#### 视觉层级检查
- [ ] 首屏第一个元素是用户的核心操作入口吗？
- [ ] 用户历史（搜索记录等）放在首屏了吗？
- [ ] 统计数据/概览是否放在了页面底部？
- [ ] 功能入口是否控制在 3 个以内？

#### 渐进式披露检查
- [ ] 高级筛选是否默认折叠？
- [ ] 折叠状态是否显示已选条件数量？
- [ ] 详细说明是否隐藏在点击/悬停后？

#### 用户认知检查
- [ ] 所有筛选/排序选项用户都能理解吗？
- [ ] 是否避免了专业术语（互动率、中心性、P分位数）？
- [ ] 复杂指标是否有括号解释？

#### 用户控制检查
- [ ] 排序是否拆分为：时间范围 + 字段 + 方向？
- [ ] 是否避免了"综合排序"等模糊概念？
- [ ] 是否用自然语言显示当前规则？

#### 平台一致性检查
- [ ] 时长分类是否使用 YouTube 原生（<4min/4-20min/>20min）？
- [ ] 时间范围是否使用 YouTube 原生（1h/24h/7d/30d/1yr）？

#### 状态持久化检查
- [ ] 搜索历史是否保存？
- [ ] 筛选条件是否保存？
- [ ] 是否提供清空选项？

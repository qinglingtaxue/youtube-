#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€è¿½è¸ªç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ç»“åˆé•¿æœŸæ¨¡å¼åˆ†æå’Œå®æ—¶åŠ¨æ€ç›‘æ§
"""

import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from utils.logger import setup_logger
from utils.config import get_config
from utils.file_utils import ensure_dir
from monitoring.dynamic_tracker import DynamicTracker
from analysis.pattern_analyzer import PatternAnalyzer

def combined_analysis_example():
    """
    ç»¼åˆåˆ†æç¤ºä¾‹
    å±•ç¤ºå¦‚ä½•ç»“åˆé•¿æœŸæ¨¡å¼åˆ†æå’ŒçŸ­æœŸåŠ¨æ€è¿½è¸ª
    """
    logger = setup_logger('combined_analysis')
    logger.info("=" * 60)
    logger.info("é•¿æœŸæ¨¡å¼ + åŠ¨æ€è¿½è¸ª - ç»¼åˆåˆ†æç¤ºä¾‹")
    logger.info("=" * 60)

    config = get_config()

    # æ­¥éª¤1ï¼šé•¿æœŸæ¨¡å¼åˆ†æ
    logger.info("\nğŸ“Š æ­¥éª¤1ï¼šé•¿æœŸæ¨¡å¼åˆ†æ")
    logger.info("åˆ†æè¿‡å»ä¸€å‘¨çš„ç¨³å®šæ¨¡å¼å’Œè¶‹åŠ¿...")

    analyzer = PatternAnalyzer(config)

    # æ¨¡æ‹Ÿé•¿æœŸæ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶åº”ä»æ•°æ®åº“åŠ è½½ï¼‰
    long_term_data = generate_mock_long_term_data()
    long_term_patterns = analyzer.analyze_videos(long_term_data)

    logger.info(f"å‘ç° {len(long_term_patterns)} ä¸ªé•¿æœŸç¨³å®šæ¨¡å¼")
    for i, pattern in enumerate(long_term_patterns[:5], 1):
        logger.info(f"  {i}. {pattern['name']} (é¢‘ç‡: {pattern['frequency']})")

    # æ­¥éª¤2ï¼šåŠ¨æ€è¶‹åŠ¿è¿½è¸ª
    logger.info("\nğŸ”¥ æ­¥éª¤2ï¼šåŠ¨æ€è¶‹åŠ¿è¿½è¸ª")
    logger.info("è¿½è¸ªä»Šæ—¥çƒ­ç‚¹å’Œæ–°å…´è¯é¢˜...")

    tracker = DynamicTracker(config)

    # æ¨¡æ‹ŸåŠ¨æ€æ•°æ®
    dynamic_trends = generate_mock_dynamic_trends()
    emerging_topics = dynamic_trends.get('emerging_topics', [])
    viral_videos = dynamic_trends.get('viral_videos', [])

    logger.info(f"å‘ç° {len(emerging_topics)} ä¸ªæ–°å…´è¯é¢˜")
    for topic in emerging_topics:
        logger.info(f"  ğŸš€ {topic['keyword']}: å¢é•¿ {topic['growth_rate']:.1%}")

    logger.info(f"å‘ç° {len(viral_videos)} ä¸ªç—…æ¯’è§†é¢‘")
    for video in viral_videos[:3]:
        logger.info(f"  ğŸ“¹ {video['title'][:40]}... (é€Ÿç‡: {video['velocity']:.0f}/h)")

    # æ­¥éª¤3ï¼šæ¨¡å¼ä¸åŠ¨æ€ç»“åˆåˆ†æ
    logger.info("\nğŸ”„ æ­¥éª¤3ï¼šæ¨¡å¼ä¸åŠ¨æ€ç»“åˆåˆ†æ")

    analysis_result = {
        'timestamp': datetime.now().isoformat(),
        'long_term_patterns': long_term_patterns,
        'dynamic_trends': dynamic_trends,
        'insights': []
    }

    # åˆ†æç¨³å®šæ¨¡å¼çš„å½“å‰è¡¨ç°
    stable_performance = []
    for pattern in long_term_patterns:
        # æ£€æŸ¥è¯¥æ¨¡å¼åœ¨åŠ¨æ€æ•°æ®ä¸­çš„è¡¨ç°
        performance = check_pattern_performance(pattern, dynamic_trends)
        stable_performance.append(performance)

    analysis_result['stable_performance'] = stable_performance

    # è¯†åˆ«æ–°å…´æœºä¼š
    emerging_opportunities = identify_emerging_opportunities(
        long_term_patterns,
        dynamic_trends
    )
    analysis_result['emerging_opportunities'] = emerging_opportunities

    logger.info("\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
    for insight in analysis_result['insights']:
        logger.info(f"  - {insight}")

    # æ­¥éª¤4ï¼šç”Ÿæˆç»¼åˆå»ºè®®
    logger.info("\nğŸ“ æ­¥éª¤4ï¼šç”Ÿæˆç»¼åˆå»ºè®®")

    recommendations = generate_combined_recommendations(
        long_term_patterns,
        dynamic_trends,
        emerging_opportunities
    )

    analysis_result['recommendations'] = recommendations

    logger.info("\nğŸ¯ è¡ŒåŠ¨å»ºè®®:")
    for i, rec in enumerate(recommendations, 1):
        logger.info(f"  {i}. {rec}")

    # ä¿å­˜åˆ†æç»“æœ
    output_dir = Path('output/combined_analysis')
    ensure_dir(output_dir)

    import json
    result_file = output_dir / f"combined_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)

    logger.info(f"\nâœ… ç»¼åˆåˆ†æç»“æœå·²ä¿å­˜: {result_file}")

    # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    report = generate_combined_report(analysis_result)
    report_file = output_dir / f"combined_report_{datetime.now().strftime('%Y%m%d')}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    logger.info(f"ğŸ“Š å¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def generate_mock_long_term_data():
    """ç”Ÿæˆæ¨¡æ‹Ÿé•¿æœŸæ•°æ®"""
    return [
        {
            'id': 'video1',
            'title': 'Pythonå…¥é—¨æ•™ç¨‹ï¼šå˜é‡å’Œæ•°æ®ç±»å‹',
            'view_count': 50000,
            'tags': ['Python', 'ç¼–ç¨‹', 'æ•™ç¨‹', 'å…¥é—¨'],
            'published_at': '2024-01-01'
        },
        {
            'id': 'video2',
            'title': 'JavaScriptåŸºç¡€æ•™ç¨‹ï¼šä»é›¶å¼€å§‹å­¦ä¹ JS',
            'view_count': 45000,
            'tags': ['JavaScript', 'å‰ç«¯', 'ç¼–ç¨‹', 'æ•™ç¨‹'],
            'published_at': '2024-01-05'
        },
        {
            'id': 'video3',
            'title': 'æ•°æ®ç§‘å­¦å…¥é—¨ï¼šä½¿ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æ',
            'view_count': 38000,
            'tags': ['æ•°æ®ç§‘å­¦', 'Python', 'æ•°æ®åˆ†æ', 'æ•™ç¨‹'],
            'published_at': '2024-01-10'
        }
    ]

def generate_mock_dynamic_trends():
    """ç”Ÿæˆæ¨¡æ‹ŸåŠ¨æ€è¶‹åŠ¿æ•°æ®"""
    return {
        'timestamp': datetime.now().isoformat(),
        'emerging_topics': [
            {'keyword': 'AIç»˜ç”»', 'growth_rate': 5.2},
            {'keyword': 'ChatGPTåº”ç”¨', 'growth_rate': 3.8},
            {'keyword': 'çŸ­è§†é¢‘å‰ªè¾‘', 'growth_rate': 2.5}
        ],
        'viral_videos': [
            {
                'title': 'AIç»˜ç”»å·¥å…·ä½¿ç”¨æ•™ç¨‹',
                'velocity': 8500,
                'tags': ['AI', 'ç»˜ç”»', 'å·¥å…·']
            },
            {
                'title': 'ChatGPTå†™ä»£ç å…¨æ”»ç•¥',
                'velocity': 6200,
                'tags': ['ChatGPT', 'ç¼–ç¨‹', 'AI']
            }
        ],
        'declining_topics': [
            {'keyword': 'ä¼ ç»Ÿç½‘é¡µè®¾è®¡', 'growth_rate': 0.3}
        ]
    }

def check_pattern_performance(pattern, dynamic_trends):
    """æ£€æŸ¥æ¨¡å¼åœ¨åŠ¨æ€æ•°æ®ä¸­çš„è¡¨ç°"""
    # ç¤ºä¾‹é€»è¾‘
    return {
        'pattern_name': pattern['name'],
        'is_trending': pattern['name'] in ['æ•™ç¨‹', 'AI'],
        'current_velocity': 'high' if pattern['name'] == 'AI' else 'normal',
        'recommendation': 'ç»§ç»­æ·±è€•' if pattern['name'] == 'æ•™ç¨‹' else 'é€‚åº¦è·Ÿè¿›'
    }

def identify_emerging_opportunities(long_term_patterns, dynamic_trends):
    """è¯†åˆ«æ–°å…´æœºä¼š"""
    opportunities = []

    # åˆ†ææ–°å…´è¯é¢˜ä¸é•¿æœŸæ¨¡å¼çš„ç»“åˆç‚¹
    for topic in dynamic_trends.get('emerging_topics', []):
        if topic['keyword'] == 'AIç»˜ç”»':
            opportunities.append({
                'topic': 'AIç»˜ç”»',
                'opportunity': 'å°†AIç»˜ç”»ä¸ç¼–ç¨‹æ•™ç¨‹ç»“åˆ',
                'potential': 'high',
                'action': 'åˆ¶ä½œ"AIç»˜ç”»å·¥å…·å¼€å‘æ•™ç¨‹"'
            })
        elif topic['keyword'] == 'ChatGPTåº”ç”¨':
            opportunities.append({
                'topic': 'ChatGPTåº”ç”¨',
                'opportunity': 'ç”¨ChatGPTè¾…åŠ©ç¼–ç¨‹æ•™å­¦',
                'potential': 'high',
                'action': 'å¼€å‘"ChatGPTç¼–ç¨‹åŠ©æ‰‹"ç³»åˆ—'
            })

    return opportunities

def generate_combined_recommendations(long_term_patterns, dynamic_trends, opportunities):
    """ç”Ÿæˆç»¼åˆå»ºè®®"""
    recommendations = [
        "ä¿æŒæŠ€æœ¯æ•™ç¨‹çš„æ ¸å¿ƒåœ°ä½ï¼ˆé•¿æœŸç¨³å®šéœ€æ±‚ï¼‰",
        "å¿«é€Ÿè·Ÿè¿›AIç›¸å…³è¯é¢˜ï¼ˆçŸ­æœŸçƒ­ç‚¹æœºä¼šï¼‰",
        "ç»“åˆé•¿æœŸæ¨¡å¼å’Œæ–°å…´è¶‹åŠ¿åˆ›ä½œå†…å®¹",
        "å‡†å¤‡2-3ä¸ªAI+ç¼–ç¨‹çš„ç»„åˆä¸»é¢˜",
        "ç›‘æ§ç«å“çš„AIå†…å®¹ç­–ç•¥"
    ]

    # æ·»åŠ åŸºäºæœºä¼šçš„å»ºè®®
    for opp in opportunities:
        recommendations.append(f"å¼€å‘{opp['topic']}ç›¸å…³è¯¾ç¨‹ï¼š{opp['action']}")

    return recommendations

def generate_combined_report(analysis_result):
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    lines = [
        "# é•¿æœŸæ¨¡å¼ + åŠ¨æ€è¿½è¸ª - ç»¼åˆåˆ†ææŠ¥å‘Š",
        "",
        f"**ç”Ÿæˆæ—¶é—´**: {analysis_result['timestamp'][:19]}",
        "",
        "## ğŸ“Š é•¿æœŸç¨³å®šæ¨¡å¼",
        ""
    ]

    for pattern in analysis_result['long_term_patterns']:
        lines.extend([
            f"### {pattern['name']}",
            f"- **é¢‘ç‡**: {pattern['frequency']} æ¬¡",
            f"- **æè¿°**: {pattern.get('description', 'N/A')}",
            ""
        ])

    lines.extend([
        "## ğŸ”¥ å½“å‰åŠ¨æ€è¶‹åŠ¿",
        "",
        "### æ–°å…´è¯é¢˜"
    ])

    for topic in analysis_result['dynamic_trends'].get('emerging_topics', []):
        lines.append(f"- **{topic['keyword']}**: å¢é•¿ {topic['growth_rate']:.1%}")

    lines.extend([
        "",
        "### ç—…æ¯’è§†é¢‘"
    ])

    for video in analysis_result['dynamic_trends'].get('viral_videos', []):
        lines.append(f"- {video['title']}")
        lines.append(f"  å¢é•¿é€Ÿç‡: {video['velocity']:.0f} è§‚çœ‹/å°æ—¶")

    lines.extend([
        "",
        "## ğŸ’¡ æ–°å…´æœºä¼š",
        ""
    ])

    for opp in analysis_result.get('emerging_opportunities', []):
        lines.extend([
            f"### {opp['topic']}",
            f"- **æœºä¼š**: {opp['opportunity']}",
            f"- **æ½œåŠ›**: {opp['potential']}",
            f"- **è¡ŒåŠ¨**: {opp['action']}",
            ""
        ])

    lines.extend([
        "## ğŸ¯ ç»¼åˆå»ºè®®",
        ""
    ])

    for i, rec in enumerate(analysis_result.get('recommendations', []), 1):
        lines.append(f"{i}. {rec}")

    lines.extend([
        "",
        "## ğŸ“ˆ å®æ–½è®¡åˆ’",
        "",
        "### æœ¬å‘¨è¡ŒåŠ¨",
        "- åˆ¶ä½œ1ä¸ªAI+ç¼–ç¨‹æ•™ç¨‹",
        "- è¿½è¸ªæ–°å…´è¯é¢˜å‘å±•",
        "- åˆ†æç«å“åŠ¨æ€",
        "",
        "### ä¸‹å‘¨è®¡åˆ’",
        "- åŸºäºæ•°æ®è°ƒæ•´å†…å®¹ç­–ç•¥",
        "- å¼€å‘æ–°çš„å†…å®¹æ¨¡æ¿",
        "- æ›´æ–°é•¿æœŸæ¨¡å¼åº“",
        ""
    ])

    return '\n'.join(lines)


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("é•¿æœŸæ¨¡å¼ + åŠ¨æ€è¿½è¸ª - ç»¼åˆåˆ†æ")
    print("=" * 60)
    print("\næœ¬ç¤ºä¾‹å°†å±•ç¤º:")
    print("1. é•¿æœŸç¨³å®šæ¨¡å¼åˆ†æ")
    print("2. çŸ­æœŸåŠ¨æ€è¶‹åŠ¿è¿½è¸ª")
    print("3. æ¨¡å¼ä¸åŠ¨æ€çš„ç»“åˆåˆ†æ")
    print("4. ç”Ÿæˆç»¼åˆè¡ŒåŠ¨å»ºè®®")
    print("\nå¼€å§‹æ‰§è¡Œ...\n")

    try:
        combined_analysis_example()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\n\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

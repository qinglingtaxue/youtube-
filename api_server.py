#!/usr/bin/env python3
"""
YouTube å†…å®¹åŠ©æ‰‹ - API æœåŠ¡ (ç®€åŒ–ç‰ˆ)
ç›´æ¥è°ƒç”¨ yt-dlp å‘½ä»¤è¡Œï¼Œä¸ä¾èµ–æœ‰é—®é¢˜çš„æ¨¡å—
"""

import asyncio
import json
import os
import subprocess
import sys
import ssl
import urllib.request
import urllib.parse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, List, Dict, Any, Set
from contextlib import asynccontextmanager
from collections import Counter

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Query, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field

# å¯¼å…¥æ•°æ®åº“æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent / 'src'))
from utils.database import get_database


# ============ Pydantic æ¨¡å‹ ============

class ResearchRequest(BaseModel):
    """ç ”ç©¶è¯·æ±‚å‚æ•°"""
    topic: str = Field(..., description="ç ”ç©¶ä¸»é¢˜/å…³é”®è¯")
    mode: str = Field(default="standard", description="åˆ†ææ¨¡å¼: quick/standard/deep")
    max_results: int = Field(default=500, description="æœ€å¤§é‡‡é›†æ•°é‡")

    # ç­›é€‰æ¡ä»¶
    views_min: Optional[int] = Field(default=None, description="æœ€å°æ’­æ”¾é‡")
    views_max: Optional[int] = Field(default=None, description="æœ€å¤§æ’­æ”¾é‡")
    duration_filter: Optional[List[str]] = Field(default=None, description="æ—¶é•¿ç­›é€‰: short/medium/long")
    time_range: Optional[str] = Field(default=None, description="å‘å¸ƒæ—¶é—´: 24h/7d/30d/90d/1y")
    regions: Optional[List[str]] = Field(default=None, description="ç›®æ ‡åœ°åŒº")
    subscribers_filter: Optional[List[str]] = Field(default=None, description="é¢‘é“ç²‰ä¸æ•°: small/medium/large/huge")
    ai_filter: Optional[str] = Field(default=None, description="AIè§†é¢‘ç­›é€‰: ai/human")
    sort_by: str = Field(default="views", description="æ’åºæ–¹å¼")


class MonitorTaskCreate(BaseModel):
    """åˆ›å»ºç›‘æ§ä»»åŠ¡"""
    keyword: str = Field(..., description="ç›‘æ§å…³é”®è¯")
    description: Optional[str] = Field(default=None, description="ä»»åŠ¡æè¿°")
    frequency: str = Field(default="daily", description="é‡‡é›†é¢‘ç‡: hourly/daily/weekly")
    max_results: int = Field(default=500, description="æ¯æ¬¡é‡‡é›†æœ€å¤§æ•°é‡")


class MonitorTaskUpdate(BaseModel):
    """æ›´æ–°ç›‘æ§ä»»åŠ¡"""
    description: Optional[str] = None
    frequency: Optional[str] = None
    max_results: Optional[int] = None
    is_active: Optional[bool] = None


# ============ ä»»åŠ¡ç»“æœç¼“å­˜ ============

class TaskResultCache:
    """ä»»åŠ¡ç»“æœç¼“å­˜ - é˜²æ­¢ WebSocket æ–­å¼€åä¸¢å¤±ç»“æœ"""

    def __init__(self, max_age_seconds: int = 3600):
        """
        Args:
            max_age_seconds: ç»“æœæœ€å¤§ä¿å­˜æ—¶é—´ï¼ˆé»˜è®¤1å°æ—¶ï¼‰
        """
        self.results: Dict[str, Dict] = {}
        self.max_age = timedelta(seconds=max_age_seconds)

    def save(self, task_id: str, result: Dict, status: str = "complete"):
        """ä¿å­˜ä»»åŠ¡ç»“æœ"""
        self.results[task_id] = {
            "result": result,
            "status": status,
            "timestamp": datetime.now(),
            "progress": 100 if status == "complete" else 0,
            "message": "ä»»åŠ¡å®Œæˆ" if status == "complete" else ""
        }
        print(f"ğŸ’¾ ä»»åŠ¡ç»“æœå·²ç¼“å­˜: {task_id} (çŠ¶æ€: {status})")

    def update_progress(self, task_id: str, progress: int, message: str):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        if task_id not in self.results:
            self.results[task_id] = {
                "result": None,
                "status": "running",
                "timestamp": datetime.now(),
                "progress": progress,
                "message": message
            }
        else:
            self.results[task_id]["progress"] = progress
            self.results[task_id]["message"] = message
            self.results[task_id]["timestamp"] = datetime.now()

    def get(self, task_id: str) -> Optional[Dict]:
        """è·å–ä»»åŠ¡ç»“æœ"""
        if task_id not in self.results:
            return None

        entry = self.results[task_id]
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if datetime.now() - entry["timestamp"] > self.max_age:
            del self.results[task_id]
            return None

        return entry

    def cleanup(self):
        """æ¸…ç†è¿‡æœŸç»“æœ"""
        now = datetime.now()
        expired = [
            task_id for task_id, entry in self.results.items()
            if now - entry["timestamp"] > self.max_age
        ]
        for task_id in expired:
            del self.results[task_id]
        if expired:
            print(f"ğŸ§¹ å·²æ¸…ç† {len(expired)} ä¸ªè¿‡æœŸä»»åŠ¡ç»“æœ")

# å…¨å±€ä»»åŠ¡ç¼“å­˜
task_cache = TaskResultCache()


# ============ AI è§†é¢‘æ£€æµ‹ ============

AI_VIDEO_KEYWORDS = [
    'sora', 'runway', 'pika', 'heygen', 'synthesia', 'd-id', 'midjourney',
    'stable diffusion', 'kling', 'luma', 'gen-2', 'gen-3', 'vidu', 'cogvideo',
    'aiç”Ÿæˆ', 'aiè§†é¢‘', 'aiåˆ¶ä½œ', 'aiåˆ›ä½œ', 'aié…éŸ³', 'aiæ¢è„¸', 'aiæ•°å­—äºº',
    'æ•°å­—äºº', 'è™šæ‹Ÿäºº', 'äººå·¥æ™ºèƒ½ç”Ÿæˆ', 'aigc',
    'ai generated', 'ai video', 'ai-generated', 'text to video', 'ai avatar'
]

def detect_ai_video(title: str, description: str = "", tags: List[str] = None) -> tuple:
    """æ£€æµ‹æ˜¯å¦ä¸ºAIè§†é¢‘"""
    text = f"{title or ''} {description or ''} {' '.join(tags or [])}".lower()
    for kw in AI_VIDEO_KEYWORDS:
        if kw.lower() in text:
            return True, kw
    return False, None


def get_duration_category(seconds: int) -> str:
    """è·å–æ—¶é•¿åˆ†ç±»"""
    if seconds < 300:
        return "short"
    elif seconds < 1200:
        return "medium"
    else:
        return "long"


# ============ yt-dlp ç›´æ¥è°ƒç”¨ ============

def search_videos_ytdlp(keyword: str, max_results: int = 50, time_range: str = None) -> List[Dict]:
    """ä½¿ç”¨ yt-dlp æœç´¢è§†é¢‘

    time_range: YouTube æ—¶é—´è¿‡æ»¤
      - 24h: æœ€è¿‘24å°æ—¶
      - 7d: æœ€è¿‘7å¤©
      - 30d: æœ€è¿‘30å¤©
      - 1y: æœ€è¿‘1å¹´
    """
    try:
        # æ„å»ºæœç´¢ URL
        # YouTube æœç´¢æ—¶é—´è¿‡æ»¤éœ€è¦ä½¿ç”¨ç‰¹å®šçš„ URL å‚æ•°
        search_url = f'ytsearch{max_results}:{keyword}'

        # å¦‚æœæŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œä½¿ç”¨ YouTube æœç´¢ URL æ ¼å¼
        if time_range:
            # YouTube sp å‚æ•°ç¼–ç  (base64):
            # æœ€è¿‘1å°æ—¶: EgIIAQ, ä»Šå¤©: EgIIAg, æœ¬å‘¨: EgIIAw, æœ¬æœˆ: EgIIBA, ä»Šå¹´: EgIIBQ
            # æ³¨æ„: YouTube æ²¡æœ‰åŸç”Ÿçš„"3ä¸ªæœˆ"è¿‡æ»¤ï¼Œ90d ä½¿ç”¨æœ¬æœˆè¿‡æ»¤ + åè¿‡æ»¤å®ç°
            sp_codes = {
                '24h': 'EgIIAg',   # ä»Šå¤©ï¼ˆ24å°æ—¶å†…ï¼‰
                '7d': 'EgIIAw',    # æœ¬å‘¨ï¼ˆ7å¤©å†…ï¼‰
                '30d': 'EgIIBA',   # æœ¬æœˆï¼ˆ30å¤©å†…ï¼‰
                '90d': 'EgIIBA',   # 3ä¸ªæœˆï¼ˆä½¿ç”¨æœ¬æœˆè¿‡æ»¤ï¼Œåç»­ä»£ç ä¼šè¡¥å……è¿‡æ»¤ï¼‰
                '1y': 'EgIIBQ',    # ä»Šå¹´
            }
            sp = sp_codes.get(time_range)
            if sp:
                # ä½¿ç”¨ YouTube æœç´¢ URL è€Œä¸æ˜¯ ytsearch
                from urllib.parse import quote
                encoded_keyword = quote(keyword)
                search_url = f'https://www.youtube.com/results?search_query={encoded_keyword}&sp={sp}'
                print(f"ä½¿ç”¨æ—¶é—´è¿‡æ»¤æœç´¢: {time_range} -> sp={sp}")

        cmd = [
            'yt-dlp',
            '--dump-json',
            '--flat-playlist',
            '--no-warnings',
            '--playlist-end', str(max_results),
            search_url
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=180  # å¢åŠ è¶…æ—¶æ—¶é—´
        )

        videos = []
        for line in result.stdout.strip().split('\n'):
            if line:
                try:
                    data = json.loads(line)
                    videos.append(data)
                except json.JSONDecodeError:
                    continue

        print(f"æœç´¢å®Œæˆ: æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
        return videos
    except subprocess.TimeoutExpired:
        print("yt-dlp æœç´¢è¶…æ—¶")
        return []
    except FileNotFoundError:
        print("yt-dlp æœªå®‰è£…")
        return []
    except Exception as e:
        print(f"æœç´¢é”™è¯¯: {e}")
        return []


def generate_search_variations(keyword: str) -> List[str]:
    """ç”Ÿæˆå…³é”®è¯å˜ä½“ï¼Œç”¨äºæ‰©å¤§æœç´¢èŒƒå›´ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰

    ç­–ç•¥ï¼š
    1. åŸå§‹å…³é”®è¯
    2. æ·»åŠ å¹´ä»½ï¼ˆ2024, 2025ï¼‰
    3. æ·»åŠ å¸¸ç”¨ä¿®é¥°è¯ï¼ˆæ•™ç¨‹, å…¥é—¨, è¿›é˜¶, å®æˆ˜, æŠ€å·§ï¼‰
    4. æ·»åŠ è¯­è¨€/åœ°åŒºå˜ä½“
    """
    variations = [keyword]

    # å¹´ä»½å˜ä½“
    current_year = datetime.now().year
    variations.extend([
        f"{keyword} {current_year}",
        f"{keyword} {current_year - 1}",
    ])

    # å†…å®¹ç±»å‹å˜ä½“
    content_modifiers = ['æ•™ç¨‹', 'å…¥é—¨', 'å®æˆ˜', 'æŠ€å·§', 'å®Œæ•´', 'æœ€æ–°']
    for mod in content_modifiers:
        if mod not in keyword:
            variations.append(f"{keyword} {mod}")

    # è‹±æ–‡å…³é”®è¯æ·»åŠ è‹±æ–‡ä¿®é¥°è¯
    if any(c.isascii() and c.isalpha() for c in keyword):
        eng_modifiers = ['tutorial', 'guide', 'beginners', 'advanced', 'tips']
        for mod in eng_modifiers:
            if mod not in keyword.lower():
                variations.append(f"{keyword} {mod}")

    return variations[:10]  # æœ€å¤šè¿”å›10ä¸ªå˜ä½“


# ============ é«˜çº§å…³é”®è¯é‡‡é›† ============

def get_search_suggestions(query: str, max_suggestions: int = 10, language: str = 'zh') -> List[str]:
    """è·å– YouTube æœç´¢å»ºè®®ï¼ˆé•¿å°¾è¯ï¼‰

    ä½¿ç”¨ YouTube çš„æœç´¢è‡ªåŠ¨è¡¥å…¨ API è·å–ç”¨æˆ·çœŸå®æœç´¢çš„å…³é”®è¯

    Args:
        query: åŸºç¡€æœç´¢è¯
        max_suggestions: æœ€å¤§å»ºè®®æ•°
        language: è¯­è¨€ä»£ç 

    Returns:
        æœç´¢å»ºè®®åˆ—è¡¨
    """
    print(f"ğŸ” è·å–æœç´¢å»ºè®®: {query}")

    # YouTube æœç´¢å»ºè®® API
    base_url = "https://suggestqueries-clients6.youtube.com/complete/search"
    params = {
        'client': 'youtube',
        'hl': language,
        'gl': 'US',
        'gs_ri': 'youtube',
        'ds': 'yt',
        'q': query
    }

    url = f"{base_url}?{urllib.parse.urlencode(params)}"

    try:
        req = urllib.request.Request(url, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

        # åˆ›å»º SSL ä¸Šä¸‹æ–‡ï¼ˆå¤„ç†è¯ä¹¦é—®é¢˜ï¼‰
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE

        with urllib.request.urlopen(req, timeout=10, context=ctx) as response:
            # å“åº”æ˜¯ JSONP æ ¼å¼: window.google.ac.h(...)
            content = response.read().decode('utf-8')

            # æå– JSON éƒ¨åˆ†
            start = content.find('(')
            end = content.rfind(')')
            if start != -1 and end != -1:
                json_str = content[start + 1:end]
                data = json.loads(json_str)

                # è§£æå»ºè®®åˆ—è¡¨
                suggestions = []
                if len(data) > 1 and isinstance(data[1], list):
                    for item in data[1]:
                        if isinstance(item, list) and len(item) > 0:
                            suggestion = item[0]
                            if suggestion != query:
                                suggestions.append(suggestion)

                print(f"   âœ“ è·å–åˆ° {len(suggestions)} ä¸ªæœç´¢å»ºè®®")
                return suggestions[:max_suggestions]

    except Exception as e:
        print(f"   âš  è·å–æœç´¢å»ºè®®å¤±è´¥: {e}")

    return []


def extract_tags_from_channel(channel_id: str, max_videos: int = 10, top_n: int = 15) -> List[str]:
    """ä»é¢‘é“è§†é¢‘ä¸­æå–é«˜é¢‘æ ‡ç­¾

    åˆ†æç«å“é¢‘é“çš„è§†é¢‘æ ‡ç­¾ï¼Œæ‰¾å‡ºä»–ä»¬å¸¸ç”¨çš„å…³é”®è¯

    Args:
        channel_id: YouTube é¢‘é“ ID
        max_videos: åˆ†æçš„è§†é¢‘æ•°é‡
        top_n: è¿”å› top N ä¸ªé«˜é¢‘æ ‡ç­¾

    Returns:
        é«˜é¢‘æ ‡ç­¾åˆ—è¡¨ï¼ˆæŒ‰é¢‘ç‡æ’åºï¼‰
    """
    print(f"ğŸ·ï¸  ä»é¢‘é“ {channel_id} æå–æ ‡ç­¾")

    try:
        # 1. è·å–é¢‘é“è§†é¢‘åˆ—è¡¨
        url = f"https://www.youtube.com/channel/{channel_id}/videos"
        cmd = [
            'yt-dlp',
            '--flat-playlist',
            '--dump-single-json',
            '--playlist-end', str(max_videos),
            '--no-warnings',
            url
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        if result.returncode != 0:
            print(f"   âš  è·å–é¢‘é“è§†é¢‘åˆ—è¡¨å¤±è´¥")
            return []

        data = json.loads(result.stdout)
        entries = data.get('entries', [])

        if not entries:
            print(f"   âš  é¢‘é“æ²¡æœ‰è§†é¢‘")
            return []

        # 2. æ”¶é›†æ‰€æœ‰æ ‡ç­¾
        all_tags: List[str] = []

        for i, video in enumerate(entries[:max_videos]):
            video_id = video.get('id')
            if not video_id:
                continue

            try:
                # è·å–è§†é¢‘è¯¦æƒ…ä»¥è·å–æ ‡ç­¾
                detail_cmd = [
                    'yt-dlp',
                    '--dump-json',
                    '--no-download',
                    '--no-warnings',
                    f'https://www.youtube.com/watch?v={video_id}'
                ]

                detail_result = subprocess.run(
                    detail_cmd,
                    capture_output=True,
                    text=True,
                    timeout=15
                )

                if detail_result.returncode == 0:
                    detail = json.loads(detail_result.stdout)
                    tags = detail.get('tags', [])
                    if tags:
                        all_tags.extend(tags)

            except Exception as e:
                continue

        # 3. ç»Ÿè®¡æ ‡ç­¾é¢‘ç‡
        tag_counts = Counter(all_tags)

        # è¿”å›é«˜é¢‘æ ‡ç­¾
        top_tags = [tag for tag, _ in tag_counts.most_common(top_n)]

        print(f"   âœ“ ä» {len(entries)} ä¸ªè§†é¢‘ä¸­æå–äº† {len(top_tags)} ä¸ªé«˜é¢‘æ ‡ç­¾")
        return top_tags

    except Exception as e:
        print(f"   âš  æå–é¢‘é“æ ‡ç­¾å¤±è´¥: {e}")
        return []


async def get_smart_search_keywords(
    keyword: str,
    competitor_channels: List[str] = None,
    max_keywords: int = 15
) -> List[str]:
    """æ™ºèƒ½è·å–æœç´¢å…³é”®è¯

    ç»¼åˆå¤šä¸ªæ•°æ®æºè·å–çœŸå®çš„å…³é”®è¯ï¼š
    1. åŸå§‹å…³é”®è¯
    2. YouTube æœç´¢å»ºè®®ï¼ˆé•¿å°¾è¯ï¼‰
    3. ç«å“é¢‘é“è§†é¢‘æ ‡ç­¾
    4. åŸºç¡€å˜ä½“ï¼ˆå¹´ä»½ã€ä¿®é¥°è¯ï¼‰

    Args:
        keyword: ä¸»å…³é”®è¯
        competitor_channels: ç«å“é¢‘é“ ID åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        max_keywords: æœ€å¤§å…³é”®è¯æ•°

    Returns:
        ç»¼åˆå…³é”®è¯åˆ—è¡¨ï¼ˆå·²å»é‡ï¼‰
    """
    print(f"\nğŸ§  æ™ºèƒ½å…³é”®è¯é‡‡é›†: {keyword}")

    all_keywords: Set[str] = set()
    all_keywords.add(keyword)

    loop = asyncio.get_event_loop()

    # 1. è·å– YouTube æœç´¢å»ºè®®
    suggestions = await loop.run_in_executor(
        None,
        lambda: get_search_suggestions(keyword, max_suggestions=10)
    )
    all_keywords.update(suggestions)
    print(f"   æœç´¢å»ºè®®: +{len(suggestions)} ä¸ªå…³é”®è¯")

    # 2. ä»ç«å“é¢‘é“æå–æ ‡ç­¾ï¼ˆå¦‚æœæä¾›äº†é¢‘é“åˆ—è¡¨ï¼‰
    if competitor_channels:
        for channel_id in competitor_channels[:3]:  # æœ€å¤šåˆ†æ3ä¸ªé¢‘é“
            tags = await loop.run_in_executor(
                None,
                lambda cid=channel_id: extract_tags_from_channel(cid, max_videos=5, top_n=10)
            )
            # åªä¿ç•™ä¸ä¸»é¢˜ç›¸å…³çš„æ ‡ç­¾
            relevant_tags = [t for t in tags if is_tag_relevant(t, keyword)]
            all_keywords.update(relevant_tags)
            print(f"   é¢‘é“æ ‡ç­¾ {channel_id[:8]}...: +{len(relevant_tags)} ä¸ªç›¸å…³æ ‡ç­¾")
            await asyncio.sleep(0.5)  # é¿å…é™æµ

    # 3. è¡¥å……åŸºç¡€å˜ä½“ï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿçš„å…³é”®è¯ï¼‰
    if len(all_keywords) < max_keywords:
        basic_variations = generate_search_variations(keyword)
        for v in basic_variations:
            if len(all_keywords) >= max_keywords:
                break
            all_keywords.add(v)

    keyword_list = list(all_keywords)
    print(f"   âœ“ å…±æ”¶é›†åˆ° {len(keyword_list)} ä¸ªå…³é”®è¯")

    return keyword_list[:max_keywords]


def is_tag_relevant(tag: str, keyword: str) -> bool:
    """æ£€æŸ¥æ ‡ç­¾æ˜¯å¦ä¸å…³é”®è¯ç›¸å…³

    ç®€å•çš„ç›¸å…³æ€§æ£€æŸ¥ï¼Œè¿‡æ»¤æ‰æ— å…³æ ‡ç­¾
    """
    tag_lower = tag.lower()
    keyword_lower = keyword.lower()

    # å®Œå…¨åŒ…å«
    if keyword_lower in tag_lower or tag_lower in keyword_lower:
        return True

    # å…³é”®è¯çš„ä»»ä½•è¯å‡ºç°åœ¨æ ‡ç­¾ä¸­
    keyword_words = keyword_lower.split()
    for word in keyword_words:
        if len(word) > 2 and word in tag_lower:
            return True

    # æ ‡ç­¾é•¿åº¦è¿‡é•¿å¯èƒ½æ˜¯æ— å…³çš„
    if len(tag) > 50:
        return False

    # æ ‡ç­¾æ˜¯å¸¸è§çš„ç¼–ç¨‹/æŠ€æœ¯ç›¸å…³è¯æ±‡
    tech_keywords = ['python', 'java', 'javascript', 'coding', 'programming', 'tutorial',
                     'æ•™ç¨‹', 'ç¼–ç¨‹', 'å¼€å‘', 'å­¦ä¹ ', 'learn', 'course', 'è¯¾ç¨‹']
    for tk in tech_keywords:
        if tk in tag_lower and tk in keyword_lower:
            return True

    return False


# ===== é”™è¯¯åˆ†ç±»å¸¸é‡ =====
class ChannelErrorType:
    """é¢‘é“è·å–é”™è¯¯ç±»å‹"""
    SUCCESS = "success"                    # æˆåŠŸ
    NETWORK_TIMEOUT = "network_timeout"    # ç½‘ç»œè¶…æ—¶ï¼ˆå¯é‡è¯•ï¼‰
    SSL_ERROR = "ssl_error"                # SSLé”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰
    CONNECTION_ERROR = "connection_error"  # è¿æ¥é”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰
    CHANNEL_NOT_FOUND = "channel_not_found"  # é¢‘é“ä¸å­˜åœ¨ï¼ˆæ°¸ä¹…æ€§ï¼‰
    CHANNEL_PRIVATE = "channel_private"      # é¢‘é“ç§æœ‰ï¼ˆæ°¸ä¹…æ€§ï¼‰
    RATE_LIMITED = "rate_limited"            # è¢«é™æµï¼ˆå¯é‡è¯•ï¼Œéœ€ç­‰å¾…ï¼‰
    UNKNOWN = "unknown"                      # æœªçŸ¥é”™è¯¯ï¼ˆå¯é‡è¯•ï¼‰

# å¯é‡è¯•çš„é”™è¯¯ç±»å‹
RETRIABLE_ERRORS = {
    ChannelErrorType.NETWORK_TIMEOUT,
    ChannelErrorType.SSL_ERROR,
    ChannelErrorType.CONNECTION_ERROR,
    ChannelErrorType.RATE_LIMITED,
    ChannelErrorType.UNKNOWN,
}

# æ°¸ä¹…æ€§é”™è¯¯ï¼ˆä¸åº”é‡è¯•ï¼‰
PERMANENT_ERRORS = {
    ChannelErrorType.CHANNEL_NOT_FOUND,
    ChannelErrorType.CHANNEL_PRIVATE,
}


def classify_error(stderr: str) -> str:
    """æ ¹æ®é”™è¯¯ä¿¡æ¯åˆ†ç±»é”™è¯¯ç±»å‹"""
    if not stderr:
        return ChannelErrorType.UNKNOWN

    stderr_lower = stderr.lower()

    # æ°¸ä¹…æ€§é”™è¯¯
    if "does not exist" in stderr_lower or "not exist" in stderr_lower:
        return ChannelErrorType.CHANNEL_NOT_FOUND
    if "private" in stderr_lower:
        return ChannelErrorType.CHANNEL_PRIVATE
    if "this channel has no" in stderr_lower:
        return ChannelErrorType.CHANNEL_NOT_FOUND

    # å¯é‡è¯•é”™è¯¯
    if "ssl" in stderr_lower or "eof occurred" in stderr_lower:
        return ChannelErrorType.SSL_ERROR
    if "timed out" in stderr_lower or "timeout" in stderr_lower:
        return ChannelErrorType.NETWORK_TIMEOUT
    if "connection" in stderr_lower or "network" in stderr_lower:
        return ChannelErrorType.CONNECTION_ERROR
    if "rate" in stderr_lower or "429" in stderr_lower or "too many" in stderr_lower:
        return ChannelErrorType.RATE_LIMITED

    return ChannelErrorType.UNKNOWN


def get_channel_real_stats_with_error(channel_id: str) -> Dict:
    """è·å–å•ä¸ªé¢‘é“çš„çœŸå®ç»Ÿè®¡æ•°æ®ï¼ˆå¸¦è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼‰

    è¿”å›: {
        'success': bool,
        'error_type': str,
        'error_message': str,
        'data': {...} æˆ– None
    }
    """
    result = {
        'success': False,
        'error_type': None,
        'error_message': None,
        'data': None
    }

    try:
        url = f"https://www.youtube.com/channel/{channel_id}/videos"
        cmd = [
            'yt-dlp',
            '--flat-playlist',
            '--dump-single-json',
            '--no-warnings',
            '--socket-timeout', '30',
            '--retries', '2',
            url
        ]

        proc_result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)

        if proc_result.returncode != 0:
            error_type = classify_error(proc_result.stderr)
            result['error_type'] = error_type
            result['error_message'] = proc_result.stderr[:200] if proc_result.stderr else "Unknown error"
            return result

        data = json.loads(proc_result.stdout)
        entries = data.get('entries', [])

        # è®¡ç®—é¢‘é“æ€»æ’­æ”¾é‡
        total_views = sum(entry.get('view_count') or 0 for entry in entries)
        video_count = len(entries)
        avg_views = total_views // video_count if video_count > 0 else 0

        result['success'] = True
        result['error_type'] = ChannelErrorType.SUCCESS
        result['data'] = {
            'subscriber_count': data.get('channel_follower_count') or 0,
            'real_video_count': video_count,
            'total_channel_views': total_views,
            'avg_channel_views': avg_views,
            'channel_name': data.get('channel') or data.get('uploader') or ''
        }
        return result

    except subprocess.TimeoutExpired:
        result['error_type'] = ChannelErrorType.NETWORK_TIMEOUT
        result['error_message'] = "Request timed out after 60 seconds"
        return result
    except json.JSONDecodeError as e:
        result['error_type'] = ChannelErrorType.UNKNOWN
        result['error_message'] = f"JSON parse error: {e}"
        return result
    except Exception as e:
        result['error_type'] = ChannelErrorType.UNKNOWN
        result['error_message'] = f"{type(e).__name__}: {e}"
        return result


def get_channel_real_stats(channel_id: str) -> Optional[Dict]:
    """è·å–å•ä¸ªé¢‘é“çš„çœŸå®ç»Ÿè®¡æ•°æ®ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
    result = get_channel_real_stats_with_error(channel_id)
    return result['data'] if result['success'] else None


async def fetch_channels_real_stats(
    channel_ids: List[str],
    max_retries: int = 2,
    progress_callback=None
) -> Dict[str, Dict]:
    """è·å–å¤šä¸ªé¢‘é“çš„çœŸå®ç»Ÿè®¡æ•°æ®ï¼ˆå¸¦æŸ¥æ¼è¡¥ç¼ºæœºåˆ¶ï¼‰

    Args:
        channel_ids: é¢‘é“IDåˆ—è¡¨
        max_retries: æœ€å¤§é‡è¯•è½®æ•°
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° async (current, total, stats) -> None
            stats åŒ…å«: success_count, fail_count, avg_time_per_channel, estimated_remaining_seconds, network_speed

    Returns:
        {channel_id: {subscriber_count, real_video_count, ...}}
    """
    import time

    results = {}
    total = len(channel_ids)

    # è·Ÿè¸ªæ¯ä¸ªé¢‘é“çš„çŠ¶æ€
    channel_status = {cid: {
        'success': False,
        'error_type': None,
        'error_message': None,
        'retry_count': 0,
        'skip': False  # æ°¸ä¹…æ€§é”™è¯¯æ ‡è®°ä¸ºè·³è¿‡
    } for cid in channel_ids}

    # ç»Ÿè®¡æ•°æ®
    fetch_times = []  # è®°å½•æ¯æ¬¡è¯·æ±‚çš„è€—æ—¶
    success_count = 0
    fail_count = 0
    processed_count = 0

    print(f"   å¼€å§‹è·å– {total} ä¸ªé¢‘é“æ•°æ®ï¼ˆå¸¦æŸ¥æ¼è¡¥ç¼ºæœºåˆ¶ï¼‰", flush=True)

    # ===== ç¬¬ä¸€è½®é‡‡é›† =====
    pending_ids = list(channel_ids)
    round_num = 1

    while pending_ids and round_num <= max_retries + 1:
        if round_num > 1:
            print(f"\n   ğŸ“¦ ç¬¬ {round_num} è½®è¡¥é‡‡: {len(pending_ids)} ä¸ªé¢‘é“", flush=True)
            await asyncio.sleep(2)  # é‡è¯•å‰ç­‰å¾…

        next_pending = []

        for i, cid in enumerate(pending_ids):
            status = channel_status[cid]

            # è·³è¿‡å·²æ ‡è®°çš„æ°¸ä¹…æ€§å¤±è´¥
            if status['skip']:
                continue

            print(f"   [{i+1}/{len(pending_ids)}] è·å–é¢‘é“ {cid[:15]}...", flush=True)

            # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
            start_time = time.time()

            fetch_result = get_channel_real_stats_with_error(cid)
            status['retry_count'] += 1

            # è®°å½•è¯·æ±‚è€—æ—¶
            elapsed = time.time() - start_time
            fetch_times.append(elapsed)
            processed_count += 1

            if fetch_result['success']:
                # æˆåŠŸ
                results[cid] = fetch_result['data']
                status['success'] = True
                status['error_type'] = ChannelErrorType.SUCCESS
                success_count += 1
                print(f"       âœ“ {fetch_result['data']['channel_name']}: è®¢é˜…={fetch_result['data']['subscriber_count']:,}", flush=True)
            else:
                # å¤±è´¥ - åˆ†ç±»å¤„ç†
                error_type = fetch_result['error_type']
                status['error_type'] = error_type
                status['error_message'] = fetch_result['error_message']
                fail_count += 1

                if error_type in PERMANENT_ERRORS:
                    # æ°¸ä¹…æ€§é”™è¯¯ - æ ‡è®°è·³è¿‡
                    status['skip'] = True
                    print(f"       âœ— æ°¸ä¹…å¤±è´¥({error_type}): {fetch_result['error_message'][:80]}", flush=True)
                elif error_type in RETRIABLE_ERRORS:
                    # å¯é‡è¯•é”™è¯¯ - åŠ å…¥ä¸‹ä¸€è½®
                    if round_num <= max_retries:
                        next_pending.append(cid)
                        print(f"       âš  æš‚æ—¶å¤±è´¥({error_type}): å°†åœ¨ä¸‹ä¸€è½®é‡è¯•", flush=True)
                    else:
                        print(f"       âœ— ç½‘ç»œå¤±è´¥({error_type}): å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°", flush=True)

            # å‘é€è¿›åº¦å›è°ƒ
            if progress_callback and round_num == 1:  # åªåœ¨ç¬¬ä¸€è½®å‘é€è¯¦ç»†è¿›åº¦
                # è®¡ç®—å¹³å‡è€—æ—¶å’Œé¢„ä¼°å‰©ä½™æ—¶é—´
                avg_time = sum(fetch_times) / len(fetch_times) if fetch_times else 0.5
                remaining = total - processed_count
                estimated_remaining = remaining * (avg_time + 0.5)  # åŠ ä¸Šé—´éš”æ—¶é—´

                # è®¡ç®—ç½‘ç»œé€Ÿåº¦çŠ¶æ€
                recent_times = fetch_times[-5:] if len(fetch_times) >= 5 else fetch_times
                recent_avg = sum(recent_times) / len(recent_times) if recent_times else 0.5

                if recent_avg < 1.0:
                    network_speed = 'fast'
                elif recent_avg < 3.0:
                    network_speed = 'normal'
                else:
                    network_speed = 'slow'

                await progress_callback(processed_count, total, {
                    'success_count': success_count,
                    'fail_count': fail_count,
                    'avg_time_per_channel': round(avg_time, 2),
                    'estimated_remaining_seconds': round(estimated_remaining),
                    'network_speed': network_speed,
                    'last_fetch_time': round(elapsed, 2),
                    'current_channel': fetch_result['data'].get('channel_name', cid[:15]) if fetch_result['success'] else cid[:15]
                })

            # è¯·æ±‚é—´éš”
            if i < len(pending_ids) - 1:
                await asyncio.sleep(0.5)

        pending_ids = next_pending
        round_num += 1

    # ===== ç»Ÿè®¡ç»“æœ =====
    success_count = sum(1 for s in channel_status.values() if s['success'])
    permanent_fail = sum(1 for s in channel_status.values() if s['skip'])
    network_fail = sum(1 for s in channel_status.values()
                       if not s['success'] and not s['skip'])

    print(f"\n   ğŸ“Š é¢‘é“æ•°æ®è·å–å®Œæˆ:", flush=True)
    print(f"      âœ“ æˆåŠŸ: {success_count}/{total}", flush=True)
    print(f"      âœ— æ°¸ä¹…å¤±è´¥(å·²è·³è¿‡): {permanent_fail}", flush=True)
    print(f"      âš  ç½‘ç»œå¤±è´¥(å¯é‡è¯•): {network_fail}", flush=True)

    # è¿”å›ç»“æœæ—¶é™„åŠ çŠ¶æ€ä¿¡æ¯
    for cid in channel_ids:
        if cid in results:
            results[cid]['_fetch_status'] = 'success'
        else:
            status = channel_status[cid]
            # ä¸ºå¤±è´¥çš„é¢‘é“ä¹Ÿåˆ›å»ºä¸€ä¸ªå ä½è®°å½•
            results[cid] = {
                'subscriber_count': 0,
                'real_video_count': 0,
                'total_channel_views': 0,
                'avg_channel_views': 0,
                'channel_name': '',
                '_fetch_status': 'permanent_failure' if status['skip'] else 'network_failure',
                '_error_type': status['error_type'],
                '_error_message': status['error_message'],
                '_retriable': not status['skip']
            }

    return results


async def search_videos_multi(
    keyword: str,
    target_count: int,
    time_range: str = None,
    progress_callback=None,
    use_smart_keywords: bool = True
) -> List[Dict]:
    """å¤šä»»åŠ¡æœç´¢ï¼Œé€šè¿‡å¤šä¸ªæœç´¢æŸ¥è¯¢è·å–æ›´å¤šè§†é¢‘

    Args:
        keyword: ä¸»å…³é”®è¯
        target_count: ç›®æ ‡è§†é¢‘æ•°é‡
        time_range: æ—¶é—´è¿‡æ»¤
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        use_smart_keywords: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½å…³é”®è¯é‡‡é›†

    Returns:
        å»é‡åçš„è§†é¢‘åˆ—è¡¨
    """
    # è®¡ç®—éœ€è¦çš„æœç´¢ä»»åŠ¡æ•°
    # æ¯æ¬¡æœç´¢æœ€å¤šè¿”å›çº¦ 50-100 ä¸ªæœ‰æ•ˆç»“æœ
    per_search_max = 100
    num_searches = min((target_count // per_search_max) + 1, 10)  # æœ€å¤š10æ¬¡æœç´¢

    # ä½¿ç”¨æ™ºèƒ½å…³é”®è¯é‡‡é›†æˆ–åŸºç¡€å˜ä½“
    if use_smart_keywords:
        if progress_callback:
            await progress_callback("æ­£åœ¨è·å– YouTube æœç´¢å»ºè®®...", 5)
        search_queries = await get_smart_search_keywords(keyword, max_keywords=num_searches)
    else:
        search_queries = generate_search_variations(keyword)[:num_searches]

    print(f"\nğŸ” å¤šä»»åŠ¡æœç´¢ç­–ç•¥:")
    print(f"   ç›®æ ‡æ•°é‡: {target_count}")
    print(f"   æœç´¢ä»»åŠ¡æ•°: {len(search_queries)}")
    print(f"   æ™ºèƒ½å…³é”®è¯: {'æ˜¯' if use_smart_keywords else 'å¦'}")
    print(f"   æœç´¢å…³é”®è¯: {search_queries}")

    all_videos = []
    seen_ids = set()

    loop = asyncio.get_event_loop()

    for i, query in enumerate(search_queries):
        if progress_callback:
            await progress_callback(
                f"æœç´¢ä»»åŠ¡ {i+1}/{len(search_queries)}: \"{query}\"",
                int(10 + (i / len(search_queries)) * 30)
            )

        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œæœç´¢
        results = await loop.run_in_executor(
            None,
            lambda q=query: search_videos_ytdlp(q, per_search_max, time_range)
        )

        # å»é‡åˆå¹¶
        new_count = 0
        for video in results:
            vid = video.get('id', '')
            if vid and vid not in seen_ids:
                seen_ids.add(vid)
                all_videos.append(video)
                new_count += 1

        print(f"   ä»»åŠ¡ {i+1}: æ‰¾åˆ° {len(results)} ä¸ªï¼Œæ–°å¢ {new_count} ä¸ªï¼ˆå»é‡åå…± {len(all_videos)} ä¸ªï¼‰")

        # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œæå‰ç»“æŸ
        if len(all_videos) >= target_count:
            print(f"   âœ“ å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ {target_count}ï¼Œåœæ­¢æœç´¢")
            break

        # æ·»åŠ å°å»¶è¿Ÿé¿å…è¢«é™æµ
        await asyncio.sleep(1)

    print(f"\nğŸ“Š å¤šä»»åŠ¡æœç´¢å®Œæˆ: å…± {len(all_videos)} ä¸ªå”¯ä¸€è§†é¢‘")
    return all_videos


def get_video_details(video_id: str) -> Optional[Dict]:
    """è·å–å•ä¸ªè§†é¢‘è¯¦æƒ…"""
    try:
        cmd = [
            'yt-dlp',
            '--dump-json',
            '--no-download',
            '--no-warnings',
            f'https://www.youtube.com/watch?v={video_id}'
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.stdout:
            return json.loads(result.stdout)
        return None
    except Exception as e:
        print(f"è·å–è¯¦æƒ…é”™è¯¯: {e}")
        return None


async def fetch_videos_details_concurrent(
    video_ids: List[str],
    max_concurrent: int = 10,
    progress_callback=None
) -> Dict[str, Dict]:
    """å¹¶å‘è·å–å¤šä¸ªè§†é¢‘çš„è¯¦ç»†ä¿¡æ¯

    Args:
        video_ids: è§†é¢‘IDåˆ—è¡¨
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤10ï¼‰
        progress_callback: è¿›åº¦å›è°ƒ async def callback(current, total, message)

    Returns:
        {video_id: video_details} å­—å…¸
    """
    results = {}
    loop = asyncio.get_event_loop()
    total = len(video_ids)

    print(f"\nğŸ¬ å¹¶å‘è·å– {total} ä¸ªè§†é¢‘è¯¦æƒ… (å¹¶å‘æ•°: {max_concurrent})")

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, total, max_concurrent):
        batch = video_ids[i:i + max_concurrent]
        batch_num = i // max_concurrent + 1
        total_batches = (total + max_concurrent - 1) // max_concurrent

        # å¹¶å‘æ‰§è¡Œ
        tasks = [
            loop.run_in_executor(None, get_video_details, vid)
            for vid in batch
        ]

        batch_results = await asyncio.gather(*tasks)

        # æ”¶é›†ç»“æœ
        success_count = 0
        for vid, details in zip(batch, batch_results):
            if details:
                results[vid] = details
                success_count += 1

        # è¿›åº¦å›è°ƒ
        completed = min(i + max_concurrent, total)
        if progress_callback:
            await progress_callback(
                completed,
                total,
                f"å·²è·å– {completed}/{total} ä¸ªè§†é¢‘è¯¦æƒ… (æ‰¹æ¬¡ {batch_num}/{total_batches})"
            )

        print(f"   æ‰¹æ¬¡ {batch_num}/{total_batches}: {success_count}/{len(batch)} æˆåŠŸ")

        # æ‰¹æ¬¡é—´çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¢«é™æµ
        if i + max_concurrent < total:
            await asyncio.sleep(0.3)

    print(f"   âœ“ å®Œæˆ: æˆåŠŸè·å– {len(results)}/{total} ä¸ªè§†é¢‘è¯¦æƒ…")
    return results


# ============ WebSocket è¿æ¥ç®¡ç† ============

class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}

    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections[client_id] = websocket

    def disconnect(self, client_id: str):
        if client_id in self.active_connections:
            del self.active_connections[client_id]

    async def send_progress(self, client_id: str, data: dict):
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json(data)
            except Exception as e:
                print(f"WebSocket å‘é€å¤±è´¥ ({client_id}): {e}")
                self.disconnect(client_id)

    def is_connected(self, client_id: str) -> bool:
        """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦ä»ç„¶è¿æ¥"""
        return client_id in self.active_connections


manager = ConnectionManager()


# ============ FastAPI åº”ç”¨ ============

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ YouTube å†…å®¹åŠ©æ‰‹ API å¯åŠ¨ä¸­...")
    yield
    print("ğŸ‘‹ API æœåŠ¡å…³é—­")


app = FastAPI(
    title="YouTube å†…å®¹åŠ©æ‰‹ API",
    description="YouTube è§†é¢‘ç ”ç©¶ä¸åˆ†æ API",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é™æ€æ–‡ä»¶æœåŠ¡
web_dir = Path(__file__).parent / "web"
if web_dir.exists():
    app.mount("/static", StaticFiles(directory=str(web_dir)), name="static")

# 2026-1-17 ç›®å½•çš„é™æ€æ–‡ä»¶æœåŠ¡
demo_dir = Path(__file__).parent / "2026-1-17"
if demo_dir.exists():
    app.mount("/2026-1-17", StaticFiles(directory=str(demo_dir), html=True), name="demo")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - è¿”å›å‰ç«¯é¡µé¢"""
    index_path = web_dir / "demo.html"
    if index_path.exists():
        return FileResponse(str(index_path))
    return {"message": "YouTube å†…å®¹åŠ©æ‰‹ API", "docs": "/docs"}


@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "timestamp": datetime.now().isoformat()}


@app.get("/api/suggestions")
async def get_suggestions(q: str = Query(..., description="æœç´¢å…³é”®è¯")):
    """è·å– YouTube æœç´¢å»ºè®®

    è¿”å› YouTube è‡ªåŠ¨è¡¥å…¨çš„æœç´¢å»ºè®®ï¼ˆé•¿å°¾å…³é”®è¯ï¼‰
    """
    loop = asyncio.get_event_loop()
    suggestions = await loop.run_in_executor(
        None,
        lambda: get_search_suggestions(q, max_suggestions=10)
    )
    return {
        "query": q,
        "suggestions": suggestions,
        "count": len(suggestions)
    }


@app.get("/api/channel/{channel_id}/tags")
async def get_channel_tags(
    channel_id: str,
    max_videos: int = Query(default=10, le=20, description="åˆ†æçš„è§†é¢‘æ•°é‡")
):
    """ä»é¢‘é“è§†é¢‘ä¸­æå–é«˜é¢‘æ ‡ç­¾

    åˆ†æé¢‘é“æœ€æ–°è§†é¢‘çš„æ ‡ç­¾ï¼Œè¿”å›é«˜é¢‘å…³é”®è¯
    """
    loop = asyncio.get_event_loop()
    tags = await loop.run_in_executor(
        None,
        lambda: extract_tags_from_channel(channel_id, max_videos=max_videos, top_n=20)
    )
    return {
        "channel_id": channel_id,
        "tags": tags,
        "count": len(tags)
    }


@app.get("/api/channel/{channel_id}")
async def get_channel_info(channel_id: str):
    """è·å–é¢‘é“çœŸå®ç»Ÿè®¡æ•°æ®

    è¿”å›ï¼š
    - channel_name: é¢‘é“åç§°
    - subscriber_count: è®¢é˜…æ•°
    - video_count: è§†é¢‘æ€»æ•°
    - description: é¢‘é“æè¿°
    """
    try:
        url = f"https://www.youtube.com/channel/{channel_id}/videos"

        cmd = [
            'yt-dlp',
            '--flat-playlist',
            '--dump-single-json',
            '--no-warnings',
            url
        ]

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode != 0:
            raise HTTPException(status_code=404, detail=f"é¢‘é“ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®: {channel_id}")

        data = json.loads(result.stdout)

        # æå–é¢‘é“ä¿¡æ¯
        entries = data.get('entries', [])

        return {
            "channel_id": channel_id,
            "channel_name": data.get('channel') or data.get('uploader') or 'æœªçŸ¥',
            "subscriber_count": data.get('channel_follower_count') or 0,
            "video_count": len(entries),
            "description": data.get('description') or '',
            "channel_url": f"https://www.youtube.com/channel/{channel_id}"
        }

    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=504, detail="è¯·æ±‚è¶…æ—¶")
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="è§£æé¢‘é“æ•°æ®å¤±è´¥")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/task/{task_id}/status")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€å’Œç»“æœï¼ˆç”¨äºæ–­çº¿é‡è¿åæ¢å¤ï¼‰

    è¿”å›:
        - status: "running" | "complete" | "error" | "not_found"
        - progress: è¿›åº¦ç™¾åˆ†æ¯”
        - message: å½“å‰çŠ¶æ€æ¶ˆæ¯
        - result: ä»»åŠ¡ç»“æœï¼ˆä»…å½“ status ä¸º complete æ—¶ï¼‰
    """
    cached = task_cache.get(task_id)

    if cached is None:
        return {
            "status": "not_found",
            "message": "ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"
        }

    return {
        "status": cached["status"],
        "progress": cached["progress"],
        "message": cached["message"],
        "result": cached["result"] if cached["status"] == "complete" else None
    }


@app.post("/api/channels/batch")
async def get_channels_batch(channel_ids: List[str]):
    """æ‰¹é‡è·å–å¤šä¸ªé¢‘é“çš„çœŸå®ç»Ÿè®¡æ•°æ®

    æœ€å¤šæ”¯æŒ 10 ä¸ªé¢‘é“åŒæ—¶æŸ¥è¯¢
    """
    if len(channel_ids) > 10:
        raise HTTPException(status_code=400, detail="æœ€å¤šæ”¯æŒ 10 ä¸ªé¢‘é“åŒæ—¶æŸ¥è¯¢")

    results = {}

    async def fetch_channel(ch_id: str):
        try:
            url = f"https://www.youtube.com/channel/{ch_id}/videos"
            cmd = [
                'yt-dlp',
                '--flat-playlist',
                '--dump-single-json',
                '--no-warnings',
                url
            ]

            # ä½¿ç”¨ asyncio åŒ…è£…åŒæ­¥è°ƒç”¨
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            )

            if result.returncode == 0:
                data = json.loads(result.stdout)
                entries = data.get('entries', [])
                return {
                    "channel_id": ch_id,
                    "channel_name": data.get('channel') or data.get('uploader') or 'æœªçŸ¥',
                    "subscriber_count": data.get('channel_follower_count') or 0,
                    "video_count": len(entries),
                    "success": True
                }
        except Exception as e:
            pass

        return {"channel_id": ch_id, "success": False, "error": "è·å–å¤±è´¥"}

    # å¹¶å‘è·å–æ‰€æœ‰é¢‘é“
    tasks = [fetch_channel(ch_id) for ch_id in channel_ids]
    channel_results = await asyncio.gather(*tasks)

    for ch_data in channel_results:
        results[ch_data['channel_id']] = ch_data

    return {"channels": results}


@app.websocket("/ws/{task_id}")
async def websocket_research(websocket: WebSocket, task_id: str):
    """WebSocket ç ”ç©¶ä»»åŠ¡"""
    await manager.connect(websocket, task_id)

    try:
        # ç­‰å¾…ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼ˆç ”ç©¶è¯·æ±‚ï¼‰
        data = await websocket.receive_json()

        # å¤„ç†å¿ƒè·³ ping
        if data.get('type') == 'ping':
            await websocket.send_json({"type": "pong", "timestamp": data.get('timestamp')})
            # ç»§ç»­ç­‰å¾…çœŸæ­£çš„è¯·æ±‚
            data = await websocket.receive_json()

        request = ResearchRequest(**data)

        await manager.send_progress(task_id, {
            "type": "start",
            "topic": request.topic,
            "mode": request.mode
        })

        # åˆ›å»ºå¿ƒè·³å¤„ç†ä»»åŠ¡
        async def handle_heartbeat():
            """æŒç»­å¤„ç†å¿ƒè·³æ¶ˆæ¯"""
            while manager.is_connected(task_id):
                try:
                    # ä½¿ç”¨çŸ­è¶…æ—¶æ¥æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¶ˆæ¯
                    msg = await asyncio.wait_for(websocket.receive_json(), timeout=1.0)
                    if msg.get('type') == 'ping':
                        await websocket.send_json({"type": "pong", "timestamp": msg.get('timestamp')})
                        print(f"ğŸ’“ å“åº”å¿ƒè·³ pong ({task_id})")
                except asyncio.TimeoutError:
                    # æ²¡æœ‰æ–°æ¶ˆæ¯ï¼Œç»§ç»­ç­‰å¾…
                    continue
                except WebSocketDisconnect:
                    print(f"ğŸ’” å¿ƒè·³æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ ({task_id})")
                    break
                except Exception as e:
                    # å…¶ä»–é”™è¯¯ï¼Œå¯èƒ½æ˜¯è¿æ¥é—®é¢˜
                    if "disconnect" in str(e).lower() or "closed" in str(e).lower():
                        break
                    continue

        # å¹¶è¡Œè¿è¡Œï¼šå¿ƒè·³å¤„ç† + ç ”ç©¶ä»»åŠ¡
        heartbeat_task = asyncio.create_task(handle_heartbeat())

        try:
            # åˆå§‹åŒ–ç¼“å­˜çŠ¶æ€
            task_cache.update_progress(task_id, 0, "ä»»åŠ¡å¼€å§‹")

            result = await run_research_task(task_id, request)

            # å–æ¶ˆå¿ƒè·³ä»»åŠ¡
            heartbeat_task.cancel()
            try:
                await heartbeat_task
            except asyncio.CancelledError:
                pass

            # ä¿å­˜ç»“æœåˆ°ç¼“å­˜ï¼ˆæ— è®ºè¿æ¥æ˜¯å¦æ–­å¼€ï¼‰
            task_cache.save(task_id, result, "complete")

            # å‘é€å®Œæˆæ¶ˆæ¯
            if manager.is_connected(task_id):
                await manager.send_progress(task_id, {
                    "type": "complete",
                    "result": result
                })
            else:
                print(f"âš ï¸ ä»»åŠ¡å®Œæˆä½† WebSocket å·²æ–­å¼€ï¼Œç»“æœå·²ç¼“å­˜: {task_id}")

        except Exception as e:
            heartbeat_task.cancel()
            # ä¿å­˜é”™è¯¯çŠ¶æ€åˆ°ç¼“å­˜
            task_cache.save(task_id, {"error": str(e)}, "error")
            raise e

    except WebSocketDisconnect:
        print(f"WebSocket å®¢æˆ·ç«¯æ–­å¼€è¿æ¥: {task_id}")
        print(f"   ğŸ’¡ å®¢æˆ·ç«¯å¯é€šè¿‡ /api/task/{task_id}/status è·å–ç»“æœ")
        manager.disconnect(task_id)
    except Exception as e:
        print(f"ç ”ç©¶ä»»åŠ¡é”™è¯¯ ({task_id}): {e}")
        # ä¿å­˜é”™è¯¯åˆ°ç¼“å­˜
        task_cache.save(task_id, {"error": str(e)}, "error")
        # å°è¯•å‘é€é”™è¯¯æ¶ˆæ¯ï¼Œä½†å¦‚æœè¿æ¥å·²æ–­å¼€åˆ™å¿½ç•¥
        if manager.is_connected(task_id):
            try:
                await manager.send_progress(task_id, {
                    "type": "error",
                    "message": str(e)
                })
            except:
                pass
        manager.disconnect(task_id)


async def run_research_task(task_id: str, request: ResearchRequest) -> dict:
    """æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""

    # æ‰“å°æ”¶åˆ°çš„ç­›é€‰å‚æ•°
    print(f"\nğŸ“‹ æ”¶åˆ°ç ”ç©¶è¯·æ±‚:")
    print(f"   ä¸»é¢˜: {request.topic}")
    print(f"   æ¨¡å¼: {request.mode}")
    print(f"   æ’­æ”¾é‡èŒƒå›´: {request.views_min} - {request.views_max}")
    print(f"   æ—¶é•¿ç­›é€‰: {request.duration_filter}")
    print(f"   æ—¶é—´èŒƒå›´: {request.time_range}")
    print(f"   AIç­›é€‰: {request.ai_filter}")
    print(f"   æ’åº: {request.sort_by}")

    # æ–°çš„æ¨¡å¼é…ç½®ï¼šæ”¯æŒæ›´å¤§æ•°é‡
    mode_config = {
        "quick": 100,      # å¿«é€Ÿæ‰«æï¼š100ä¸ªè§†é¢‘
        "standard": 500,   # æ ‡å‡†åˆ†æï¼š500ä¸ªè§†é¢‘ï¼ˆå¤šä»»åŠ¡æœç´¢ï¼‰
        "deep": 1000       # æ·±åº¦æŒ–æ˜ï¼š1000ä¸ªè§†é¢‘ï¼ˆå¤šä»»åŠ¡æœç´¢ï¼‰
    }
    target_count = mode_config.get(request.mode, 500)

    # åˆ¤æ–­æ˜¯å¦éœ€è¦å¤šä»»åŠ¡æœç´¢
    use_multi_search = target_count > 200

    await manager.send_progress(task_id, {
        "type": "progress",
        "stage": "search",
        "progress": 10,
        "message": f"æ­£åœ¨æœç´¢ \"{request.topic}\" ç›¸å…³è§†é¢‘...ï¼ˆç›®æ ‡: {target_count}ä¸ªï¼‰"
    })

    # è¿›åº¦å›è°ƒï¼ˆå¸¦è¿æ¥æ£€æŸ¥ + ç¼“å­˜æ›´æ–°ï¼‰
    async def progress_callback(message: str, progress: int):
        # å§‹ç»ˆæ›´æ–°ç¼“å­˜ï¼ˆå³ä½¿è¿æ¥æ–­å¼€ï¼‰
        task_cache.update_progress(task_id, progress, message)

        if not manager.is_connected(task_id):
            return
        try:
            await manager.send_progress(task_id, {
                "type": "progress",
                "stage": "search",
                "progress": progress,
                "message": message
            })
        except Exception as e:
            print(f"   âš  å‘é€æœç´¢è¿›åº¦å¤±è´¥: {e}", flush=True)

    # æ ¹æ®ç›®æ ‡æ•°é‡é€‰æ‹©æœç´¢ç­–ç•¥
    if use_multi_search:
        print(f"\nğŸš€ ä½¿ç”¨å¤šä»»åŠ¡æœç´¢ç­–ç•¥ï¼ˆç›®æ ‡: {target_count}ä¸ªè§†é¢‘ï¼‰")
        search_results = await search_videos_multi(
            request.topic,
            target_count,
            request.time_range,
            progress_callback
        )
    else:
        # å•ä»»åŠ¡æœç´¢
        loop = asyncio.get_event_loop()
        search_results = await loop.run_in_executor(
            None,
            lambda: search_videos_ytdlp(request.topic, target_count, request.time_range)
        )

    await manager.send_progress(task_id, {
        "type": "progress",
        "stage": "search",
        "progress": 40,
        "message": f"æ‰¾åˆ° {len(search_results)} ä¸ªè§†é¢‘ï¼Œæ­£åœ¨å¤„ç†..."
    })

    videos = []
    channels_map = {}

    for i, video in enumerate(search_results):
        is_ai, ai_kw = detect_ai_video(
            video.get('title', ''),
            video.get('description', ''),
            video.get('tags', [])
        )

        # è®¡ç®—è§†é¢‘å¹´é¾„å’Œæ—¥å‡æ’­æ”¾é‡
        published_at = video.get('upload_date', '')
        video_age_days = 1  # é»˜è®¤1å¤©ï¼Œé¿å…é™¤é›¶
        if published_at and len(published_at) == 8:
            try:
                pub_date = datetime.strptime(published_at, '%Y%m%d')
                video_age_days = max(1, (datetime.now() - pub_date).days)
            except:
                pass

        view_count = video.get('view_count', 0) or 0
        daily_views = view_count // video_age_days if video_age_days > 0 else 0

        video_item = {
            'youtube_id': video.get('id', ''),
            'title': video.get('title', ''),
            'channel_name': video.get('channel', video.get('uploader', '')),
            'channel_id': video.get('channel_id', ''),
            'view_count': view_count,
            'like_count': video.get('like_count', 0) or 0,
            'comment_count': video.get('comment_count', 0) or 0,
            'duration': video.get('duration', 0) or 0,
            'published_at': published_at,
            'thumbnail_url': video.get('thumbnail', ''),
            'is_ai_video': is_ai,
            'ai_keyword': ai_kw,
            # æ–°å¢ï¼šè§†é¢‘å¹´é¾„å’Œæ—¥å‡æ’­æ”¾
            'video_age_days': video_age_days,
            'daily_views': daily_views
        }

        # åº”ç”¨ç­›é€‰
        if not apply_filters(video_item, request):
            continue

        videos.append(video_item)

        # ç»Ÿè®¡é¢‘é“
        ch_name = video_item['channel_name']
        if ch_name:
            if ch_name not in channels_map:
                channels_map[ch_name] = {
                    'channel_id': video_item['channel_id'],
                    'channel_name': ch_name,
                    'video_count': 0,
                    'total_views': 0
                }
            channels_map[ch_name]['video_count'] += 1
            channels_map[ch_name]['total_views'] += video_item['view_count']

        if i % 20 == 0:
            progress = 40 + int((i / len(search_results)) * 30)
            await manager.send_progress(task_id, {
                "type": "progress",
                "stage": "collect",
                "progress": progress,
                "message": f"å·²å¤„ç† {i}/{len(search_results)} ä¸ªè§†é¢‘..."
            })

    # æ‰“å°ç­›é€‰ç»“æœç»Ÿè®¡
    filtered_count = len(search_results) - len(videos)
    print(f"\nğŸ“Š ç­›é€‰ç»“æœ:")
    print(f"   åŸå§‹è§†é¢‘æ•°: {len(search_results)}")
    print(f"   ç­›é€‰åè§†é¢‘æ•°: {len(videos)}")
    print(f"   è¢«è¿‡æ»¤æ‰: {filtered_count} ä¸ª")

    await manager.send_progress(task_id, {
        "type": "progress",
        "stage": "analyze",
        "progress": 70,
        "message": f"æ­£åœ¨åˆ†æ {len(videos)} ä¸ªè§†é¢‘æ•°æ®..."
    })

    # ===== å¹¶å‘è·å– Top è§†é¢‘çš„å®Œæ•´è¯¦æƒ… =====
    # æŒ‰æ’­æ”¾é‡æ’åºï¼Œå– Top N è·å–è¯¦æƒ…
    videos_sorted_by_views = sorted(videos, key=lambda x: x['view_count'], reverse=True)
    top_video_count = min(200, len(videos_sorted_by_views))  # æœ€å¤šè·å– 200 ä¸ªè§†é¢‘è¯¦æƒ…
    top_video_ids = [v['youtube_id'] for v in videos_sorted_by_views[:top_video_count] if v.get('youtube_id')]

    if top_video_ids:
        await manager.send_progress(task_id, {
            "type": "progress",
            "stage": "details",
            "progress": 72,
            "message": f"æ­£åœ¨è·å– Top {len(top_video_ids)} ä¸ªè§†é¢‘çš„å®Œæ•´è¯¦æƒ…..."
        })

        async def video_progress_callback(current, total, message):
            if not manager.is_connected(task_id):
                return
            try:
                progress = 72 + int((current / total) * 10)  # 72-82%
                await manager.send_progress(task_id, {
                    "type": "progress",
                    "stage": "details",
                    "progress": progress,
                    "message": message
                })
            except Exception as e:
                print(f"   âš  å‘é€è§†é¢‘è¯¦æƒ…è¿›åº¦å¤±è´¥: {e}", flush=True)

        video_details = await fetch_videos_details_concurrent(
            top_video_ids,
            max_concurrent=10,
            progress_callback=video_progress_callback
        )

        # åˆå¹¶è¯¦æƒ…åˆ°è§†é¢‘åˆ—è¡¨
        enriched_count = 0
        for v in videos:
            vid = v.get('youtube_id')
            if vid and vid in video_details:
                details = video_details[vid]
                # è¡¥å……æ›´å¤šå­—æ®µ - å…³é”®ï¼šæ›´æ–° view_countï¼
                if details.get('view_count'):
                    v['view_count'] = details.get('view_count', 0) or 0
                # æ›´æ–°å‘å¸ƒæ—¥æœŸï¼ˆå…³é”®ï¼šç”¨äºæ—¶é—´ç­›é€‰ï¼‰
                if details.get('upload_date'):
                    v['published_at'] = details.get('upload_date')
                    # é‡æ–°è®¡ç®—è§†é¢‘å¹´é¾„
                    try:
                        pub_date = datetime.strptime(v['published_at'], '%Y%m%d')
                        v['video_age_days'] = max(1, (datetime.now() - pub_date).days)
                        v['daily_views'] = v['view_count'] // v['video_age_days'] if v['video_age_days'] > 0 else 0
                    except:
                        pass
                v['description'] = details.get('description', '')[:500]  # é™åˆ¶é•¿åº¦
                v['tags'] = details.get('tags', [])[:20]  # é™åˆ¶æ ‡ç­¾æ•°é‡
                v['like_count'] = details.get('like_count', v.get('like_count', 0)) or 0
                v['comment_count'] = details.get('comment_count', v.get('comment_count', 0)) or 0
                v['categories'] = details.get('categories', [])
                v['has_details'] = True
                enriched_count += 1
            else:
                v['has_details'] = False

        print(f"   âœ“ å·²è¡¥å…… {enriched_count} ä¸ªè§†é¢‘çš„å®Œæ•´è¯¦æƒ…")

        # é‡æ–°æŒ‰çœŸå®æ’­æ”¾é‡æ’åº
        videos.sort(key=lambda x: x['view_count'], reverse=True)

        # é‡æ–°è®¡ç®—é¢‘é“çš„ total_viewsï¼ˆå› ä¸ºè§†é¢‘æ’­æ”¾é‡å·²æ›´æ–°ï¼‰
        for ch_name in channels_map:
            channels_map[ch_name]['total_views'] = 0
        for v in videos:
            ch_name = v.get('channel_name')
            if ch_name and ch_name in channels_map:
                channels_map[ch_name]['total_views'] += v['view_count']

    # æ•´ç†é¢‘é“åˆ—è¡¨ï¼ˆä»…ç”¨äºæ’åºï¼Œä¸è®¡ç®—æœç´¢ç»Ÿè®¡ï¼‰
    channels = list(channels_map.values())

    # æŒ‰æœç´¢ç»“æœæ’åº
    channels.sort(key=lambda x: x['total_views'], reverse=True)

    # è·å–æ‰€æœ‰éœ€è¦æ˜¾ç¤ºçš„é¢‘é“æ•°æ®ï¼ˆæœ€å¤š100ä¸ªï¼Œä¸è¿”å›æ•°é‡ä¸€è‡´ï¼‰
    max_channels_to_fetch = 100
    all_channel_ids = [ch['channel_id'] for ch in channels[:max_channels_to_fetch] if ch.get('channel_id')]

    # è·å–çœŸå®é¢‘é“æ•°æ®
    if all_channel_ids:
        await manager.send_progress(task_id, {
            "type": "progress",
            "stage": "channels",
            "progress": 85,
            "message": f"æ­£åœ¨è·å– {len(all_channel_ids)} ä¸ªé¢‘é“çš„çœŸå®æ•°æ®...",
            "channel_fetch": {
                "total": len(all_channel_ids),
                "current": 0,
                "estimated_seconds": len(all_channel_ids) * 1.5  # åˆå§‹é¢„ä¼°
            }
        })

        # é¢‘é“è·å–è¿›åº¦å›è°ƒï¼ˆå¸¦è¿æ¥æ£€æŸ¥ï¼‰
        async def channel_progress_callback(current, total, stats):
            # æ£€æŸ¥ WebSocket æ˜¯å¦ä»ç„¶è¿æ¥
            if not manager.is_connected(task_id):
                print(f"   âš  WebSocket å·²æ–­å¼€ï¼Œåœæ­¢å‘é€è¿›åº¦", flush=True)
                return

            # è¿›åº¦ä» 85% åˆ° 94%
            progress = 85 + int((current / total) * 9)
            try:
                await manager.send_progress(task_id, {
                    "type": "progress",
                    "stage": "channels",
                    "progress": progress,
                    "message": f"è·å–é¢‘é“æ•°æ® {current}/{total}",
                    "channel_fetch": {
                        "total": total,
                        "current": current,
                        "success_count": stats['success_count'],
                        "fail_count": stats['fail_count'],
                        "estimated_remaining_seconds": stats['estimated_remaining_seconds'],
                        "network_speed": stats['network_speed'],
                        "avg_time_per_channel": stats['avg_time_per_channel'],
                        "last_fetch_time": stats['last_fetch_time'],
                        "current_channel": stats['current_channel']
                    }
                })
            except Exception as e:
                print(f"   âš  å‘é€è¿›åº¦å¤±è´¥: {e}", flush=True)

        print(f"\nğŸ“Š è·å–é¢‘é“çœŸå®æ•°æ®: {len(all_channel_ids)} ä¸ªé¢‘é“", flush=True)
        real_stats = await fetch_channels_real_stats(
            all_channel_ids,
            max_retries=2,
            progress_callback=channel_progress_callback
        )

        # ç»Ÿè®¡æˆåŠŸæ•°é‡
        success_count = sum(1 for s in real_stats.values() if s.get('_fetch_status') == 'success')
        print(f"   âœ“ æˆåŠŸè·å– {success_count}/{len(all_channel_ids)} ä¸ªé¢‘é“çš„çœŸå®æ•°æ®", flush=True)

        # åˆå¹¶çœŸå®æ•°æ®åˆ°é¢‘é“ç»Ÿè®¡
        for ch in channels:
            cid = ch.get('channel_id')
            if cid and cid in real_stats:
                stats = real_stats[cid]
                fetch_status = stats.get('_fetch_status', 'unknown')

                if fetch_status == 'success':
                    # æˆåŠŸè·å–
                    ch['subscriber_count'] = stats['subscriber_count']
                    ch['real_video_count'] = stats['real_video_count']
                    ch['total_channel_views'] = stats['total_channel_views']
                    ch['avg_channel_views'] = stats['avg_channel_views']
                    ch['has_real_stats'] = True
                    ch['fetch_status'] = 'success'
                elif fetch_status == 'permanent_failure':
                    # æ°¸ä¹…æ€§å¤±è´¥ï¼ˆé¢‘é“ä¸å­˜åœ¨ç­‰ï¼‰
                    ch['subscriber_count'] = 0
                    ch['real_video_count'] = 0
                    ch['total_channel_views'] = 0
                    ch['avg_channel_views'] = 0
                    ch['has_real_stats'] = False
                    ch['fetch_status'] = 'permanent_failure'
                    ch['fetch_error'] = stats.get('_error_type', 'unknown')
                else:
                    # ç½‘ç»œå¤±è´¥ï¼ˆå¯é‡è¯•ï¼‰
                    ch['subscriber_count'] = 0
                    ch['real_video_count'] = 0
                    ch['total_channel_views'] = 0
                    ch['avg_channel_views'] = 0
                    ch['has_real_stats'] = False
                    ch['fetch_status'] = 'network_failure'
                    ch['fetch_error'] = stats.get('_error_type', 'unknown')
                    ch['retriable'] = True  # æ ‡è®°å¯é‡è¯•
            else:
                # ä¸åœ¨è·å–åˆ—è¡¨ä¸­ï¼ˆè¶…å‡ºæœ€å¤§è·å–æ•°é‡ï¼‰
                ch['subscriber_count'] = 0
                ch['real_video_count'] = 0
                ch['total_channel_views'] = 0
                ch['avg_channel_views'] = 0
                ch['has_real_stats'] = False
                ch['fetch_status'] = 'not_fetched'

    # é‡æ–°æ’åºï¼ˆæŒ‰æ€»æ’­æ”¾é‡ï¼Œå› ä¸ºè®¢é˜…æ•°å¯èƒ½è·å–ä¸åˆ°ï¼‰
    videos = sort_videos(videos, request.sort_by)
    channels.sort(key=lambda x: x['total_views'], reverse=True)

    # è®¡ç®—åˆ†å¸ƒ
    duration_dist = calculate_duration_distribution(videos)

    # ===== åˆ†ç±»è§†é¢‘ï¼šè¿‘æœŸçˆ†æ¬¾ vs é•¿é’è§†é¢‘ =====
    # è¿‘æœŸçˆ†æ¬¾ï¼š30å¤©å†…å‘å¸ƒï¼Œæ’­æ”¾é‡>=1000ï¼ŒæŒ‰æ’­æ”¾é‡ä»é«˜åˆ°ä½æ’åº
    recent_hits = [
        v for v in videos
        if v['video_age_days'] <= 30 and v['view_count'] >= 1000
    ]
    recent_hits.sort(key=lambda x: x['view_count'], reverse=True)

    # é•¿é’è§†é¢‘ï¼šå‘å¸ƒè¶…è¿‡180å¤©ï¼Œæ—¥å‡æ’­æ”¾>=1000ï¼ˆè¯´æ˜æŒç»­æœ‰æµé‡ï¼‰
    evergreen_videos = [
        v for v in videos
        if v['video_age_days'] > 180 and v['daily_views'] >= 1000
    ]
    evergreen_videos.sort(key=lambda x: x['daily_views'], reverse=True)

    print(f"\nğŸ“Š è§†é¢‘åˆ†ç±»:")
    print(f"   è¿‘æœŸçˆ†æ¬¾(30å¤©å†…,â‰¥1000æ’­æ”¾): {len(recent_hits)} ä¸ª")
    print(f"   é•¿é’è§†é¢‘(>180å¤©,æ—¥å‡â‰¥1000): {len(evergreen_videos)} ä¸ª")

    await manager.send_progress(task_id, {
        "type": "progress",
        "stage": "report",
        "progress": 95,
        "message": "æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š..."
    })

    insights = generate_insights(videos, channels, duration_dist)

    return {
        "topic": request.topic,
        "total_videos": len(videos),
        "total_views": sum(v['view_count'] for v in videos),
        "total_channels": len(channels),
        "videos": videos[:200],
        "channels": channels[:100],
        "insights": insights,
        "duration_distribution": duration_dist,
        "ai_video_count": sum(1 for v in videos if v.get('is_ai_video')),
        # æ–°å¢ï¼šè§†é¢‘åˆ†ç±»
        "recent_hits": recent_hits[:50],  # è¿‘æœŸçˆ†æ¬¾ Top 50
        "evergreen_videos": evergreen_videos[:50]  # é•¿é’è§†é¢‘ Top 50
    }


def apply_filters(video: dict, request: ResearchRequest) -> bool:
    """åº”ç”¨ç­›é€‰æ¡ä»¶"""
    if request.views_min and video['view_count'] < request.views_min:
        return False
    if request.views_max and video['view_count'] > request.views_max:
        return False

    if request.duration_filter:
        duration_cat = get_duration_category(video['duration'])
        if duration_cat not in request.duration_filter:
            return False

    if request.ai_filter == 'ai' and not video.get('is_ai_video'):
        return False
    if request.ai_filter == 'human' and video.get('is_ai_video'):
        return False

    # æ—¶é—´èŒƒå›´ç­›é€‰ï¼š
    # - 24h/7d/30d/1y: YouTube sp å‚æ•°å·²åœ¨æœç´¢æ—¶è¿‡æ»¤
    # - 90dï¼ˆ3ä¸ªæœˆï¼‰: YouTube æ²¡æœ‰åŸç”Ÿæ”¯æŒï¼Œéœ€è¦åœ¨è¿™é‡Œè¡¥å……è¿‡æ»¤
    if request.time_range == '90d':
        if not check_time_range(video.get('published_at', ''), '90d'):
            return False

    return True


def check_time_range(published_at: str, time_range: str) -> bool:
    """æ£€æŸ¥è§†é¢‘å‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…"""
    if not published_at:
        return True  # å¦‚æœæ²¡æœ‰å‘å¸ƒæ—¶é—´ï¼Œä¸è¿‡æ»¤

    try:
        # yt-dlp è¿”å›çš„æ—¥æœŸæ ¼å¼æ˜¯ YYYYMMDD
        if len(published_at) == 8:
            pub_date = datetime.strptime(published_at, '%Y%m%d')
        else:
            pub_date = datetime.strptime(published_at[:10], '%Y-%m-%d')

        now = datetime.now()

        time_deltas = {
            '24h': 1,
            '7d': 7,
            '30d': 30,
            '90d': 90,
            '1y': 365
        }

        days = time_deltas.get(time_range)
        if days:
            cutoff = now - timedelta(days=days)
            return pub_date >= cutoff

        return True
    except Exception as e:
        print(f"æ—¥æœŸè§£æé”™è¯¯: {e}, published_at={published_at}")
        return True  # è§£æå¤±è´¥æ—¶ä¸è¿‡æ»¤


def sort_videos(videos: List[dict], sort_by: str) -> List[dict]:
    """æ’åºè§†é¢‘"""
    sort_keys = {
        'views': lambda x: x['view_count'],
        'likes': lambda x: x['like_count'],
        'comments': lambda x: x['comment_count'],
        'recent': lambda x: x['published_at'] or '',
        'engagement': lambda x: (x['like_count'] + x['comment_count']) / max(x['view_count'], 1)
    }
    key_func = sort_keys.get(sort_by, sort_keys['views'])
    return sorted(videos, key=key_func, reverse=True)


def calculate_duration_distribution(videos: List[dict]) -> dict:
    """è®¡ç®—æ—¶é•¿åˆ†å¸ƒ"""
    dist = {
        "short": {"count": 0, "total_views": 0, "label": "<5åˆ†é’Ÿ"},
        "medium": {"count": 0, "total_views": 0, "label": "5-20åˆ†é’Ÿ"},
        "long": {"count": 0, "total_views": 0, "label": ">20åˆ†é’Ÿ"}
    }

    for v in videos:
        cat = get_duration_category(v['duration'])
        dist[cat]["count"] += 1
        dist[cat]["total_views"] += v['view_count']

    for cat in dist:
        if dist[cat]["count"] > 0:
            dist[cat]["avg_views"] = dist[cat]["total_views"] // dist[cat]["count"]
        else:
            dist[cat]["avg_views"] = 0

    return dist


def generate_insights(videos: List[dict], channels: List[dict], duration_dist: dict) -> dict:
    """ç”Ÿæˆæ´å¯Ÿ"""
    insights = {
        "opportunities": [],
        "top_performing": None,
        "market_summary": {}
    }

    if not videos:
        return insights

    total_views = sum(v['view_count'] for v in videos)
    avg_views = total_views // len(videos) if videos else 0

    insights["market_summary"] = {
        "total_videos": len(videos),
        "total_views": total_views,
        "avg_views": avg_views,
        "total_channels": len(channels)
    }

    # æœ€ä½³æ—¶é•¿æœºä¼š
    best_duration = max(duration_dist.items(), key=lambda x: x[1]["avg_views"])
    if best_duration[1]["count"] > 0:
        supply_pct = (best_duration[1]["count"] / len(videos)) * 100
        insights["opportunities"].append({
            "type": "duration",
            "title": f"{best_duration[1]['label']}è§†é¢‘è¡¨ç°æœ€ä½³",
            "description": f"è¿™ä¸ªæ—¶é•¿åªå  {supply_pct:.1f}% ä¾›ç»™ï¼Œä½†å¹³å‡æ’­æ”¾é‡è¾¾ {best_duration[1]['avg_views']:,}",
            "score": "A" if best_duration[1]["avg_views"] > avg_views * 2 else "B"
        })

    # çˆ†æ¬¾è§†é¢‘
    if videos:
        top_video = videos[0]
        insights["top_performing"] = {
            "title": top_video['title'],
            "views": top_video['view_count'],
            "channel": top_video['channel_name']
        }

    # AI è§†é¢‘æœºä¼š
    ai_count = sum(1 for v in videos if v.get('is_ai_video'))
    if ai_count > 0:
        ai_pct = (ai_count / len(videos)) * 100
        insights["opportunities"].append({
            "type": "ai_content",
            "title": f"å‘ç° {ai_count} ä¸ª AI ç”Ÿæˆè§†é¢‘",
            "description": f"AI å†…å®¹å æ¯” {ai_pct:.1f}%ï¼Œ{'è¿™ä¸ªé¢†åŸŸ AI å†…å®¹è¾ƒå¤š' if ai_pct > 10 else 'ä»ä»¥çœŸäººå†…å®¹ä¸ºä¸»'}",
            "score": "B"
        })

    return insights


# ============ ç›‘æ§ä»»åŠ¡ API ============

@app.post("/api/monitor/tasks")
async def create_monitor_task(task: MonitorTaskCreate):
    """åˆ›å»ºç›‘æ§ä»»åŠ¡

    è®¾ç½®å…³é”®è¯å’Œé‡‡é›†é¢‘ç‡ï¼Œç³»ç»Ÿå°†å®šæ—¶é‡‡é›†æ•°æ®
    """
    db = get_database()

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    existing = db.get_monitor_task_by_keyword(task.keyword)
    if existing:
        raise HTTPException(status_code=400, detail=f"å…³é”®è¯ '{task.keyword}' å·²å­˜åœ¨ç›‘æ§ä»»åŠ¡")

    task_id = db.create_monitor_task(
        keyword=task.keyword,
        description=task.description,
        frequency=task.frequency,
        max_results=task.max_results
    )

    return {
        "success": True,
        "task_id": task_id,
        "message": f"ç›‘æ§ä»»åŠ¡å·²åˆ›å»º: {task.keyword}"
    }


@app.get("/api/monitor/tasks")
async def list_monitor_tasks(active_only: bool = Query(default=True)):
    """è·å–æ‰€æœ‰ç›‘æ§ä»»åŠ¡"""
    db = get_database()
    tasks = db.list_monitor_tasks(active_only=active_only)
    return {
        "tasks": tasks,
        "count": len(tasks)
    }


@app.get("/api/monitor/tasks/{task_id}")
async def get_monitor_task(task_id: int):
    """è·å–å•ä¸ªç›‘æ§ä»»åŠ¡è¯¦æƒ…"""
    db = get_database()
    task = db.get_monitor_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ç›‘æ§ä»»åŠ¡ä¸å­˜åœ¨")

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    growth_summary = db.get_growth_summary(task_id, days=7)

    return {
        **task,
        "growth_summary": growth_summary
    }


@app.put("/api/monitor/tasks/{task_id}")
async def update_monitor_task(task_id: int, update: MonitorTaskUpdate):
    """æ›´æ–°ç›‘æ§ä»»åŠ¡"""
    db = get_database()

    task = db.get_monitor_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ç›‘æ§ä»»åŠ¡ä¸å­˜åœ¨")

    update_data = {k: v for k, v in update.dict().items() if v is not None}
    if update_data:
        db.update_monitor_task(task_id, **update_data)

    return {"success": True, "message": "ç›‘æ§ä»»åŠ¡å·²æ›´æ–°"}


@app.delete("/api/monitor/tasks/{task_id}")
async def delete_monitor_task(task_id: int):
    """åˆ é™¤ç›‘æ§ä»»åŠ¡"""
    db = get_database()
    result = db.delete_monitor_task(task_id)
    if result == 0:
        raise HTTPException(status_code=404, detail="ç›‘æ§ä»»åŠ¡ä¸å­˜åœ¨")
    return {"success": True, "message": "ç›‘æ§ä»»åŠ¡å·²åˆ é™¤"}


@app.post("/api/monitor/tasks/{task_id}/toggle")
async def toggle_monitor_task(task_id: int):
    """åˆ‡æ¢ç›‘æ§ä»»åŠ¡çš„å¯ç”¨/æš‚åœçŠ¶æ€"""
    db = get_database()
    result = db.toggle_monitor_task(task_id)
    if result == 0:
        raise HTTPException(status_code=404, detail="ç›‘æ§ä»»åŠ¡ä¸å­˜åœ¨")

    task = db.get_monitor_task(task_id)
    status = "å·²å¯ç”¨" if task['is_active'] else "å·²æš‚åœ"
    return {"success": True, "is_active": task['is_active'], "message": f"ç›‘æ§ä»»åŠ¡{status}"}


@app.post("/api/monitor/tasks/{task_id}/run")
async def run_monitor_task_now(task_id: int, background_tasks: BackgroundTasks):
    """ç«‹å³æ‰§è¡Œç›‘æ§ä»»åŠ¡ï¼ˆæ‰‹åŠ¨è§¦å‘ï¼‰"""
    db = get_database()
    task = db.get_monitor_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ç›‘æ§ä»»åŠ¡ä¸å­˜åœ¨")

    # åœ¨åå°æ‰§è¡Œé‡‡é›†
    background_tasks.add_task(execute_monitor_task, task_id)

    return {
        "success": True,
        "message": f"ç›‘æ§ä»»åŠ¡å·²å¼€å§‹æ‰§è¡Œ: {task['keyword']}",
        "task_id": task_id
    }


async def execute_monitor_task(task_id: int):
    """æ‰§è¡Œå•ä¸ªç›‘æ§ä»»åŠ¡ï¼ˆåå°è¿è¡Œï¼‰"""
    db = get_database()
    task = db.get_monitor_task(task_id)

    if not task:
        print(f"âŒ ç›‘æ§ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
        return

    keyword = task['keyword']
    max_results = task.get('max_results', 500)

    print(f"\nğŸ”„ å¼€å§‹æ‰§è¡Œç›‘æ§ä»»åŠ¡: {keyword}")
    print(f"   ä»»åŠ¡ID: {task_id}")
    print(f"   ç›®æ ‡æ•°é‡: {max_results}")

    try:
        # 1. æ‰§è¡Œæœç´¢
        loop = asyncio.get_event_loop()
        videos = await search_videos_multi(
            keyword,
            target_count=max_results,
            use_smart_keywords=True
        )

        print(f"   âœ“ æœç´¢å®Œæˆ: {len(videos)} ä¸ªè§†é¢‘")

        # 2. ä¿å­˜å¿«ç…§
        saved_count = 0
        for video in videos:
            video_data = {
                'video_id': video.get('id'),
                'title': video.get('title'),
                'channel_name': video.get('channel', video.get('uploader')),
                'channel_id': video.get('channel_id'),
                'view_count': video.get('view_count', 0) or 0,
                'like_count': video.get('like_count', 0) or 0,
                'comment_count': video.get('comment_count', 0) or 0,
                'duration': video.get('duration', 0) or 0,
                'published_at': video.get('upload_date')
            }
            try:
                db.save_video_snapshot(task_id, video_data)
                saved_count += 1
            except Exception as e:
                pass  # å¿½ç•¥é‡å¤ä¿å­˜ç­‰é”™è¯¯

        print(f"   âœ“ ä¿å­˜å¿«ç…§: {saved_count} ä¸ªè§†é¢‘")

        # 3. è®¡ç®—å¢é•¿æ•°æ®
        growth_count = db.calculate_and_save_growth(task_id)
        print(f"   âœ“ è®¡ç®—å¢é•¿: {growth_count} ä¸ªè§†é¢‘")

        # 4. æ›´æ–°ä»»åŠ¡çŠ¶æ€
        db.update_monitor_task_last_run(task_id)

        print(f"   âœ“ ç›‘æ§ä»»åŠ¡å®Œæˆ: {keyword}")

    except Exception as e:
        print(f"   âŒ ç›‘æ§ä»»åŠ¡å¤±è´¥: {e}")


@app.get("/api/monitor/tasks/{task_id}/growth")
async def get_task_growth(
    task_id: int,
    days: int = Query(default=7, le=30),
    trending_only: bool = Query(default=False)
):
    """è·å–ç›‘æ§ä»»åŠ¡çš„å¢é•¿æ•°æ®"""
    db = get_database()

    task = db.get_monitor_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ç›‘æ§ä»»åŠ¡ä¸å­˜åœ¨")

    # è·å–å¢é•¿æ•°æ®
    growth_data = db.get_video_growth(
        task_id=task_id,
        trending_only=trending_only,
        limit=100
    )

    # è·å–è¶‹åŠ¿è§†é¢‘
    trending = db.get_trending_videos(task_id=task_id, days=days, limit=20)

    # è·å–æ‘˜è¦
    summary = db.get_growth_summary(task_id, days=days)

    return {
        "task_id": task_id,
        "keyword": task['keyword'],
        "days": days,
        "summary": summary,
        "trending_videos": trending,
        "growth_data": growth_data[:50]
    }


@app.get("/api/monitor/tasks/{task_id}/snapshots")
async def get_task_snapshots(
    task_id: int,
    date: Optional[str] = Query(default=None, description="æ—¥æœŸ YYYY-MM-DD"),
    limit: int = Query(default=100, le=500)
):
    """è·å–ç›‘æ§ä»»åŠ¡çš„è§†é¢‘å¿«ç…§"""
    db = get_database()

    task = db.get_monitor_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ç›‘æ§ä»»åŠ¡ä¸å­˜åœ¨")

    snapshots = db.get_video_snapshots(task_id=task_id, date=date, limit=limit)

    return {
        "task_id": task_id,
        "keyword": task['keyword'],
        "date": date,
        "snapshots": snapshots,
        "count": len(snapshots)
    }


@app.get("/api/monitor/due")
async def get_due_tasks():
    """è·å–å¾…æ‰§è¡Œçš„ç›‘æ§ä»»åŠ¡"""
    db = get_database()
    tasks = db.get_due_monitor_tasks()
    return {
        "tasks": tasks,
        "count": len(tasks)
    }


# ============ å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ ============

scheduler_running = False


async def scheduler_loop():
    """å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å¾ªç¯"""
    global scheduler_running
    print("ğŸ• å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨")

    while scheduler_running:
        try:
            db = get_database()
            due_tasks = db.get_due_monitor_tasks()

            if due_tasks:
                print(f"\nğŸ“‹ å‘ç° {len(due_tasks)} ä¸ªå¾…æ‰§è¡Œä»»åŠ¡")
                for task in due_tasks:
                    print(f"   - æ‰§è¡Œä»»åŠ¡: {task['keyword']}")
                    await execute_monitor_task(task['id'])
                    await asyncio.sleep(2)  # ä»»åŠ¡é—´éš”

        except Exception as e:
            print(f"âŒ è°ƒåº¦å™¨é”™è¯¯: {e}")

        # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        await asyncio.sleep(60)

    print("ğŸ›‘ å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")


@app.post("/api/monitor/scheduler/start")
async def start_scheduler(background_tasks: BackgroundTasks):
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    global scheduler_running

    if scheduler_running:
        return {"success": False, "message": "è°ƒåº¦å™¨å·²åœ¨è¿è¡Œ"}

    scheduler_running = True
    background_tasks.add_task(scheduler_loop)

    return {"success": True, "message": "å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨"}


@app.post("/api/monitor/scheduler/stop")
async def stop_scheduler():
    """åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    global scheduler_running

    if not scheduler_running:
        return {"success": False, "message": "è°ƒåº¦å™¨æœªè¿è¡Œ"}

    scheduler_running = False
    return {"success": True, "message": "å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢"}


@app.get("/api/monitor/scheduler/status")
async def scheduler_status():
    """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
    return {
        "running": scheduler_running,
        "message": "è°ƒåº¦å™¨è¿è¡Œä¸­" if scheduler_running else "è°ƒåº¦å™¨å·²åœæ­¢"
    }


if __name__ == "__main__":
    import uvicorn

    port = int(os.environ.get("PORT", 8000))

    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     YouTube å†…å®¹åŠ©æ‰‹ API æœåŠ¡             â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  API æ–‡æ¡£:  http://localhost:{port}/docs      â•‘
â•‘  å‰ç«¯é¡µé¢:  http://localhost:{port}/           â•‘
â•‘  WebSocket: ws://localhost:{port}/ws/{{task_id}} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=port,
        reload=True
    )
